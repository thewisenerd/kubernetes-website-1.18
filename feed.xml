<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes – Production-Grade Container Orchestration</title><link>https://kubernetes.io/</link><description>The Kubernetes project blog</description><generator>Hugo -- gohugo.io</generator><image><url>https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png</url><title>Kubernetes.io</title><link>https://kubernetes.io/</link></image><atom:link href="https://kubernetes.io/feed.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Moving Forward From Beta</title><link>https://kubernetes.io/blog/2020/08/21/moving-forward-from-beta/</link><pubDate>Fri, 21 Aug 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/08/21/moving-forward-from-beta/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Tim Bannister, The Scale Factory&lt;/p>
&lt;p>In Kubernetes, features follow a defined
&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/#feature-stages">lifecycle&lt;/a>.
First, as the twinkle of an eye in an interested developer. Maybe, then,
sketched in online discussions, drawn on the online equivalent of a cafe
napkin. This rough work typically becomes a
&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/0001-kubernetes-enhancement-proposal-process.md#kubernetes-enhancement-proposal-process">Kubernetes Enhancement Proposal&lt;/a> (KEP), and
from there it usually turns into code.&lt;/p>
&lt;p>For Kubernetes v1.20 and onwards, we're focusing on helping that code
graduate into stable features.&lt;/p>
&lt;p>That lifecycle I mentioned runs as follows:&lt;/p>
&lt;p>&lt;img src="feature_stages.svg" alt="Alpha → Beta → General Availability">&lt;/p>
&lt;p>Usually, alpha features aren't enabled by default. You turn them on by setting a feature
gate; usually, by setting a command line flag on each of the components that use the
feature.&lt;/p>
&lt;p>(If you use Kubernetes through a managed service offering such as AKS, EKS, GKE, etc then
the vendor who runs that service may have decided what feature gates are enabled for you).&lt;/p>
&lt;p>There's a defined process for graduating an existing, alpha feature into the beta phase.
This is important because &lt;strong>beta features are enabled by default&lt;/strong>, with the feature flag still
there so cluster operators can opt out if they want.&lt;/p>
&lt;p>A similar but more thorough set of graduation criteria govern the transition to general
availability (GA), also known as &amp;quot;stable&amp;quot;. GA features are part of Kubernetes, with a
commitment that they are staying in place throughout the current major version.&lt;/p>
&lt;p>Having beta features on by default lets Kubernetes and its contributors get valuable
real-world feedback. However, there's a mismatch of incentives. Once a feature is enabled
by default, people will use it. Even if there might be a few details to shake out,
the way Kubernetes' REST APIs and conventions work mean that any future stable API is going
to be compatible with the most recent beta API: your API objects won't stop working when
a beta feature graduates to GA.&lt;/p>
&lt;p>For the API and its resources in particular, there's a much less strong incentive to move
features from beta to GA than from alpha to beta. Vendors who want a particular feature
have had good reason to help get code to the point where features are enabled by default,
and beyond that the journey has been less clear.&lt;/p>
&lt;p>KEPs track more than code improvements. Essentially, anything that would need
communicating to the wider community merits a KEP. That said, most KEPs cover
Kubernetes features (and the code to implement them).&lt;/p>
&lt;p>You might know that &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress&lt;/a>
has been in Kubernetes for a while, but did you realize that it actually went beta in 2015? To help
drive things forward, Kubernetes' Architecture Special Interest Group (SIG) have a new approach in
mind.&lt;/p>
&lt;h2 id="avoiding-permanent-beta">Avoiding permanent beta&lt;/h2>
&lt;p>For Kubernetes REST APIs, when a new feature's API reaches beta, that starts a countdown.
The beta-quality API now has &lt;strong>nine calendar months&lt;/strong> to either:&lt;/p>
&lt;ul>
&lt;li>reach GA, and deprecate the beta, or&lt;/li>
&lt;li>have a new beta version (&lt;em>and deprecate the previous beta&lt;/em>).&lt;/li>
&lt;/ul>
&lt;p>To be clear, at this point &lt;strong>only REST APIs are affected&lt;/strong>. For example, &lt;em>APIListChunking&lt;/em> is
a beta feature but isn't itself a REST API. Right now there are no plans to automatically
deprecate &lt;em>APIListChunking&lt;/em> nor any other features that aren't REST APIs.&lt;/p>
&lt;p>If a REST API reaches the end of that 9 month countdown, then the next Kubernetes release
will deprecate that API version. There's no option for the REST API to stay at the same
beta version beyond the first Kubernetes release to come out after the 9 month window.&lt;/p>
&lt;h3 id="what-this-means-for-you">What this means for you&lt;/h3>
&lt;p>If you're using Kubernetes, there's a good chance that you're using a beta feature. Like
I said, there are lots of them about.
As well as Ingress, you might be using &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">CronJob&lt;/a>,
or &lt;a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">PodSecurityPolicy&lt;/a>, or others.
There's an even bigger chance that you're running on a control plane with at least one beta
feature enabled.&lt;/p>
&lt;p>If you're using or generating Kubernetes manifests that use beta APIs like Ingress, you'll
need to plan to revise those. The current APIs are going to be deprecated following a
schedule (the 9 months I mentioned earlier) and after a further 9 months those deprecated
APIs will be removed. At that point, to stay current with Kubernetes, you should already
have migrated.&lt;/p>
&lt;h3 id="what-this-means-for-kubernetes-contributors">What this means for Kubernetes contributors&lt;/h3>
&lt;p>The motivation here seems pretty clear: get features stable. Guaranteeing that beta
features will be deprecated adds a pretty big incentive so that people who want the
feature continue their effort until the code, documentation and tests are ready for this
feature to graduate to stable, backed by several Kubernetes' releases of evidence in
real-world use.&lt;/p>
&lt;h3 id="what-this-means-for-the-ecosystem">What this means for the ecosystem&lt;/h3>
&lt;p>In my opinion, these harsh-seeming measures make a lot of sense, and are going to be
good for Kubernetes. Deprecating existing APIs, through a rule that applies across all
the different Special Interest Groups (SIGs), helps avoid stagnation and encourages
fixes.&lt;/p>
&lt;p>Let's say that an API goes to beta and then real-world experience shows that it
just isn't right - that, fundamentally, the API has shortcomings. With that 9 month
countdown ticking, the people involved have the means and the justification to revise
and release an API that deals with the problem cases. Anyone who wants to live with
the deprecated API is welcome to - Kubernetes is open source - but their needs do not
have to hold up progress on the feature.&lt;/p></description></item><item><title>Blog: Introducing Hierarchical Namespaces</title><link>https://kubernetes.io/blog/2020/08/14/introducing-hierarchical-namespaces/</link><pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/08/14/introducing-hierarchical-namespaces/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Adrian Ludwin (Google)&lt;/p>
&lt;p>Safely hosting large numbers of users on a single Kubernetes cluster has always
been a troublesome task. One key reason for this is that different organizations
use Kubernetes in different ways, and so no one tenancy model is likely to suit
everyone. Instead, Kubernetes offers you building blocks to create your own
tenancy solution, such as Role Based Access Control (RBAC) and NetworkPolicies;
the better these building blocks, the easier it is to safely build a multitenant
cluster.&lt;/p>
&lt;h1 id="namespaces-for-tenancy">Namespaces for tenancy&lt;/h1>
&lt;p>By far the most important of these building blocks is the namespace, which forms
the backbone of almost all Kubernetes control plane security and sharing
policies. For example, RBAC, NetworkPolicies and ResourceQuotas all respect
namespaces by default, and objects such as Secrets, ServiceAccounts and
Ingresses are freely usable &lt;em>within&lt;/em> any one namespace, but fully segregated
from &lt;em>other&lt;/em> namespaces.&lt;/p>
&lt;p>Namespaces have two key properties that make them ideal for policy enforcement.
Firstly, they can be used to &lt;strong>represent ownership&lt;/strong>. Most Kubernetes objects
&lt;em>must&lt;/em> be in a namespace, so if you use namespaces to represent ownership, you
can always count on there being an owner.&lt;/p>
&lt;p>Secondly, namespaces have &lt;strong>authorized creation and use&lt;/strong>. Only
highly-privileged users can create namespaces, and other users require explicit
permission to use those namespaces - that is, create, view or modify objects in
those namespaces. This allows them to be carefully created with appropriate
policies, before unprivileged users can create “regular” objects like pods and
services.&lt;/p>
&lt;h1 id="the-limits-of-namespaces">The limits of namespaces&lt;/h1>
&lt;p>However, in practice, namespaces are not flexible enough to meet some common use
cases. For example, let’s say that one team owns several microservices with
different secrets and quotas. Ideally, they should place these services into
different namespaces in order to isolate them from each other, but this presents
two problems.&lt;/p>
&lt;p>Firstly, these namespaces have no common concept of ownership, even though
they’re both owned by the same team. This means that if the team controls
multiple namespaces, not only does Kubernetes not have any record of their
common owner, but namespaced-scoped policies cannot be applied uniformly across
them.&lt;/p>
&lt;p>Secondly, teams generally work best if they can operate autonomously, but since
namespace creation is highly privileged, it’s unlikely that any member of the
dev team is allowed to create namespaces. This means that whenever a team wants
a new namespace, they must raise a ticket to the cluster administrator. While
this is probably acceptable for small organizations, it generates unnecessary
toil as the organization grows.&lt;/p>
&lt;h1 id="introducing-hierarchical-namespaces">Introducing hierarchical namespaces&lt;/h1>
&lt;p>&lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/blob/master/incubator/hnc/docs/user-guide/concepts.md#basic">Hierarchical
namespaces&lt;/a>
are a new concept developed by the &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy">Kubernetes Working Group for Multi-Tenancy
(wg-multitenancy)&lt;/a> in order to
solve these problems. In its simplest form, a hierarchical namespace is a
regular Kubernetes namespace that contains a small custom resource that
identifies a single, optional, parent namespace. This establishes the concept of
ownership &lt;em>across&lt;/em> namespaces, not just &lt;em>within&lt;/em> them.&lt;/p>
&lt;p>This concept of ownership enables two additional types of behaviours:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Policy inheritance:&lt;/strong> if one namespace is a child of another, policy objects
such as RBAC RoleBindings are &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/blob/master/incubator/hnc/docs/user-guide/concepts.md#basic-propagation">copied from the parent to the
child&lt;/a>.&lt;/li>
&lt;li>&lt;strong>Delegated creation:&lt;/strong> you usually need cluster-level privileges to create a
namespace, but hierarchical namespaces adds an alternative:
&lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/blob/master/incubator/hnc/docs/user-guide/concepts.md#basic-subns">&lt;em>subnamespaces&lt;/em>&lt;/a>,
which can be manipulated using only limited permissions in the parent
namespace.&lt;/li>
&lt;/ul>
&lt;p>This solves both of the problems for our dev team. The cluster administrator can
create a single “root” namespace for the team, along with all necessary
policies, and then delegate permission to create subnamespaces to members of
that team. Those team members can then create subnamespaces for their own use,
without violating the policies that were imposed by the cluster administrators.&lt;/p>
&lt;h1 id="hands-on-with-hierarchical-namespaces">Hands-on with hierarchical namespaces&lt;/h1>
&lt;p>Hierarchical namespaces are provided by a Kubernetes extension known as the
&lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/tree/master/incubator/hnc">&lt;strong>Hierarchical Namespace
Controller&lt;/strong>&lt;/a>,
or &lt;strong>HNC&lt;/strong>. The HNC consists of two components:&lt;/p>
&lt;ul>
&lt;li>The &lt;strong>manager&lt;/strong> runs on your cluster, manages subnamespaces, propagates policy
objects, ensures that your hierarchies are legal and manages extension points.&lt;/li>
&lt;li>The &lt;strong>kubectl plugin&lt;/strong>, called &lt;code>kubectl-hns&lt;/code>, makes it easy for users to
interact with the manager.&lt;/li>
&lt;/ul>
&lt;p>Both can be easily installed from the &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/releases">releases page of our
repo&lt;/a>.&lt;/p>
&lt;p>Let’s see HNC in action. Imagine that I do not have namespace creation
privileges, but I can view the namespace &lt;code>team-a&lt;/code> and create subnamespaces
within it&lt;sup>&lt;a href="#note-1">1&lt;/a>&lt;/sup>. Using the plugin, I can now say:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl hns create svc1-team-a -n team-a
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This creates a subnamespace called &lt;code>svc1-team-a&lt;/code>. Note that since subnamespaces
are just regular Kubernetes namespaces, all subnamespace names must still be
unique.&lt;/p>
&lt;p>I can view the structure of these namespaces by asking for a tree view:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl hns tree team-a
&lt;span style="color:#080;font-style:italic"># Output:&lt;/span>
team-a
└── svc1-team-a
&lt;/code>&lt;/pre>&lt;/div>&lt;p>And if there were any policies in the parent namespace, these now appear in the
child as well&lt;sup>&lt;a href="#note-2">2&lt;/a>&lt;/sup>. For example, let’s say that &lt;code>team-a&lt;/code> had
an RBAC RoleBinding called &lt;code>sres&lt;/code>. This rolebinding will also be present in the
subnamespace:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl describe rolebinding sres -n svc1-team-a
&lt;span style="color:#080;font-style:italic"># Output:&lt;/span>
Name: sres
Labels: hnc.x-k8s.io/inheritedFrom&lt;span style="color:#666">=&lt;/span>team-a &lt;span style="color:#080;font-style:italic"># inserted by HNC&lt;/span>
Annotations: &amp;lt;none&amp;gt;
Role:
Kind: ClusterRole
Name: admin
Subjects: ...
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, HNC adds labels to these namespaces with useful information about the
hierarchy which you can use to apply other policies. For example, you can create
the following NetworkPolicy:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NetworkPolicy&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>networking.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>allow-team-a&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>team-a&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">ingress&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">from&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">namespaceSelector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">matchExpressions&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">key&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;team-a.tree.hnc.x-k8s.io/depth&amp;#39;&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Label created by HNC&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">operator&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Exists&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This policy will both be propagated to all descendants of &lt;code>team-a&lt;/code>, and will
&lt;em>also&lt;/em> allow ingress traffic between all of those namespaces. The “tree” label
can only be applied by HNC, and is guaranteed to reflect the latest hierarchy.&lt;/p>
&lt;p>You can learn all about the features of HNC from the &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/tree/master/incubator/hnc/docs/user-guide">user
guide&lt;/a>.&lt;/p>
&lt;h1 id="next-steps-and-getting-involved">Next steps and getting involved&lt;/h1>
&lt;p>If you think that hierarchical namespaces can work for your organization, &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/releases/tag/hnc-v0.5.1">HNC
v0.5.1 is available on
GitHub&lt;/a>.
We’d love to know what you think of it, what problems you’re using it to solve
and what features you’d most like to see added. As with all early software, you
should be cautious about using HNC in production environments, but the more
feedback we get, the sooner we’ll be able to drive to HNC 1.0.&lt;/p>
&lt;p>We’re also open to additional contributors, whether it’s to fix or report bugs,
or help prototype new features such as exceptions, improved monitoring,
hierarchical resource quotas or fine-grained configuration.&lt;/p>
&lt;p>Please get in touch with us via our
&lt;a href="https://github.com/kubernetes-sigs/multi-tenancy">repo&lt;/a>, &lt;a href="https://groups.google.com/g/kubernetes-wg-multitenancy">mailing
list&lt;/a> or on
&lt;a href="https://kubernetes.slack.com/messages/wg-multitenancy">Slack&lt;/a> - we look forward
to hearing from you!&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/aludwin">Adrian Ludwin&lt;/a> is a software engineer and the
tech lead for the Hierarchical Namespace Controller.&lt;/em>&lt;/p>
&lt;a name="note-1"/>
&lt;p>&lt;em>Note 1: technically, you create a small object called a &amp;quot;subnamespace anchor&amp;quot;
in the parent namespace, and then HNC creates the subnamespace for you.&lt;/em>&lt;/p>
&lt;a name="note-2"/>
&lt;p>&lt;em>Note 2: By default, only RBAC Roles and RoleBindings are propagated, but you
can configure HNC to propagate any namespaced Kubernetes object.&lt;/em>&lt;/p></description></item><item><title>Blog: Physics, politics and Pull Requests: the Kubernetes 1.18 release interview</title><link>https://kubernetes.io/blog/2020/08/03/physics-politics-and-pull-requests-the-kubernetes-1.18-release-interview/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/08/03/physics-politics-and-pull-requests-the-kubernetes-1.18-release-interview/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Craig Box (Google)&lt;/p>
&lt;p>The start of the COVID-19 pandemic couldn't delay the release of Kubernetes 1.18, but unfortunately &lt;a href="https://github.com/kubernetes/utils/issues/141">a small bug&lt;/a> could — thankfully only by a day. This was the last cat that needed to be herded by 1.18 release lead &lt;a href="https://twitter.com/alejandrox135">Jorge Alarcón&lt;/a> before the &lt;a href="https://kubernetes.io/blog/2020/03/25/kubernetes-1-18-release-announcement/">release on March 25&lt;/a>.&lt;/p>
&lt;p>One of the best parts about co-hosting the weekly &lt;a href="https://kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> is the conversations we have with the people who help bring Kubernetes releases together. &lt;a href="https://kubernetespodcast.com/episode/096-kubernetes-1.18/">Jorge was our guest on episode 96&lt;/a> back in March, and &lt;a href="https://kubernetes.io/blog/2020/07/27/music-and-math-the-kubernetes-1.17-release-interview/">just like last week&lt;/a> we are delighted to bring you the transcript of this interview.&lt;/p>
&lt;p>If you'd rather enjoy the &amp;quot;audiobook version&amp;quot;, including another interview when 1.19 is released later this month, &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe to the show&lt;/a> wherever you get your podcasts.&lt;/p>
&lt;p>In the last few weeks, we've talked to long-time Kubernetes contributors and SIG leads &lt;a href="https://kubernetespodcast.com/episode/114-scheduling/">David Oppenheimer&lt;/a>, &lt;a href="https://kubernetespodcast.com/episode/113-instrumentation-and-cadvisor/">David Ashpole&lt;/a> and &lt;a href="https://kubernetespodcast.com/episode/111-scalability/">Wojciech Tyczynski&lt;/a>. All are worth taking the dog for a longer walk to listen to!&lt;/p>
&lt;hr>
&lt;p>&lt;strong>ADAM GLICK: You're a former physicist. I have to ask, what kind of physics did you work on?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Back in my days of math and all that, I used to work in &lt;a href="https://en.wikipedia.org/wiki/Computational_biology">computational biology&lt;/a> and a little bit of high energy physics. Computational biology was, for the most part, what I spent most of my time on. And it was essentially exploring the big idea of we have the structure of proteins. We know what they're made of. Now, based on that structure, we want to be able to predict &lt;a href="https://en.wikipedia.org/wiki/Protein_folding">how they're going to fold&lt;/a> and how they're going to behave, which essentially translates into the whole idea of designing pharmaceuticals, designing vaccines, or anything that you can possibly think of that has any connection whatsoever to a living organism.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: That would seem to ladder itself well into maybe going to something like bioinformatics. Did you take a tour into that, or did you decide to go elsewhere directly?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: It is related, and I worked a little bit with some people that did focus on bioinformatics on the field specifically, but I never took a detour into it. Really, my big idea with computational biology, to be honest, it wasn't even the biology. That's usually what sells it, what people are really interested in, because protein engineering, all the cool and amazing things that you can do.&lt;/p>
&lt;p>Which is definitely good, and I don't want to take away from it. But my big thing is because biology is such a real thing, it is amazingly complicated. And the math— the models that you have to design to study those systems, to be able to predict something that people can actually experiment and measure, it just captivated me. The level of complexity, the beauty, the mechanisms, all the structures that you see once you got through the math and look at things, it just kind of got to me.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: How did you go from that world into the world of Kubernetes?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: That's both a really boring story and an interesting one.&lt;/p>
&lt;p>[LAUGHING]&lt;/p>
&lt;p>I did my thing with physics, and it was good. It was fun. But at some point, I wanted— working in academia— at least my feeling for it is that generally all the people that you're surrounded with are usually academics. Just another bunch of physics, a bunch of mathematicians.&lt;/p>
&lt;p>But very seldom do you actually get the opportunity to take what you're working on and give it to someone else to use. Even with the mathematicians and physicists, the things that we're working on are super specialized, and you can probably find three, four, five people that can actually understand everything that you're saying. A lot of people are going to get the gist of it, but understanding the details, it's somewhat rare.&lt;/p>
&lt;p>One of the things that I absolutely love about tech, about software engineering, coding, all that, is how open and transparent everything is. You can write your library in Python, you can publish it, and suddenly the world is going to actually use it, actually consume it. And because normally, I've seen that it has a large avenue where you can work in something really complicated, you can communicate it, and people can actually go ahead and take it and run with it in their given direction. And that is kind of what happened.&lt;/p>
&lt;p>At some point, by pure accident and chance, I came across this group of people on the internet, and they were in the stages of making up this new group that's called &lt;a href="https://datafordemocracy.org/">Data for Democracy&lt;/a>, a non-profit. And the whole idea was the internet, especially Twitter— that's how we congregated— Twitter, the internet. We have a ton of data scientists, people who work as software engineers, and the like. What if we all come together and try to solve some issues that actually affect the daily lives of people. And there were a ton of projects. Helping the ACLU gather data for something interesting that they were doing, gather data and analyze it for local governments— where do you have potholes, how much water is being consumed.&lt;/p>
&lt;p>Try to apply all the science that we knew, combined with all the code that we could write, and offer a good and digestible idea for people to say, OK, this makes sense, let's do something about it— policy, action, whatever. And I started working with this group, Data for Democracy— wonderful set of people. And the person who I believe we can blame for Data for Democracy— the one who got the idea and got it up and running, his name is Jonathan Morgan. And eventually, we got to work together. He started a startup, and I went to work with the startup. And that was essentially the thing that took me away from physics and into the world of software engineering— Data for Democracy, definitely.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Were you using Kubernetes as part of that work there?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: No, it was simple as it gets. You just try to get some data. You create a couple &lt;a href="https://ipython.org/">IPython notebooks&lt;/a>, some setting up of really simple MySQL databases, and that was it.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Where did you get started using Kubernetes? And was it before you started contributing to it and being a part, or did you decide to jump right in?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: When I first started using Kubernetes, it was also on my first job. So there wasn't a lot of specific training in regards to software engineering or anything of the sort that I did before I actually started working as a software engineer. I just went from physicist to engineer. And in my days of physics, at least on the computer side, I was completely trained in the super old school system administrator, where you have your 10, 20 computers. You know physically where they are, and you have to connect the cables.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: All pets— all pets all the time.&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: [LAUGHING] You have to have your huge Python, bash scripts, three, five major versions, all because doing an upgrade will break something really important and you have no idea how to work on it. And that was my training. That was the way that I learned how to do things. Those were the kind of things that I knew how to do.&lt;/p>
&lt;p>And when I got to this company— startup— we were pretty much starting from scratch. We were building a couple applications. We work testing them, we were deploying them on a couple of managed instances. But like everything, there was a lot of toil that we wanted to automate. The whole issue of, OK, after days of work, we finally managed to get this version of the application up and running in these machines.&lt;/p>
&lt;p>It's open to the internet. People can test it out. But it turns out that it is now two weeks behind the latest on all the master branches for this repo, so now we want to update. And we have to go through the process of bringing it back up, creating new machines, do that whole thing. And I had no idea what Kubernetes was, to be honest. My boss at the moment mentioned it to me like, hey, we should use Kubernetes because apparently, Kubernetes is something that might be able to help us here. And we did some— I want to call it research and development.&lt;/p>
&lt;p>It was actually just making— again, startup, small company, small team, so really me just playing around with Kubernetes trying to get it to work, trying to get it to run. I was so lost. I had no idea what I was doing— not enough. I didn't have an idea of how Kubernetes was supposed to help me. And at that point, I did the best Googling that I could manage. Didn't really find a lot of examples. Didn't find a lot of blog posts. It was early.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What time frame was this?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Three, four years ago, so definitely not 1.13. That's the best guesstimate that I can give at this point. But I wasn't able to find any good examples, any tutorials. The only book that I was able to get my hands on was the one written by Joe Beda, Kelsey Hightower, and I forget the other author. But what is it? &amp;quot;&lt;a href="%5D(http://shop.oreilly.com/product/0636920223788.do)">Kubernetes— Up and Running&lt;/a>&amp;quot;?&lt;/p>
&lt;p>And in general, right now I use it as reference— it's really good. But as a beginner, I still was lost. They give all these amazing examples, they provide the applications, but I had no idea why someone might need a Pod, why someone might need a Deployment. So my last resort was to try and find someone who actually knew Kubernetes.&lt;/p>
&lt;p>By accident, during my eternal Googling, I actually found a link to the &lt;a href="http://slack.kubernetes.io/">Kubernetes Slack&lt;/a>. I jumped into the Kubernetes Slack hoping that someone might be able to help me out. And that was my entry point into the Kubernetes community. I just kept on exploring the Slack, tried to see what people were talking about, what they were asking to try to make sense of it, and just kept on iterating. And at some point, I think I got the hang of it.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What made you decide to be a release lead?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: The answer to this is my answer to why I have been contributing to Kubernetes. I really just want to be able to help out the community. Kubernetes is something that I absolutely adore.&lt;/p>
&lt;p>Comparing Kubernetes to old school system administration, a handful of years ago, it took me like a week to create a node for an application to run. It took me months to get something that vaguely looked like an Ingress resource— just setting up the Nginx, and allowing someone else to actually use my application. And the fact that I could do all of that in five minutes, it really captivated me. Plus I've got to blame it on the physics. The whole idea with physics, I really like the patterns, and I really like the design of Kubernetes.&lt;/p>
&lt;p>Once I actually got the hang of it, I loved the idea of how everything was designed, and I just wanted to learn a lot more about it. And I wanted to help the contributors. I wanted to help the people who actually build it. I wanted to help maintain it, and help provide the information for new contributors or new users. So instead of taking months for them to be up and running, let's just chat about what your issue is, and let's try to get a fix within the next hour or so.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: You work for a stealth startup right now. Is it fair to assume that they're using Kubernetes?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Yes—&lt;/p>
&lt;p>[LAUGHING]&lt;/p>
&lt;p>—for everything.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Are you able to say what &lt;a href="https://www.searchable.ai/">Searchable&lt;/a> does?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: The thing that we are trying to build is kind of like a search engine for your documents. Usually, if people have a question, they jump on Google. And for the most part, you're going to be able to get a good answer. You can ask something really random, like 'what is the weight of an elephant?'&lt;/p>
&lt;p>Which, if you think about it, it's kind of random, but Google is going to give you an answer. And the thing that we are trying to build is something similar to that, but for files. So essentially, a search engine for your files. And most people, you have your local machine loaded up with— at least mine, I have a couple tens of gigabytes of different files.&lt;/p>
&lt;p>I have Google Drive. I have a lot of documents that live in my email and the like. So the idea is to kind of build a search engine that is going to be able to connect all of those pieces. And besides doing simple word searches— for example, 'Kubernetes interview', and bring me the documents that we're looking at with all the questions— I can also ask things like what issue did I find last week while testing Prometheus. And it's going to be able to read my files, like through natural language processing, understand it, and be able to give me an answer.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: It is a Google for your personal and non-public information, essentially?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Hopefully.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Is the work that you do with Kubernetes as the release lead— is that part of your day job, or is that something that you're doing kind of nights and weekends separate from your day job?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Both. Strictly speaking, my day job is just keep working on the application, build the things that it needs, maintain the infrastructure, and all that. When I started working at the company— which by the way, the person who brought me into the company was also someone that I met from my days in Data for Democracy— we started talking about the work.&lt;/p>
&lt;p>I mentioned that I do a lot of work with the Kubernetes community and if it was OK that I continue doing it. And to my surprise, the answer was not only a yes, but yeah, you can do it during your day work. And at least for the time being, I just balance— I try to keep things organized.&lt;/p>
&lt;p>Some days I just focus on Kubernetes. Some mornings I do Kubernetes. And then afternoon, I do Searchable, vice-versa, or just go back and forth, and try to balance the work as much as possible. But being release lead, definitely, it is a lot, so nights and weekends.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: How much time does it take to be the release lead?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: It varies, but probably, if I had to give an estimate, at the very least you have to be able to dedicate four hours most days.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Four hours a day?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Yeah, most days. It varies a lot. For example, at the beginning of the release cycle, you don't need to put in that much work because essentially, you're just waiting and helping people get set up, and people are writing their &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps">Kubernetes Enhancement Proposals&lt;/a>, they are implementing it, and you can answer some questions. It's relatively easy, but for the most part, a lot of the time the four hours go into talking with people, just making sure that, hey, are people actually writing their enhancements, do we have all the enhancements that we want. And most of those fours hours, going around, chatting with people, and making sure that things are being done. And if, for some reason, someone needs help, just directing them to the right place to get their answer.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What does Searchable get out of you doing this work?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Physically, nothing. The thing that we're striving for is to give back to the community. My manager/boss/homeslice— I told him I was going to call him my homeslice— both of us have experience working in open source. At some point, he was also working on a project that I'm probably going to mispronounce, but Mahout with Apache.&lt;/p>
&lt;p>And he also has had this experience. And both of us have this general idea and strive to build something for Searchable that's going to be useful for people, but also build knowledge, build guides, build applications that are going to be useful for the community. And at least one of the things that I was able to do right now is be the lead for the Kubernetes team. And this is a way of giving back to the community. We're using Kubernetes to run our things, so let's try to balance how things work.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Lachlan Evenson was the release lead on 1.16 as well as &lt;a href="https://kubernetespodcast.com/episode/072-kubernetes-1.16/">our guest back in episode 72&lt;/a>, and he's returned on this release as the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team/role-handbooks/emeritus-adviser">emeritus advisor&lt;/a>. What did you learn from him?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Oh, everything. And it actually all started back on 1.16. So like you said, an amazing person— he's an amazing individual. And it's truly an opportunity to be able to work with him. During 1.16, I was the CI Signal lead, and Lachie is very hands on.&lt;/p>
&lt;p>He's not the kind of person to just give you a list of things and say, do them. He actually comes to you, has a conversation, and he works with you more than anything. And when we were working together on 1.16, I got to learn a lot from him in terms of CI Signal. And especially because we talked about everything just to make sure that 1.16 was ready to go, I also got to pick up a couple of things that a release lead has to know, has to be able to do, has to work on to get a release out the door.&lt;/p>
&lt;p>And now, during this release, there is a lot of information that's really useful, and there's a lot of advice and general wisdom that comes in handy. For most of the things that impact a lot of things, we are always in communication. Like, I'm doing this, you're doing that, advice. And essentially, every single thing that we do is pretty much a code review. You do it, and then you wait for someone else to give you comments. And that's been a strong part of our relationship working.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What would you say the theme for this release is?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: I think one of the themes is &amp;quot;fit and finish&amp;quot;. There are a lot of features that we are bumping from alpha to beta, from beta to stable. And we want to make sure that people have a good user experience. Operators and developers alike just want to get rid of as many bugs as possible, improve the flow of things.&lt;/p>
&lt;p>But the other really cool thing is we have about an equal distribution between alpha, beta, and stable. We are also bringing up a lot of new features. So besides making Kubernetes more stable for all the users that are already using it, we are working on bringing up new things that people can try out for the next release and see how it goes in the future.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Did you have a release team mascot?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Kind of.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Who/what was it?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: [LAUGHING] I say kind of because I'm using the mascot in the &lt;a href="https://twitter.com/KubernetesPod/status/1242953121380392963">logo&lt;/a>, and the logo is inspired by the Large Hadron Collider.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Oh, fantastic.&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Being the release lead, I really had to take a chance on this opportunity to use the LHC as the mascot.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: We've had &lt;a href="https://kubernetespodcast.com/episode/062-cern/">some of the folks from the LHC on the show&lt;/a>, and I know they listen, and they will be thrilled with that.&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: [LAUGHING] Hopefully, they like the logo.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: If you look at this release, what part of this release, what thing that has been added to it are you personally most excited about?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Like a parent can't choose which child is his or her favorite, you really can't choose a specific thing.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: We have been following online and in the issues an enhancement that's called &lt;a href="https://github.com/kubernetes/enhancements/issues/753">sidecar containers&lt;/a>. You'd be able to mark the order of containers starting in a pod. Tim Hockin posted &lt;a href="https://github.com/kubernetes/enhancements/issues/753#issuecomment-597372056">a long comment on behalf of a number of SIG Node contributors&lt;/a> citing social, procedural, and technical concerns about what's going on with that— in particular, that it moved out of 1.18 and is now moving to 1.19. Did you have any thoughts on that?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: The sidecar enhancement has definitely been an interesting one. First off, thank you very much to Joseph Irving, the author of the KEP. And thank you very much to Tim Hockin, who voiced out the point of view of the approvers, maintainers of SIG Node. And I guess a little bit of context before we move on is, in the Kubernetes community, we have contributors, we have reviewers, and we have approvers.&lt;/p>
&lt;p>Contributors are people who write PRs, who file issues, who troubleshoot issues. Reviewers are contributors who focus on one or multiple specific areas within the project, and then approvers are maintainers for the specific area, for one or multiple specific areas, of the project. So you can think of approvers as people who have write access in a repo or someplace within a repo.&lt;/p>
&lt;p>The issue with the sidecar enhancement is that it has been deferred for multiple releases now, and that's been because there hasn't been a lot of collaboration between the KEP authors and the approvers for specific parts of the project. Something worthwhile to mention— and this was brought up during the original discussion— is this can obviously be frustrating for both contributors and for approvers. From the contributor's side of things, you are working on something. You are doing your best to make sure that it works.&lt;/p>
&lt;p>And to build something that's going to be used by people, both from the approver side of things and, I think, for the most part, every single person in the Kubernetes community, we are all really excited to see this project grow. We want to help improve it, and we love when new people come in and work on new enhancements, bug fixes, and the like.&lt;/p>
&lt;p>But one of the limitations is the day only has so many hours, and there are only so many things that we can work on at a time. So people prioritize in whatever way works best, and some things just fall behind. And a lot of the time, the things that fall behind are not because people don't want them to continue moving forward, but it's just a limited amount of resources, a limited amount of people.&lt;/p>
&lt;p>And I think this discussion around the sidecar enhancement proposal has been very useful, and it points us to the need for more standardized mentoring programs. This is something that multiple SIGs are working on. For example, SIG Contribex, SIG Cluster Lifecycle, SIG Release. The idea is to standardize some sort of mentoring experience so that we can better prepare new contributors to become reviewers and ultimately approvers.&lt;/p>
&lt;p>Because ultimately at the end of the day, if we have more people who are knowledgeable about Kubernetes, or even some specific area of Kubernetes, we can better distribute the load, and we can better collaborate on whatever new things come up. I think the sidecar enhancement has shown us mentoring is something worthwhile, and we need a lot more of it. Because as much work as we do, more things are going to continue popping in throughout the project. And the more people we have who are comfortable working in these really complicated areas of Kubernetes, the better off that we are going to be.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Was there any talk of delaying 1.18 due to the current worldwide health situation?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: We thought about it, and the plan was to just wait and see how people felt. Tried make sure that people were comfortable continuing to work and all the people were landing in new enhancements, or fixing tests, or members of the release team who were making sure that things were happening. We wanted to see that people were comfortable, that they could continue doing their job. And for a moment, I actually thought about delaying just outright— we're going to give it more time, and hopefully at some point, things are going to work out.&lt;/p>
&lt;p>But people just continue doing their amazing work. There was no delay. There was no hitch throughout the process. So at some point, I just figured we stay with the current timeline and see how we went. And at this point, things are more or less set.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Amazing power of a distributed team.&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Yeah, definitely.&lt;/p>
&lt;p>[LAUGHING]&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: &lt;a href="https://twitter.com/alejandrox135/status/1239629281766096898">Taylor Dolezal was announced as the 1.19 release lead&lt;/a>. Do you know how that choice was made, and by whom?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: I actually got to choose the lead. The practice is the current lead for the release team is going to look at people and see, first off, who's interested and out of the people interested, who can do the job, who's comfortable enough with the release team, with the Kubernetes community at large who can actually commit the amount of hours throughout the next, hopefully, three months.&lt;/p>
&lt;p>And for one, I think Taylor has been part of my team. So there is the release team. Then the release team has multiple subgroups. One of those subgroups is actually just for me and my shadows. So for this release, it was mrbobbytables and Taylor. And Taylor volunteered to take over 1.19, and I'm sure that he will do an amazing job.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: I am as well. What advice will you give Taylor?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Over-communicate as much as possible. Normally, if you made it to the point that you are the lead for a release, or even the shadow for a release, you more or less are familiar with a lot of the work— CI Signal, enhancements, documentation, and the like. And a lot of people, if they know how to do their job, they might tell themselves, yeah, I could do it— no need to worry about it. I'm just going to go ahead and sign this PR, debug this test, whatever.&lt;/p>
&lt;p>But one of the interesting aspects is whenever we are actually working in a release, 50% of the work has to go into actually making the release happen. The other 50% of the work has to go into mentoring people, and making sure the newcomers, new members are able to learn everything that they need to learn to do your job, you being in the lead for a subgroup or the entire team. And whenever you actually see that things need to happen, just over-communicate.&lt;/p>
&lt;p>Try to provide the opportunity for someone else to do the work, and over-communicate with them as much as possible to make sure that they are learning whatever it is that they need to learn. If neither you or the other person knows what's going on, then I can over-communicate, so someone hopefully will see your messages and come to the rescue. That happens a lot. There's a lot of really nice and kind people who will come out and tell you how something works, help you fix it.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: If you were to sum up your experience running this release, what would it be?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: It's been super fun and a little bit stressing, to be honest. Being the release lead is definitely amazing. You're kind of sitting at the center of Kubernetes.&lt;/p>
&lt;p>You not only see the people who are working on things— the things that are broken, and the users filling out issues, and saying what broke, and the like. But you also get the opportunity to work with a lot of people who do a lot of non-code related work. Docs is one of the most obvious things. There's a lot of work that goes into communications, contributor experience, public relations.&lt;/p>
&lt;p>And being connected, getting to talk with those people mostly every other day, it's really fun. It's a really good experience in terms of becoming a better contributor to the community, but also taking some of that knowledge home with you and applying it somewhere else. If you are a software engineer, if you are a project manager, whatever, it's amazing how much you can learn.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: I know the community likes to rotate around who are the release leads. But if you were given the opportunity to be a release lead for a future release of Kubernetes, would you do it again?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Yeah, it's a fun job. To be honest, it can be really stressing. Especially, as I mentioned, at some point, most of that work is just going to be talking with people, and talking requires a lot more thought and effort than just sitting down and thinking about things sometimes. And some of that can be really stressful.&lt;/p>
&lt;p>But the job itself, it is definitely fun. And at some distant point in the future, if for some reason it was a possibility, I will think about it. But definitely, as you mentioned, one thing that we try to do is cycle out, because I can have fun in it, and that's all good and nice. And hopefully I can help another release go out the door. But providing the opportunity for other people to learn I think is a lot more important than just being the lead itself.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/alejandrox135">Jorge Alarcón&lt;/a> is a site reliability engineer with Searchable AI and served as the Kubernetes 1.18 release team lead.&lt;/em>&lt;/p>
&lt;p>&lt;em>You can find the &lt;a href="http://www.kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> at &lt;a href="https://twitter.com/KubernetesPod">@KubernetesPod&lt;/a> on Twitter, and you can &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe&lt;/a> so you never miss an episode.&lt;/em>&lt;/p></description></item><item><title>Blog: Music and math: the Kubernetes 1.17 release interview</title><link>https://kubernetes.io/blog/2020/07/27/music-and-math-the-kubernetes-1.17-release-interview/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/07/27/music-and-math-the-kubernetes-1.17-release-interview/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Adam Glick (Google)&lt;/p>
&lt;p>Every time the Kubernetes release train stops at the station, we like to ask the release lead to take a moment to reflect on their experience. That takes the form of an interview on the weekly &lt;a href="https://kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> that I co-host with &lt;a href="https://twitter.com/craigbox">Craig Box&lt;/a>. If you're not familiar with the show, every week we summarise the new in the Cloud Native ecosystem, and have an insightful discussion with an interesting guest from the broader Kubernetes community.&lt;/p>
&lt;p>At the time of the 1.17 release in December, we &lt;a href="https://kubernetespodcast.com/episode/083-kubernetes-1.17/">talked to release team lead Guinevere Saenger&lt;/a>. We have &lt;a href="https://kubernetes.io/blog/2018/07/16/how-the-sausage-is-made-the-kubernetes-1.11-release-interview-from-the-kubernetes-podcast/">shared&lt;/a> &lt;a href="https://kubernetes.io/blog/2019/05/13/cat-shirts-and-groundhog-day-the-kubernetes-1.14-release-interview/">the&lt;/a> &lt;a href="https://kubernetes.io/blog/2019/12/06/when-youre-in-the-release-team-youre-family-the-kubernetes-1.16-release-interview/">transcripts&lt;/a> of previous interviews on the Kubernetes blog, and we're very happy to share another today.&lt;/p>
&lt;p>Next week we will bring you up to date with the story of Kubernetes 1.18, as we gear up for the release of 1.19 next month. &lt;a href="https://kubernetespodcast.com/subscribe/">Subscribe to the show&lt;/a> wherever you get your podcasts to make sure you don't miss that chat!&lt;/p>
&lt;hr>
&lt;p>&lt;strong>ADAM GLICK: You have a nontraditional background for someone who works as a software engineer. Can you explain that background?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: My first career was as a &lt;a href="https://en.wikipedia.org/wiki/Collaborative_piano">collaborative pianist&lt;/a>, which is an academic way of saying &amp;quot;piano accompanist&amp;quot;. I was a classically trained pianist who spends most of her time onstage, accompanying other people and making them sound great.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Is that the piano equivalent of pair-programming?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: No one has said it to me like that before, but all sorts of things are starting to make sense in my head right now. I think that's a really great way of putting it.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: That's a really interesting background, as someone who also has a background with music. What made you decide to get into software development?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I found myself in a life situation where I needed more stable source of income, and teaching music, and performing for various gig opportunities, was really just not cutting it anymore. And I found myself to be working really, really hard with not much to show for it. I had a lot of friends who were software engineers. I live in Seattle. That's sort of a thing that happens to you when you live in Seattle — you get to know a bunch of software engineers, one way or the other.&lt;/p>
&lt;p>The ones I met were all lovely people, and they said, hey, I'm happy to show you how to program in Python. And so I did that for a bit, and then I heard about this program called &lt;a href="https://adadevelopersacademy.org/">Ada Developers Academy&lt;/a>. That's a year long coding school, targeted at women and non-binary folks that are looking for a second career in tech. And so I applied for that.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What can you tell us about that program?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: It's incredibly selective, for starters. It's really popular in Seattle and has gotten quite a good reputation. It took me three tries to get in. They do two classes a year, and so it was a while before I got my response saying 'congratulations, we are happy to welcome you into Cohort 6'. I think what sets Ada Developers Academy apart from other bootcamp style coding programs are three things, I think? The main important one is that if you get in, you pay no tuition. The entire program is funded by company sponsors.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Right.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: The other thing that really convinced me is that five months of the 11-month program are an industry internship, which means you get both practical experience, mentorship, and potential job leads at the end of it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: So very much like a condensed version of the University of Waterloo degree, where you do co-op terms.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Interesting. I didn't know about that.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Having lived in Waterloo for a while, I knew a lot of people who did that. But what would you say the advantages were of going through such a condensed schooling process in computer science?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I'm not sure that the condensed process is necessarily an advantage. I think it's a necessity, though. People have to quit their jobs to go do this program. It's not an evening school type of thing.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Right.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: And your internship is basically a full-time job when you do it. One thing that Ada was really, really good at is giving us practical experience that directly relates to the workplace. We learned how to use Git. We learned how to design websites using &lt;a href="https://rubyonrails.org/">Rails&lt;/a>. And we also learned how to collaborate, how to pair-program. We had a weekly retrospective, so we sort of got a soft introduction to workflows at a real workplace. Adding to that, the internship, and I think the overall experience is a little bit more 'practical workplace oriented' and a little bit less academic.&lt;/p>
&lt;p>When you're done with it, you don't have to relearn how to be an adult in a working relationship with other people. You come with a set of previous skills. There are Ada graduates who have previously been campaign lawyers, and veterinarians, and nannies, cooks, all sorts of people. And it turns out these skills tend to translate, and they tend to matter.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: With your background in music, what do you think that that allows you to bring to software development that could be missing from, say, standard software development training that people go through?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: People tend to really connect the dots when I tell them I used to be a musician. Of course, I still consider myself a musician, because you don't really ever stop being a musician. But they say, 'oh, yeah, music and math', and that's just a similar sort of brain. And that makes so much sense. And I think there's a little bit of a point to that. When you learn a piece of music, you have to start recognizing patterns incredibly quickly, almost intuitively.&lt;/p>
&lt;p>And I think that is the main skill that translates into programming— recognizing patterns, finding the things that work, finding the things that don't work. And for me, especially as a collaborative pianist, it's the communicating with people, the finding out what people really want, where something is going, how to figure out what the general direction is that we want to take, before we start writing the first line of code.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: In your experience at Ada or with other experiences you've had, have you been able to identify patterns in other backgrounds for people that you'd recommend, 'hey, you're good at music, so therefore you might want to consider doing something like a course in computer science'?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Overall, I think ultimately writing code is just giving a set of instructions to a computer. And we do that in daily life all the time. We give instructions to our kids, we give instructions to our students. We do math, we write textbooks. We give instructions to a room full of people when you're in court as a lawyer.&lt;/p>
&lt;p>Actually, the entrance exam to Ada Developers Academy used to have questions from the &lt;a href="https://en.wikipedia.org/wiki/Law_School_Admission_Test">LSAT&lt;/a> on it to see if you were qualified to join the program. They changed that when I applied, but I think that's a thing that happened at one point. So, overall, I think software engineering is a much more varied field than we give it credit for, and that there are so many ways in which you can apply your so-called other skills and bring them under the umbrella of software engineering.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I do think that programming is effectively half art and half science. There's creativity to be applied. There is perhaps one way to solve a problem most efficiently. But there are many different ways that you can choose to express how you compiled something down to that way.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Yeah, I mean, that's definitely true. I think one way that you could probably prove that is that if you write code at work and you're working on something with other people, you can probably tell which one of your co-workers wrote which package, just by the way it's written, or how it is documented, or how it is styled, or any of those things. I really do think that the human character shines through.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What got you interested in Kubernetes and open source?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: The honest answer is absolutely nothing. Going back to my programming school— and remember that I had to do a five-month internship as part of my training— the way that the internship works is that sponsor companies for the program get interns in according to how much they sponsored a specific cohort of students.&lt;/p>
&lt;p>So at the time, Samsung and SDS offered to host two interns for five months on their &lt;a href="https://samsung-cnct.github.io/">Cloud Native Computing team&lt;/a> and have that be their practical experience. So I go out of a Ruby on Rails full stack web development bootcamp and show up at my internship, and they said, &amp;quot;Welcome to Kubernetes. Try to bring up a cluster.&amp;quot; And I said, &amp;quot;Kuber what?&amp;quot;&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: We've all said that on occasion.&lt;/strong>&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Trial by fire, wow.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I will say that that entire team was absolutely wonderful, delightful to work with, incredibly helpful. And I will forever be grateful for all of the help and support that I got in that environment. It was a great place to learn.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You now work on GitHub's Kubernetes infrastructure. Obviously, there was GitHub before there was a Kubernetes, so a migration happened. What can you tell us about the transition that GitHub made to running on Kubernetes?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: A disclaimer here— I was not at GitHub at the time that the transition to Kubernetes was made. However, to the best of my knowledge, the decision to transition to Kubernetes was made and people decided, yes, we want to try Kubernetes. We want to use Kubernetes. And mostly, the only decision left was, which one of our applications should we move over to Kubernetes?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I thought GitHub was written on Rails, so there was only one application.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: [LAUGHING] We have a lot of supplementary stuff under the covers.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I'm sure.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: But yes, GitHub is written in Rails. It is still written in Rails. And most of the supplementary things are currently running on Kubernetes. We have a fair bit of stuff that currently does not run on Kubernetes. Mainly, that is GitHub Enterprise related things. I would know less about that because I am on the platform team that helps people use the Kubernetes infrastructure. But back to your question, leadership at the time decided that it would be a good idea to start with GitHub the Rails website as the first project to move to Kubernetes.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: High stakes!&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: The reason for this was that they decided if they were going to not start big, it really wasn't going to transition ever. It was really not going to happen. So they just decided to go all out, and it was successful, for which I think the lesson would probably be commit early, commit big.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Are there any other lessons that you would take away or that you've learned kind of from the transition that the company made, and might be applicable to other people who are looking at moving their companies from a traditional infrastructure to a Kubernetes infrastructure?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I'm not sure this is a lesson specifically, but I was on support recently, and it turned out that, due to unforeseen circumstances and a mix of human error, a bunch of the namespaces on one of our Kubernetes clusters got deleted.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Oh, my.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: It should not have affected any customers, I should mention, at this point. But all in all, it took a few of us a few hours to almost completely recover from this event. I think that, without Kubernetes, this would not have been possible.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Generally, deleting something like that is quite catastrophic. We've seen a number of other vendors suffer large outages when someone's done something to that effect, which is why we get &lt;a href="https://twitter.com/hashtag/hugops">#hugops&lt;/a> on Twitter all the time.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: People did send me #hugops, that is a thing that happened. But overall, something like this was an interesting stress test and sort of proved that it wasn't nearly as catastrophic as a worst case scenario.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: GitHub &lt;a href="https://githubengineering.com/githubs-metal-cloud/">runs its own data centers&lt;/a>. Kubernetes was largely built for running on the cloud, but a lot of people do choose to run it on their own, bare metal. How do you manage clusters and provisioning of the machinery you run?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: When I started, my onboarding project was to deprovision an old cluster, make sure all the traffic got moved to somewhere where it would keep running, provision a new cluster, and then move website traffic onto the new cluster. That was a really exciting onboarding project. At the time, we provisioned bare metal machines using Puppet. We still do that to a degree, but I believe the team that now runs our computing resources actually inserts virtual machines as an extra layer between the bare metal and the Kubernetes nodes.&lt;/p>
&lt;p>Again, I was not intrinsically part of that decision, but my understanding is that it just makes for a greater reliability and reproducibility across the board. We've had some interesting hardware dependency issues come up, and the virtual machines basically avoid those.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You've been working with Kubernetes for a couple of years now. How did you get involved in the release process?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: When I first started in the project, I started at the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-contributor-experience#readme">special interest group for contributor experience&lt;/a>, namely because one of my co-workers at the time, Aaron Crickenberger, was a big Kubernetes community person. Still is.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: We've &lt;a href="https://kubernetespodcast.com/episode/046-kubernetes-1.14/">had him on the show&lt;/a> for one of these very release interviews!&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: In fact, this is true! So Aaron and I actually go way back to Samsung SDS. Anyway, Aaron suggested that I should write up a contribution to the Kubernetes project, and I said, me? And he said, yes, of course. You will be &lt;a href="https://www.youtube.com/watch?v=TkCDUFR6xqw">speaking at KubeCon&lt;/a>, so you should probably get started with a PR or something. So I tried, and it was really, really hard. And I complained about it &lt;a href="https://github.com/kubernetes/community/issues/141">in a public GitHub issue&lt;/a>, and people said, yeah. Yeah, we know it's hard. Do you want to help with that?&lt;/p>
&lt;p>And so I started getting really involved with the &lt;a href="https://github.com/kubernetes/community/tree/master/contributors/guide">process for new contributors to get started&lt;/a> and have successes, kind of getting a foothold into a project that's as large and varied as Kubernetes. From there on, I began to talk to people, get to know people. The great thing about the Kubernetes community is that there is so much mentorship to go around.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Right.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: There are so many friendly people willing to help. It's really funny when I talk to other people about it. They say, what do you mean, your coworker? And I said, well, he's really a colleague. He really works for another company.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: He's sort-of officially a competitor.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Yeah.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: But we're friends.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: But he totally helped me when I didn't know how to git patch my borked pull request. So that happened. And eventually, somebody just suggested that I start following along in the release process and shadow someone on their release team role. And that, at the time, was Tim Pepper, who was bug triage lead, and I shadowed him for that role.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Another &lt;a href="https://kubernetespodcast.com/episode/010-kubernetes-1.11/">podcast guest&lt;/a> on the interview train.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: This is a pattern that probably will make more sense once I explain to you about the shadow process of the release team.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Well, let's turn to the Kubernetes release and the release process. First up, what's new in this release of 1.17?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: We have only a very few new things. The one that I'm most excited about is that we have moved &lt;a href="https://github.com/kubernetes/enhancements/issues/563">IPv4 and IPv6 dual stack&lt;/a> support to alpha. That is the most major change, and it has been, I think, a year and a half in coming. So this is the very first cut of that feature, and I'm super excited about that.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The people who have been promised IPv6 for many, many years and still don't really see it, what will this mean for them?&lt;/strong>&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: And most importantly, why did we skip IPv5 support?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I don't know!&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Please see &lt;a href="https://softwareengineering.stackexchange.com/questions/185380/ipv4-to-ipv6-where-is-ipv5">the appendix to this podcast&lt;/a> for technical explanations.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Having a dual stack configuration obviously enables people to have a much more flexible infrastructure and not have to worry so much about making decisions that will become outdated or that may be over-complicated. This basically means that pods can have dual stack addresses, and nodes can have dual stack addresses. And that basically just makes communication a lot easier.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What about features that didn't make it into the release? We had a conversation with Lachie in the &lt;a href="https://kubernetespodcast.com/episode/072-kubernetes-1.16/">1.16 interview&lt;/a>, where he mentioned &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/sidecarcontainers.md">sidecar containers&lt;/a>. They unfortunately didn't make it into that release. And I see now that they haven't made this one either.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: They have not, and we are actually currently undergoing an effort of tracking features that flip multiple releases.&lt;/p>
&lt;p>As a community, we need everyone's help. There are a lot of features that people want. There is also a lot of cleanup that needs to happen. And we have started talking at previous KubeCons repeatedly about problems with maintainer burnout, reviewer burnout, have a hard time finding reviews for your particular contributions, especially if you are not an entrenched member of the community. And it has become very clear that this is an area where the entire community needs to improve.&lt;/p>
&lt;p>So the unfortunate reality is that sometimes life happens, and people are busy. This is an open source project. This is not something that has company mandated OKRs. Particularly during the fourth quarter of the year in North America, but around the world, we have a lot of holidays. It is the end of the year. Kubecon North America happened as well. This makes it often hard to find a reviewer in time or to rally the support that you need for your enhancement proposal. Unfortunately, slipping releases is fairly common and, at this point, expected. We started out with having 42 enhancements and &lt;a href="https://docs.google.com/spreadsheets/d/1ebKGsYB1TmMnkx86bR2ZDOibm5KWWCs_UjV3Ys71WIs/edit#gid=0">landed with roughly half of that&lt;/a>.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I was going to ask about the truncated schedule due to the fourth quarter of the year, where there are holidays in large parts of the world. Do you find that the Q4 release on the whole is smaller than others, if not for the fact that it's some week shorter?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Q4 releases are shorter by necessity because we are trying to finish the final release of the year before the end of the year holidays. Often, releases are under pressure of KubeCons, during which finding reviewers or even finding the time to do work can be hard to do, if you are attending. And even if you're not attending, your reviewers might be attending.&lt;/p>
&lt;p>It has been brought up last year to make the final release more of a stability release, meaning no new alpha features. In practice, for this release, this is actually quite close to the truth. We have four features graduating to beta and most of our features are graduating to stable. I am hoping to use this as a precedent to change our process to make the final release a stability release from here on out. The timeline fits. The past experience fits this model.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: On top of all of the release work that was going on, there was also KubeCon that happened. And you were involved in the &lt;a href="https://github.com/kubernetes/community/tree/master/events/2019/11-contributor-summit">contributor summit&lt;/a>. How was the summit?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: This was the first contributor summit where we had an organized events team with events organizing leads, and handbooks, and processes. And I have heard from multiple people— this is just word of mouth— that it was their favorite contributor summit ever.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Was someone allocated to hat production? &lt;a href="https://flickr.com/photos/143247548@N03/49093218951/">Everyone had sailor hats&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Yes, the entire event staff had sailor hats with their GitHub handle on them, and it was pretty fantastic. You can probably see me wearing one in some of the pictures from the contributor summit. That literally was something that was pulled out of a box the morning of the contributor summit, and no one had any idea. But at first, I was a little skeptical, but then I put it on and looked at myself in the mirror. And I was like, yes. Yes, this is accurate. We should all wear these.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Did getting everyone together for the contributor summit help with the release process?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: It did not. It did quite the opposite, really. Well, that's too strong.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Is that just a matter of the time taken up?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: It's just a completely different focus. Honestly, it helped getting to know people face-to-face that I had currently only interacted with on video. But we did have to cancel the release team meeting the day of the contributor summit because there was kind of no sense in having it happen. We moved it to the Tuesday, I believe.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The role of the release team leader has been described as servant leadership. Do you consider the position proactive or reactive?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Honestly, I think that depends on who's the release team lead, right? There are some people who are very watchful and look for trends, trying to detect problems before they happen. I tend to be in that camp, but I also know that sometimes it's not possible to predict things. There will be last minute bugs sometimes, sometimes not. If there is a last minute bug, you have to be ready to be on top of that. So for me, the approach has been I want to make sure that I have my priorities in order and also that I have backups in case I can't be available.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What was the most interesting part of the release process for you?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: A release lead has to have served in other roles on the release team prior to being release team lead. To me, it was very interesting to see what other roles were responsible for, ones that I hadn't seen from the inside before, such as docs, CI signal. I had helped out with CI signal for a bit, but I want to give a big shout out to CI signal lead, Alena Varkockova, who was able to communicate effectively and kindly with everyone who was running into broken tests, failing tests. And she was very effective in getting all of our tests up and running.&lt;/p>
&lt;p>So that was actually really cool to see. And yeah, just getting to see more of the workings of the team, for me, it was exciting. The other big exciting thing, of course, was to see all the changes that were going in and all the efforts that were being made.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The release lead for 1.18 has just been announced as &lt;a href="https://twitter.com/alejandrox135">Jorge Alarcon&lt;/a>. What are you going to put in the proverbial envelope as advice for him?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I would want Jorge to be really on top of making sure that every Special Interest Group that enters a change, that has an enhancement for 1.18, is on top of the timelines and is responsive. Communication tends to be a problem. And I had hinted at this earlier, but some enhancements slipped simply because there wasn't enough reviewer bandwidth.&lt;/p>
&lt;p>Greater communication of timelines and just giving people more time and space to be able to get in their changes, or at least, seemingly give them more time and space by sending early warnings, is going to be helpful. Of course, he's going to have a slightly longer release, too, than I did. This might be related to a unique Q4 challenge. Overall, I would encourage him to take more breaks, to rely more on his release shadows, and split out the work in a fashion that allows everyone to have a turn and everyone to have a break as well.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What would your advice be to someone who is hearing your experience and is inspired to get involved with the Kubernetes release or contributer process?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Those are two separate questions. So let me tackle the Kubernetes release question first. Kubernetes &lt;a href="https://github.com/kubernetes/sig-release/#readme">SIG Release&lt;/a> has, in my opinion, a really excellent onboarding program for new members. We have what is called the &lt;a href="https://github.com/kubernetes/sig-release/blob/master/release-team/shadows.md">Release Team Shadow Program&lt;/a>. We also have the Release Engineering Shadow Program, or the Release Management Shadow Program. Those are two separate subprojects within SIG Release. And each subproject has a team of roles, and each role can have two to four shadows that are basically people who are part of that role team, and they are learning that role as they are doing it.&lt;/p>
&lt;p>So for example, if I am the lead for bug triage on the release team, I may have two, three or four people that I closely work with on the bug triage tasks. These people are my shadows. And once they have served one release cycle as a shadow, they are now eligible to be lead in that role. We have an application form for this process, and it should probably be going up in January. It usually happens the first week of the release once all the release leads are put together.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Do you think being a member of the release team is something that is a good first contribution to the Kubernetes project overall?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: It depends on what your goals are, right? I believe so. I believe, for me, personally, it has been incredibly helpful looking into corners of the project that I don't know very much about at all, like API machinery, storage. It's been really exciting to look over all the areas of code that I normally never touch.&lt;/p>
&lt;p>It depends on what you want to get out of it. In general, I think that being a release team shadow is a really, really great on-ramp to being a part of the community because it has a paved path solution to contributing. All you have to do is show up to the meetings, ask questions of your lead, who is required to answer those questions.&lt;/p>
&lt;p>And you also do real work. You really help, you really contribute. If you go across the issues and pull requests in the repo, you will see, 'Hi, my name is so-and-so. I am shadowing the CI signal lead for the current release. Can you help me out here?' And that's a valuable contribution, and it introduces people to others. And then people will recognize your name. They'll see a pull request by you, and they're like oh yeah, I know this person. They're legit.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/guincodes">Guinevere Saenger&lt;/a> is a software engineer for GitHub and served as the Kubernetes 1.17 release team lead.&lt;/em>&lt;/p>
&lt;p>&lt;em>You can find the &lt;a href="http://www.kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> at &lt;a href="https://twitter.com/KubernetesPod">@KubernetesPod&lt;/a> on Twitter, and you can &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe&lt;/a> so you never miss an episode.&lt;/em>&lt;/p></description></item><item><title>Blog: SIG-Windows Spotlight</title><link>https://kubernetes.io/blog/2020/06/30/sig-windows-spotlight-2020/</link><pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/06/30/sig-windows-spotlight-2020/</guid><description>
&lt;p>&lt;em>This post tells the story of how Kubernetes contributors work together to provide a container orchestrator that works for both Linux and Windows.&lt;/em>&lt;/p>
&lt;img alt="Image of a computer with Kubernetes logo" width="30%" src="KubernetesComputer_transparent.png">
&lt;p>Most people who are familiar with Kubernetes are probably used to associating it with Linux. The connection makes sense, since Kubernetes ran on Linux from its very beginning. However, many teams and organizations working on adopting Kubernetes need the ability to orchestrate containers on Windows. Since the release of Docker and rise to popularity of containers, there have been efforts both from the community and from Microsoft itself to make container technology as accessible in Windows systems as it is in Linux systems.&lt;/p>
&lt;p>Within the Kubernetes community, those who are passionate about making Kubernetes accessible to the Windows community can find a home in the Windows Special Interest Group. To learn more about SIG-Windows and the future of Kubernetes on Windows, I spoke to co-chairs &lt;a href="https://github.com/marosset">Mark Rossetti&lt;/a> and &lt;a href="https://github.com/michmike">Michael Michael&lt;/a> about the SIG's goals and how others can contribute.&lt;/p>
&lt;h2 id="intro-to-windows-containers-kubernetes">Intro to Windows Containers &amp;amp; Kubernetes&lt;/h2>
&lt;p>Kubernetes is the most popular tool for orchestrating container workloads, so to understand the Windows Special Interest Group (SIG) within the Kubernetes project, it's important to first understand what we mean when we talk about running containers on Windows.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&amp;quot;When looking at Windows support in Kubernetes,&amp;quot; says SIG (Special Interest Group) Co-chairs Mark Rossetti and Michael Michael, &amp;quot;many start drawing comparisons to Linux containers. Although some of the comparisons that highlight limitations are fair, it is important to distinguish between operational limitations and differences between the Windows and Linux operating systems. Windows containers run the Windows operating system and Linux containers run Linux.&amp;quot;&lt;/em>&lt;/p>
&lt;hr>
&lt;p>In essence, any &amp;quot;container&amp;quot; is simply a process being run on its host operating system, with some key tooling in place to isolate that process and its dependencies from the rest of the environment. The goal is to make that running process safely isolated, while taking up minimal resources from the system to perform that isolation. On Linux, the tooling used to isolate processes to create &amp;quot;containers&amp;quot; commonly boils down to cgroups and namespaces (among a few others), which are themselves tools built in to the Linux Kernel.&lt;/p>
&lt;img alt="A visual analogy using dogs to explain Linux cgroups and namespaces." width="40%" src="cgroupsNamespacesComboPic.png">
&lt;h4 id="if-dogs-were-processes-containerization-would-be-like-giving-each-dog-their-own-resources-like-toys-and-food-using-cgroups-and-isolating-troublesome-dogs-using-namespaces">&lt;em>If dogs were processes: containerization would be like giving each dog their own resources like toys and food using cgroups, and isolating troublesome dogs using namespaces.&lt;/em>&lt;/h4>
&lt;p>Native Windows processes are processes that are or must be run on a Windows operating system. This makes them fundamentally different from a process running on a Linux operating system. Since Linux containers are Linux processes being isolated by the Linux kernel tools known as cgroups and namespaces, containerizing native Windows processes meant implementing similar isolation tools within the Windows kernel itself. Thus, &amp;quot;Windows Containers&amp;quot; and &amp;quot;Linux Containers&amp;quot; are fundamentally different technologies, even though they have the same goals (isolating processes) and in some ways work similarly (using kernel level containerization).&lt;/p>
&lt;p>So when it comes to running containers on Windows, there are actually two very important concepts to consider:&lt;/p>
&lt;ul>
&lt;li>Native Windows processes running as native Windows Server style containers,&lt;/li>
&lt;li>and traditional Linux containers running on a Linux Kernel, generally hosted on a lightweight Hyper-V Virtual Machine.&lt;/li>
&lt;/ul>
&lt;p>You can learn more about Linux and Windows containers in this &lt;a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/linux-containers">tutorial&lt;/a> from Microsoft.&lt;/p>
&lt;h3 id="kubernetes-on-windows">Kubernetes on Windows&lt;/h3>
&lt;p>Kubernetes was initially designed with Linux containers in mind and was itself designed to run on Linux systems. Because of that, much of the functionality of Kubernetes involves unique Linux functionality. The Linux-specific work is intentional--we all want Kubernetes to run optimally on Linux--but there is a growing demand for similar optimization for Windows servers. For cases where users need container orchestration on Windows, the Kubernetes contributor community of SIG-Windows has incorporated functionality for Windows-specific use cases.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&amp;quot;A common question we get is, will I be able to have a Windows-only cluster. The answer is NO. Kubernetes control plane components will continue to be based on Linux, while SIG-Windows is concentrating on the experience of having Windows worker nodes in a Kubernetes cluster.&amp;quot;&lt;/em>&lt;/p>
&lt;hr>
&lt;p>Rather than separating out the concepts of &amp;quot;Windows Kubernetes,&amp;quot; and &amp;quot;Linux Kubernetes,&amp;quot; the community of SIG-Windows works toward adding functionality to the main Kubernetes project which allows it to handle use cases for Windows. These Windows capabilities mirror, and in some cases add unique functionality to, the Linux use cases Kubernetes has served since its release in 2014 (want to learn more history? Scroll through this &lt;a href="https://github.com/kubernetes/kubernetes/blob/e2b948dbfbba62b8cb681189377157deee93bb43/DESIGN.md">original design document&lt;/a>.&lt;/p>
&lt;h2 id="what-does-sig-windows-do">What Does SIG-Windows Do?&lt;/h2>
&lt;hr>
&lt;p>&lt;em>&amp;quot;SIG-Windows is really the center for all things Windows in Kubernetes,&amp;quot;&lt;/em> SIG chairs Mark and Michael said, &lt;em>&amp;quot;We mainly focus on the compute side of things, but really anything related to running Kubernetes on Windows is in scope for SIG-Windows.&amp;quot;&lt;/em>&lt;/p>
&lt;hr>
&lt;p>In order to best serve users, SIG-Windows works to make the Kubernetes user experience as consistent as possible for users of Windows and Linux. However some use cases simply only apply to one Operating System, and as such, the SIG-Windows group also works to create functionality that is unique to Windows-only workloads.&lt;/p>
&lt;p>Many SIGs, or &amp;quot;Special Interest Groups&amp;quot; within Kubernetes have a narrow focus, allowing members to dive deep on a certain facet of the technology. While specific expertise is welcome, those interested in SIG-Windows will find it to be a great community to build broad understanding across many focus areas of Kubernetes. &amp;quot;Members from our SIG interface with storage, network, testing, cluster-lifecycle and others groups in Kubernetes.&amp;quot;&lt;/p>
&lt;h3 id="who-are-sig-windows-users">Who are SIG-Windows' Users?&lt;/h3>
&lt;p>The best way to understand the technology a group makes, is often to understand who their customers or users are.&lt;/p>
&lt;h4 id="a-majority-of-the-users-we-ve-interacted-with-have-business-critical-infrastructure-running-on-windows-developed-over-many-years-and-can-t-move-those-workloads-to-linux-for-various-reasons-cost-time-compliance-etc-the-sig-chairs-shared-by-transporting-those-workloads-into-windows-containers-and-running-them-in-kubernetes-they-are-able-to-quickly-modernize-their-infrastructure-and-help-migrate-it-to-the-cloud">&amp;quot;A majority of the users we've interacted with have business-critical infrastructure running on Windows developed over many years and can't move those workloads to Linux for various reasons (cost, time, compliance, etc),&amp;quot; the SIG chairs shared. &amp;quot;By transporting those workloads into Windows containers and running them in Kubernetes they are able to quickly modernize their infrastructure and help migrate it to the cloud.&amp;quot;&lt;/h4>
&lt;p>As anyone in the Kubernetes space can attest, companies around the world, in many different industries, see Kubernetes as their path to modernizing their infrastructure. Often this involves re-architecting or event totally re-inventing many of the ways they've been doing business. With the goal being to make their systems more scalable, more robust, and more ready for anything the future may bring. But not every application or workload can or should change the core operating system it runs on, so many teams need the ability to run containers at scale on Windows, or Linux, or both.&lt;/p>
&lt;p>&amp;quot;Sometimes the driver to Windows containers is a modernization effort and sometimes it’s because of expiring hardware warranties or end-of-support cycles for the current operating system. Our efforts in SIG-Windows enable Windows developers to take advantage of cloud native tools and Kubernetes to build and deploy distributed applications faster. That’s exciting! In essence, users can retain the benefits of application availability while decreasing costs.&amp;quot;&lt;/p>
&lt;h2 id="who-are-sig-windows">Who are SIG-Windows?&lt;/h2>
&lt;p>Who are these contributors working on enabling Windows workloads for Kubernetes? It could be you!&lt;/p>
&lt;p>Like with other Kubernetes SIGs, contributors to SIG-Windows can be anyone from independent hobbyists to professionals who work at many different companies. They come from many different parts of the world and bring to the table many different skill sets.&lt;/p>
&lt;img alt="Image of several people chatting pleasantly" width="30%" src="PeopleDoodle_transparent.png">
&lt;p>&lt;em>&amp;quot;Like most other Kubernetes SIGs, we are a very welcome and open community,&amp;quot; explained the SIG co-chairs Michael Michael and Mark Rosetti.&lt;/em>&lt;/p>
&lt;h3 id="becoming-a-contributor">Becoming a contributor&lt;/h3>
&lt;p>For anyone interested in getting started, the co-chairs added, &amp;quot;New contributors can view old community meetings on GitHub (we record every single meeting going back three years), read our documentation, attend new community meetings, ask questions in person or on Slack, and file some issues on Github. We also attend all KubeCon conferences and host 1-2 sessions, a contributor session, and meet-the-maintainer office hours.&amp;quot;&lt;/p>
&lt;p>The co-chairs also shared a glimpse into what the path looks like to becoming a member of the SIG-Windows community:&lt;/p>
&lt;p>&amp;quot;We encourage new contributors to initially just join our community and listen, then start asking some questions and get educated on Windows in Kubernetes. As they feel comfortable, they could graduate to improving our documentation, file some bugs/issues, and eventually they can be a code contributor by fixing some bugs. If they have long-term and sustained substantial contributions to Windows, they could become a technical lead or a chair of SIG-Windows. You won't know if you love this area unless you get started :) To get started, &lt;a href="https://github.com/kubernetes/community/tree/master/sig-windows">visit this getting-started page&lt;/a>. It's a one stop shop with links to everything related to SIG-Windows in Kubernetes.&amp;quot;&lt;/p>
&lt;p>When asked if there were any useful skills for new contributors, the co-chairs said,&lt;/p>
&lt;p>&amp;quot;We are always looking for expertise in Go and Networking and Storage, along with a passion for Windows. Those are huge skills to have. However, we don’t require such skills, and we welcome any and all contributors, with varying skill sets. If you don’t know something, we will help you acquire it.&amp;quot;&lt;/p>
&lt;p>You can get in touch with the folks at SIG-Windows in their &lt;a href="https://kubernetes.slack.com/archives/C0SJ4AFB7">Slack channel&lt;/a> or attend one of their regular meetings - currently 30min long on Tuesdays at 12:30PM EST! You can find links to their regular meetings as well as past meeting notes and recordings from the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-windows#readme">SIG-Windows README&lt;/a> on GitHub.&lt;/p>
&lt;p>As a closing message from SIG-Windows:&lt;/p>
&lt;hr>
&lt;h4 id="we-welcome-you-to-get-involved-and-join-our-community-to-share-feedback-and-deployment-stories-and-contribute-to-code-docs-and-improvements-of-any-kind">&lt;em>&amp;quot;We welcome you to get involved and join our community to share feedback and deployment stories, and contribute to code, docs, and improvements of any kind.&amp;quot;&lt;/em>&lt;/h4>
&lt;hr></description></item><item><title>Blog: Working with Terraform and Kubernetes</title><link>https://kubernetes.io/blog/2020/06/working-with-terraform-and-kubernetes/</link><pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/06/working-with-terraform-and-kubernetes/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> &lt;a href="https://twitter.com/pst418">Philipp Strube&lt;/a>, Kubestack&lt;/p>
&lt;p>Maintaining Kubestack, an open-source &lt;a href="https://www.kubestack.com/lp/terraform-gitops-framework">Terraform GitOps Framework&lt;/a> for Kubernetes, I unsurprisingly spend a lot of time working with Terraform and Kubernetes. Kubestack provisions managed Kubernetes services like AKS, EKS and GKE using Terraform but also integrates cluster services from Kustomize bases into the GitOps workflow. Think of cluster services as everything that's required on your Kubernetes cluster, before you can deploy application workloads.&lt;/p>
&lt;p>Hashicorp recently announced &lt;a href="https://www.hashicorp.com/blog/deploy-any-resource-with-the-new-kubernetes-provider-for-hashicorp-terraform/">better integration between Terraform and Kubernetes&lt;/a>. I took this as an opportunity to give an overview of how Terraform can be used with Kubernetes today and what to be aware of.&lt;/p>
&lt;p>In this post I will however focus only on using Terraform to provision Kubernetes API resources, not Kubernetes clusters.&lt;/p>
&lt;p>&lt;a href="https://www.terraform.io/intro/index.html">Terraform&lt;/a> is a popular infrastructure as code solution, so I will only introduce it very briefly here. In a nutshell, Terraform allows declaring a desired state for resources as code, and will determine and execute a plan to take the infrastructure from its current state, to the desired state.&lt;/p>
&lt;p>To be able to support different resources, Terraform requires providers that integrate the respective API. So, to create Kubernetes resources we need a Kubernetes provider. Here are our options:&lt;/p>
&lt;h2 id="terraform-kubernetes-provider-official">Terraform &lt;code>kubernetes&lt;/code> provider (official)&lt;/h2>
&lt;p>First, the &lt;a href="https://github.com/hashicorp/terraform-provider-kubernetes">official Kubernetes provider&lt;/a>. This provider is undoubtedly the most mature of the three. However, it comes with a big caveat that's probably the main reason why using Terraform to maintain Kubernetes resources is not a popular choice.&lt;/p>
&lt;p>Terraform requires a schema for each resource and this means the maintainers have to translate the schema of each Kubernetes resource into a Terraform schema. This is a lot of effort and was the reason why for a long time the supported resources where pretty limited. While this has improved over time, still not everything is supported. And especially &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resources&lt;/a> are not possible to support this way.&lt;/p>
&lt;p>This schema translation also results in some edge cases to be aware of. For example, &lt;code>metadata&lt;/code> in the Terraform schema is a list of maps. Which means you have to refer to the &lt;code>metadata.name&lt;/code> of a Kubernetes resource like this in Terraform: &lt;code>kubernetes_secret.example.metadata.0.name&lt;/code>.&lt;/p>
&lt;p>On the plus side however, having a Terraform schema means full integration between Kubernetes and other Terraform resources. Like for &lt;a href="https://github.com/kbst/terraform-kubestack/blob/e5caa6d20926d546a045144ebe79c7cc8c0b4c8a/aws/_modules/eks/ingress.tf#L37">example&lt;/a>, using Terraform to create a Kubernetes service of type &lt;code>LoadBalancer&lt;/code> and then use the returned ELB hostname in a Route53 record to configure DNS.&lt;/p>
&lt;p>The biggest benefit when using Terraform to maintain Kubernetes resources is integration into the Terraform plan/apply life-cycle. So you can review planned changes before applying them. Also, using &lt;code>kubectl&lt;/code>, purging of resources from the cluster is not trivial without manual intervention. Terraform does this reliably.&lt;/p>
&lt;h2 id="terraform-kubernetes-alpha-provider">Terraform &lt;code>kubernetes-alpha&lt;/code> provider&lt;/h2>
&lt;p>Second, the new &lt;a href="https://github.com/hashicorp/terraform-provider-kubernetes-alpha">alpha Kubernetes provider&lt;/a>. As a response to the limitations of the current Kubernetes provider the Hashicorp team recently released an alpha version of a new provider.&lt;/p>
&lt;p>This provider uses dynamic resource types and server-side-apply to support all Kubernetes resources. I personally think this provider has the potential to be a game changer - even if &lt;a href="https://github.com/hashicorp/terraform-provider-kubernetes-alpha#moving-from-yaml-to-hcl">managing Kubernetes resources in HCL&lt;/a> may still not be for everyone. Maybe the Kustomize provider below will help with that.&lt;/p>
&lt;p>The only downside really is, that it's explicitly discouraged to use it for anything but testing. But the more people test it, the sooner it should be ready for prime time. So I encourage everyone to give it a try.&lt;/p>
&lt;h2 id="terraform-kustomize-provider">Terraform &lt;code>kustomize&lt;/code> provider&lt;/h2>
&lt;p>Last, we have the &lt;a href="https://github.com/kbst/terraform-provider-kustomize">&lt;code>kustomize&lt;/code> provider&lt;/a>. Kustomize provides a way to do customizations of Kubernetes resources using inheritance instead of templating. It is designed to output the result to &lt;code>stdout&lt;/code>, from where you can apply the changes using &lt;code>kubectl&lt;/code>. This approach means that &lt;code>kubectl&lt;/code> edge cases like no purging or changes to immutable attributes still make full automation difficult.&lt;/p>
&lt;p>Kustomize is a popular way to handle customizations. But I was looking for a more reliable way to automate applying changes. Since this is exactly what Terraform is great at the Kustomize provider was born.&lt;/p>
&lt;p>Not going into too much detail here, but from Terraform's perspective, this provider treats every Kubernetes resource as a JSON string. This way it can handle any Kubernetes resource resulting from the Kustomize build. But it has the big disadvantage that Kubernetes resources can not easily be integrated with other Terraform resources. Remember the load balancer example from above.&lt;/p>
&lt;p>Under the hood, similarly to the new Kubernetes alpha provider, the Kustomize provider also uses the dynamic Kubernetes client and server-side-apply. Going forward, I plan to deprecate this part of the Kustomize provider that overlaps with the new Kubernetes provider and only keep the Kustomize integration.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>For teams that are already invested into Terraform, or teams that are looking for ways to replace &lt;code>kubectl&lt;/code> in automation, Terraform's plan/apply life-cycle has always been a promising option to automate changes to Kubernetes resources. However, the limitations of the official Kubernetes provider resulted in this not seeing significant adoption.&lt;/p>
&lt;p>The new alpha provider removes the limitations and has the potential to make Terraform a prime option to automate changes to Kubernetes resources.&lt;/p>
&lt;p>Teams that have already adopted Kustomize, may find integrating Kustomize and Terraform using the Kustomize provider beneficial over &lt;code>kubectl&lt;/code> because it avoids common edge cases. Even if in this set up, Terraform can only easily be used to plan and apply the changes, not to adapt the Kubernetes resources. In the future, this issue may be resolved by combining the Kustomize provider with the new Kubernetes provider.&lt;/p>
&lt;p>If you have any questions regarding these three options, feel free to reach out to me on the Kubernetes Slack in either the &lt;a href="https://app.slack.com/client/T09NY5SBT/CMBCT7XRQ">#kubestack&lt;/a> or the &lt;a href="https://app.slack.com/client/T09NY5SBT/C9A5ALABG">#kustomize&lt;/a> channel. If you happen to give any of the providers a try and encounter a problem, please file a GitHub issue to help the maintainers fix it.&lt;/p></description></item><item><title>Blog: A Better Docs UX With Docsy</title><link>https://kubernetes.io/blog/2020/06/better-docs-ux-with-docsy/</link><pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/06/better-docs-ux-with-docsy/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Zach Corleissen, Cloud Native Computing Foundation&lt;/p>
&lt;p>&lt;em>Editor's note: Zach is one of the chairs for the Kubernetes documentation special interest group (SIG Docs).&lt;/em>&lt;/p>
&lt;p>I'm pleased to announce that the &lt;a href="https://kubernetes.io">Kubernetes website&lt;/a> now features the &lt;a href="https://docsy.dev">Docsy Hugo theme&lt;/a>.&lt;/p>
&lt;p>The Docsy theme improves the site's organization and navigability, and opens a path to improved API references. After over 4 years with few meaningful UX improvements, Docsy implements some best practices for technical content. The theme makes the Kubernetes site easier to read and makes individual pages easier to navigate. It gives the site a much-needed facelift.&lt;/p>
&lt;p>For example: adding a right-hand rail for navigating topics on the page. No more scrolling up to navigate!&lt;/p>
&lt;p>The theme opens a path for future improvements to the website. The Docsy functionality I'm most excited about is the theme's &lt;a href="https://www.docsy.dev/docs/adding-content/shortcodes/#swaggerui">&lt;code>swaggerui&lt;/code> shortcode&lt;/a>, which provides native support for generating API references from an OpenAPI spec. The CNCF is partnering with &lt;a href="https://developers.google.com/season-of-docs">Google Season of Docs&lt;/a> (GSoD) for staffing to make better API references a reality in Q4 this year. We're hopeful to be chosen, and we're looking forward to Google's list of announced projects on August 16th. Better API references have been a personal goal since I first started working with SIG Docs in 2017. It's exciting to see the goal within reach.&lt;/p>
&lt;p>One of SIG Docs' tech leads, &lt;a href="https://github.com/kbhawkey">Karen Bradshaw&lt;/a> did a lot of heavy lifting to fix a wide range of site compatibility issues, including a fix to the last of our &lt;a href="https://github.com/kubernetes/website/pull/21359">legacy pieces&lt;/a> when we &lt;a href="2018-05-05-hugo-migration/">migrated from Jekyll to Hugo&lt;/a> in 2018. Our other tech leads, &lt;a href="https://github.com/sftim">Tim Bannister&lt;/a> and &lt;a href="https://github.com/onlydole">Taylor Dolezal&lt;/a> provided extensive reviews.&lt;/p>
&lt;p>Thanks also to &lt;a href="https://bep.is/">Björn-Erik Pedersen&lt;/a>, who provided invaluable advice about how to navigate a Hugo upgrade beyond &lt;a href="https://gohugo.io/news/0.60.0-relnotes/">version 0.60.0&lt;/a>.&lt;/p>
&lt;p>The CNCF contracted with &lt;a href="https://gearboxbuilt.com/">Gearbox&lt;/a> in Victoria, BC to apply the theme to the site. Thanks to Aidan, Troy, and the rest of the team for all their work!&lt;/p></description></item><item><title>Blog: Supporting the Evolving Ingress Specification in Kubernetes 1.18</title><link>https://kubernetes.io/blog/2020/06/05/supporting-the-evolving-ingress-specification-in-kubernetes-1.18/</link><pubDate>Fri, 05 Jun 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/06/05/supporting-the-evolving-ingress-specification-in-kubernetes-1.18/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Alex Gervais (Datawire.io)&lt;/p>
&lt;p>Earlier this year, the Kubernetes team released &lt;a href="https://kubernetes.io/blog/2020/03/25/kubernetes-1-18-release-announcement/">Kubernetes 1.18&lt;/a>, which extended Ingress. In this blog post, we’ll walk through what’s new in the new Ingress specification, what it means for your applications, and how to upgrade to an ingress controller that supports this new specification.&lt;/p>
&lt;h3 id="what-is-kubernetes-ingress">What is Kubernetes Ingress&lt;/h3>
&lt;p>When deploying your applications in Kubernetes, one of the first challenges many people encounter is how to get traffic into their cluster. &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Kubernetes ingress&lt;/a> is a collection of routing rules that govern how external users access services running in a Kubernetes cluster. There are &lt;a href="https://blog.getambassador.io/kubernetes-ingress-nodeport-load-balancers-and-ingress-controllers-6e29f1c44f2d">three general approaches&lt;/a> for exposing your application:&lt;/p>
&lt;ul>
&lt;li>Using a &lt;code>NodePort&lt;/code> to expose your application on a port across each of your nodes&lt;/li>
&lt;li>Using a &lt;code>LoadBalancer&lt;/code> service to create an external load balancer that points to a Kubernetes service in your cluster&lt;/li>
&lt;li>Using a Kubernetes Ingress resource&lt;/li>
&lt;/ul>
&lt;h3 id="what-s-new-in-kubernetes-1-18-ingress">What’s new in Kubernetes 1.18 Ingress&lt;/h3>
&lt;p>There are three significant additions to the Ingress API in Kubernetes 1.18:&lt;/p>
&lt;ul>
&lt;li>A new &lt;code>pathType&lt;/code> field&lt;/li>
&lt;li>A new &lt;code>IngressClass&lt;/code> resource&lt;/li>
&lt;li>Support for wildcards in hostnames&lt;/li>
&lt;/ul>
&lt;p>The new &lt;code>pathType&lt;/code> field allows you to specify how Ingress paths should match.
The field supports three types: &lt;code>ImplementationSpecific&lt;/code> (default), &lt;code>exact&lt;/code>, and &lt;code>prefix&lt;/code>. Explicitly defining the expected behavior of path matching will allow every ingress-controller to support a user’s needs and will increase portability between ingress-controller implementation solutions.&lt;/p>
&lt;p>The &lt;code>IngressClass&lt;/code> resource specifies how Ingresses should be implemented by controllers. This was added to formalize the commonly used but never standardized &lt;code>kubernetes.io/ingress.class&lt;/code> annotation and allow for implementation-specific extensions and configuration.&lt;/p>
&lt;p>You can read more about these changes, as well as the support for wildcards in hostnames in more detail in &lt;a href="https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/">a previous blog post&lt;/a>.&lt;/p>
&lt;h2 id="supporting-kubernetes-ingress">Supporting Kubernetes ingress&lt;/h2>
&lt;p>&lt;a href="https://www.getambassador.io">Ambassador&lt;/a> is an open-source Envoy-based ingress controller. We believe strongly in supporting common standards such as Kubernetes ingress, which we adopted and &lt;a href="https://blog.getambassador.io/ambassador-ingress-controller-better-config-reporting-updated-envoy-proxy-99dc9139e28f">announced our initial support for back in 2019&lt;/a>.&lt;/p>
&lt;p>Every Ambassador release goes through rigorous testing. Therefore, we also contributed an &lt;a href="https://github.com/kubernetes-sigs/ingress-controller-conformance">open conformance test suite&lt;/a>, supporting Kubernetes ingress. We wrote the initial bits of test code and will keep iterating over the newly added features and different versions of the Ingress specification as it evolves to a stable v1 GA release. Documentation and usage samples, is one of our top priorities. We understand how complex usage can be, especially when transitioning from a previous version of an API.&lt;/p>
&lt;p>Following a test-driven development approach, the first step we took in supporting Ingress improvements in Ambassador was to translate the revised specification -- both in terms of API and behavior -- into a comprehensible test suite. The test suite, although still under heavy development and going through multiple iterations, was rapidly added to the Ambassador CI infrastructure and acceptance criteria. This means every change to the Ambassador codebase going forward will be compliant with the Ingress API and be tested end-to-end in a lightweight &lt;a href="https://kind.sigs.k8s.io/">KIND cluster&lt;/a>. Using KIND allowed us to make rapid improvements while limiting our cloud provider infrastructure bill and testing out unreleased Kubernetes features with pre-release builds.&lt;/p>
&lt;h3 id="adopting-a-new-specification">Adopting a new specification&lt;/h3>
&lt;p>With a global comprehension of additions to Ingress introduced in Kubernetes 1.18 and a test suite on hand, we tackled the task of adapting the Ambassador code so that it would support translating the high-level Ingress API resources into Envoy configurations and constructs. Luckily Ambassador already supported previous versions of ingress functionalities so the development effort was incremental.&lt;/p>
&lt;p>We settled on a controller name of &lt;code>getambassador.io/ingress-controller&lt;/code>. This value, consistent with Ambassador's domain and CRD versions, must be used to tie in an IngressClass &lt;code>spec.controller&lt;/code> with an Ambassador deployment. The new IngressClass resource allows for extensibility by setting a &lt;code>spec.parameters&lt;/code> field. At the moment Ambassador makes no use of this field and its usage is reserved for future development.&lt;/p>
&lt;p>Paths can now define different matching behaviors using the &lt;code>pathType&lt;/code> field. The field will default to a value of &lt;code>ImplementationSpecific&lt;/code>, which uses the same matching rules as the &lt;a href="https://www.getambassador.io/docs/latest/topics/using/mappings/">Ambassador Mappings&lt;/a> prefix field and previous Ingress specification for backward compatibility reasons.&lt;/p>
&lt;h3 id="kubernetes-ingress-controllers">Kubernetes Ingress Controllers&lt;/h3>
&lt;p>A comprehensive &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/">list of Kubernetes ingress controllers&lt;/a> is available in the Kubernetes documentation. Currently, Ambassador is the only ingress controller that supports these new additions to the ingress specification. Powered by the &lt;a href="https://www.envoyproxy.io">Envoy Proxy&lt;/a>, Ambassador is the fastest way for you to try out the new ingress specification today.&lt;/p>
&lt;p>Check out the following resources:&lt;/p>
&lt;ul>
&lt;li>Ambassador on &lt;a href="https://www.github.com/datawire/ambassador">GitHub&lt;/a>&lt;/li>
&lt;li>The Ambassador &lt;a href="https://www.getambassador.io/docs">documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/">Improvements to the Ingress API&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Or join the community on &lt;a href="http://d6e.co/slack">Slack&lt;/a>!&lt;/p></description></item><item><title>Blog: K8s KPIs with Kuberhealthy</title><link>https://kubernetes.io/blog/2020/05/29/k8s-kpis-with-kuberhealthy/</link><pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/05/29/k8s-kpis-with-kuberhealthy/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Joshulyne Park (Comcast), Eric Greer (Comcast)&lt;/p>
&lt;h3 id="building-onward-from-kuberhealthy-v2-0-0">Building Onward from Kuberhealthy v2.0.0&lt;/h3>
&lt;p>Last November at KubeCon San Diego 2019, we announced the release of
&lt;a href="https://www.youtube.com/watch?v=aAJlWhBtzqY">Kuberhealthy 2.0.0&lt;/a> - transforming Kuberhealthy into a Kubernetes operator
for synthetic monitoring. This new ability granted developers the means to create their own Kuberhealthy check
containers to synthetically monitor their applications and clusters. The community was quick to adopt this new feature and we're grateful for everyone who implemented and tested Kuberhealthy 2.0.0 in their clusters. Thanks to all of you who reported
issues and contributed to discussions on the #kuberhealthy Slack channel. We quickly set to work to address all your feedback
with a newer version of Kuberhealthy. Additionally, we created a guide on how to easily install and use Kuberhealthy in order to capture some helpful synthetic &lt;a href="https://kpi.org/KPI-Basics">KPIs&lt;/a>.&lt;/p>
&lt;h3 id="deploying-kuberhealthy">Deploying Kuberhealthy&lt;/h3>
&lt;p>To install Kuberhealthy, make sure you have &lt;a href="https://helm.sh/docs/intro/install/">Helm 3&lt;/a> installed. If not, you can use the generated flat spec files located
in this &lt;a href="https://github.com/Comcast/kuberhealthy/tree/master/deploy">deploy folder&lt;/a>. You should use &lt;a href="https://github.com/Comcast/kuberhealthy/blob/master/deploy/kuberhealthy-prometheus.yaml">kuberhealthy-prometheus.yaml&lt;/a> if you don't use the &lt;a href="https://github.com/coreos/prometheus-operator">Prometheus Operator&lt;/a>, and &lt;a href="https://github.com/Comcast/kuberhealthy/blob/master/deploy/kuberhealthy-prometheus-operator.yaml">kuberhealthy-prometheus-operator.yaml&lt;/a> if you do. If you don't use Prometheus at all, you can still use Kuberhealthy with a JSON status page and/or InfluxDB integration using &lt;a href="https://github.com/Comcast/kuberhealthy/blob/master/deploy/kuberhealthy.yaml">this spec&lt;/a>.&lt;/p>
&lt;h4 id="to-install-using-helm-3">To install using Helm 3:&lt;/h4>
&lt;h5 id="1-create-namespace-kuberhealthy-in-the-desired-kubernetes-cluster-context">1. Create namespace &amp;quot;kuberhealthy&amp;quot; in the desired Kubernetes cluster/context:&lt;/h5>
&lt;pre>&lt;code>kubectl create namespace kuberhealthy
&lt;/code>&lt;/pre>&lt;h5 id="2-set-your-current-namespace-to-kuberhealthy">2. Set your current namespace to &amp;quot;kuberhealthy&amp;quot;:&lt;/h5>
&lt;pre>&lt;code>kubectl config set-context --current --namespace=kuberhealthy
&lt;/code>&lt;/pre>&lt;h5 id="3-add-the-kuberhealthy-repo-to-helm">3. Add the kuberhealthy repo to Helm:&lt;/h5>
&lt;pre>&lt;code>helm repo add kuberhealthy https://comcast.github.io/kuberhealthy/helm-repos
&lt;/code>&lt;/pre>&lt;h5 id="4-depending-on-your-prometheus-implementation-install-kuberhealthy-using-the-appropriate-command-for-your-cluster">4. Depending on your Prometheus implementation, install Kuberhealthy using the appropriate command for your cluster:&lt;/h5>
&lt;ul>
&lt;li>If you use the &lt;a href="https://github.com/coreos/prometheus-operator">Prometheus Operator&lt;/a>:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code>helm install kuberhealthy kuberhealthy/kuberhealthy --set prometheus.enabled=true,prometheus.enableAlerting=true,prometheus.enableScraping=true,prometheus.serviceMonitor=true
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>If you use Prometheus, but NOT Prometheus Operator:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code>helm install kuberhealthy kuberhealthy/kuberhealthy --set prometheus.enabled=true,prometheus.enableAlerting=true,prometheus.enableScraping=true
&lt;/code>&lt;/pre>&lt;p>See additional details about configuring the appropriate scrape annotations in the section &lt;a href="#prometheus-integration-details">Prometheus Integration Details&lt;/a> below.&lt;/p>
&lt;ul>
&lt;li>Finally, if you don't use Prometheus:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code>helm install kuberhealthy kuberhealthy/kuberhealthy
&lt;/code>&lt;/pre>&lt;p>Running the Helm command should automatically install the newest version of Kuberhealthy (v2.2.0) along with a few basic checks. If you run &lt;code>kubectl get pods&lt;/code>, you should see two Kuberhealthy pods. These are the pods that create, coordinate, and track test pods. These two Kuberhealthy pods also serve a JSON status page as well as a &lt;code>/metrics&lt;/code> endpoint. Every other pod you see created is a checker pod designed to execute and shut down when done.&lt;/p>
&lt;h3 id="configuring-additional-checks">Configuring Additional Checks&lt;/h3>
&lt;p>Next, you can run &lt;code>kubectl get khchecks&lt;/code>. You should see three Kuberhealthy checks installed by default:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/Comcast/kuberhealthy/tree/master/cmd/daemonset-check">daemonset&lt;/a>: Deploys and tears down a daemonset to ensure all nodes in the cluster are functional.&lt;/li>
&lt;li>&lt;a href="https://github.com/Comcast/kuberhealthy/tree/master/cmd/deployment-check">deployment&lt;/a>: Creates a deployment and then triggers a rolling update. Tests that the deployment is reachable via a service and then deletes everything. Any problem in this process will cause this check to report a failure.&lt;/li>
&lt;li>&lt;a href="https://github.com/Comcast/kuberhealthy/tree/master/cmd/dns-resolution-check">dns-status-internal&lt;/a>: Validates that internal cluster DNS is functioning as expected.&lt;/li>
&lt;/ul>
&lt;p>To view other available external checks, check out the &lt;a href="https://github.com/Comcast/kuberhealthy/blob/master/docs/EXTERNAL_CHECKS_REGISTRY.md">external checks registry&lt;/a> where you can find other yaml files you can apply to your cluster to enable various checks.&lt;/p>
&lt;p>Kuberhealthy check pods should start running shortly after Kuberhealthy starts running (1-2 minutes). Additionally, the check-reaper cronjob runs every few minutes to ensure there are no more than 5 completed checker pods left lying around at a time.&lt;/p>
&lt;p>To get status page view of these checks, you'll need to either expose the &lt;code>kuberhealthy&lt;/code> service externally by editing the service &lt;code>kuberhealthy&lt;/code> and setting &lt;code>Type: LoadBalancer&lt;/code> or use &lt;code>kubectl port-forward service/kuberhealthy 8080:80&lt;/code>. When viewed, the service endpoint will display a JSON status page that looks like this:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;OK&amp;#34;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;Errors&amp;#34;&lt;/span>: [],
&lt;span style="color:#008000;font-weight:bold">&amp;#34;CheckDetails&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;kuberhealthy/daemonset&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;OK&amp;#34;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;Errors&amp;#34;&lt;/span>: [],
&lt;span style="color:#008000;font-weight:bold">&amp;#34;RunDuration&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;22.512278967s&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;Namespace&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kuberhealthy&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;LastRun&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;2020-04-06T23:20:31.7176964Z&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;AuthoritativePod&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kuberhealthy-67bf8c4686-mbl2j&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;uuid&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;9abd3ec0-b82f-44f0-b8a7-fa6709f759cd&amp;#34;&lt;/span>
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;kuberhealthy/deployment&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;OK&amp;#34;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;Errors&amp;#34;&lt;/span>: [],
&lt;span style="color:#008000;font-weight:bold">&amp;#34;RunDuration&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;29.142295647s&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;Namespace&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kuberhealthy&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;LastRun&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;2020-04-06T23:20:31.7176964Z&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;AuthoritativePod&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kuberhealthy-67bf8c4686-mbl2j&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;uuid&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;5f0d2765-60c9-47e8-b2c9-8bc6e61727b2&amp;#34;&lt;/span>
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;kuberhealthy/dns-status-internal&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;OK&amp;#34;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;Errors&amp;#34;&lt;/span>: [],
&lt;span style="color:#008000;font-weight:bold">&amp;#34;RunDuration&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;2.43940936s&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;Namespace&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kuberhealthy&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;LastRun&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;2020-04-06T23:20:44.6294547Z&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;AuthoritativePod&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kuberhealthy-67bf8c4686-mbl2j&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;uuid&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;c85f95cb-87e2-4ff5-b513-e02b3d25973a&amp;#34;&lt;/span>
}
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;CurrentMaster&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kuberhealthy-7cf79bdc86-m78qr&amp;#34;&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This JSON page displays all Kuberhealthy checks running in your cluster. If you have Kuberhealthy checks running in different namespaces, you can filter them by adding the &lt;code>GET&lt;/code> variable &lt;code>namespace&lt;/code> parameter: &lt;code>?namespace=kuberhealthy,kube-system&lt;/code> onto the status page URL.&lt;/p>
&lt;h3 id="writing-your-own-checks">Writing Your Own Checks&lt;/h3>
&lt;p>Kuberhealthy is designed to be extended with custom check containers that can be written by anyone to check anything. These checks can be written in any language as long as they are packaged in a container. This makes Kuberhealthy an excellent platform for creating your own synthetic checks!&lt;/p>
&lt;p>Creating your own check is a great way to validate your client library, simulate real user workflow, and create a high level of confidence in your service or system uptime.&lt;/p>
&lt;p>To learn more about writing your own checks, along with simple examples, check the &lt;a href="https://github.com/Comcast/kuberhealthy/blob/master/docs/EXTERNAL_CHECK_CREATION.md">custom check creation&lt;/a> documentation.&lt;/p>
&lt;h3 id="prometheus-integration-details">Prometheus Integration Details&lt;/h3>
&lt;p>When enabling Prometheus (not the operator), the Kuberhealthy service gets the following annotations added:&lt;/p>
&lt;pre>&lt;code class="language-.env" data-lang=".env">prometheus.io/path: /metrics
prometheus.io/port: &amp;quot;80&amp;quot;
prometheus.io/scrape: &amp;quot;true&amp;quot;
&lt;/code>&lt;/pre>&lt;p>In your prometheus configuration, add the following example scrape_config that scrapes the Kuberhealthy service given the added prometheus annotation:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">- &lt;span style="color:#a2f;font-weight:bold">job_name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;kuberhealthy&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">scrape_interval&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>1m&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">honor_labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">metrics_path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/metrics&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kubernetes_sd_configs&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">role&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>service&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespaces&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">names&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- kuberhealthy&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">relabel_configs&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">source_labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[__meta_kubernetes_service_annotation_prometheus_io_scrape]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">action&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>keep&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">regex&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can also specify the target endpoint to be scraped using this example job:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">- &lt;span style="color:#a2f;font-weight:bold">job_name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kuberhealthy&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">scrape_interval&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>1m&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">honor_labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">metrics_path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/metrics&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">static_configs&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">targets&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- kuberhealthy.kuberhealthy.svc.cluster.local:&lt;span style="color:#666">80&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once the appropriate prometheus configurations are applied, you should be able to see the following Kuberhealthy metrics:&lt;/p>
&lt;ul>
&lt;li>&lt;code>kuberhealthy_check&lt;/code>&lt;/li>
&lt;li>&lt;code>kuberhealthy_check_duration_seconds&lt;/code>&lt;/li>
&lt;li>&lt;code>kuberhealthy_cluster_states&lt;/code>&lt;/li>
&lt;li>&lt;code>kuberhealthy_running&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="creating-key-performance-indicators">Creating Key Performance Indicators&lt;/h3>
&lt;p>Using these Kuberhealthy metrics, our team has been able to collect KPIs based on the following definitions, calculations, and PromQL queries.&lt;/p>
&lt;p>&lt;em>Availability&lt;/em>&lt;/p>
&lt;p>We define availability as the K8s cluster control plane being up and functioning as expected. This is measured by our ability to create a deployment, do a rolling update, and delete the deployment within a set period of time.&lt;/p>
&lt;p>We calculate this by measuring Kuberhealthy's &lt;a href="https://github.com/Comcast/kuberhealthy/tree/master/cmd/deployment-check">deployment check&lt;/a> successes and failures.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Availability = Uptime / (Uptime * Downtime)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Uptime = Number of Deployment Check Passes * Check Run Interval&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Downtime = Number of Deployment Check Fails * Check Run Interval&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Check Run Interval = how often the check runs (&lt;code>runInterval&lt;/code> set in your KuberhealthyCheck Spec)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>PromQL Query (Availability % over the past 30 days):&lt;/p>
&lt;pre>&lt;code class="language-promql" data-lang="promql">1 - (sum(count_over_time(kuberhealthy_check{check=&amp;quot;kuberhealthy/deployment&amp;quot;, status=&amp;quot;0&amp;quot;}[30d])) OR vector(0)) / sum(count_over_time(kuberhealthy_check{check=&amp;quot;kuberhealthy/deployment&amp;quot;, status=&amp;quot;1&amp;quot;}[30d]))
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ul>
&lt;p>&lt;em>Utilization&lt;/em>&lt;/p>
&lt;p>We define utilization as user uptake of product (k8s) and its resources (pods, services, etc.). This is measured by how many nodes, deployments, statefulsets, persistent volumes, services, pods, and jobs are being utilized by our customers.
We calculate this by counting the total number of nodes, deployments, statefulsets, persistent volumes, services, pods, and jobs.&lt;/p>
&lt;p>&lt;em>Duration (Latency)&lt;/em>&lt;/p>
&lt;p>We define duration as the control plane's capacity and utilization of throughput. We calculate this by capturing the average run duration of a Kuberhealthy &lt;a href="https://github.com/Comcast/kuberhealthy/tree/master/cmd/deployment-check">deployment check&lt;/a> run.&lt;/p>
&lt;ul>
&lt;li>PromQL Query (Deployment check average run duration):
&lt;pre>&lt;code class="language-promql" data-lang="promql">avg(kuberhealthy_check_duration_seconds{check=&amp;quot;kuberhealthy/deployment&amp;quot;})
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ul>
&lt;p>&lt;em>Errors / Alerts&lt;/em>&lt;/p>
&lt;p>We define errors as all k8s cluster and Kuberhealthy related alerts. Every time one of our Kuberhealthy check fails, we are alerted of this failure.&lt;/p>
&lt;h3 id="thank-you">Thank You!&lt;/h3>
&lt;p>Thanks again to everyone in the community for all of your contributions and help! We are excited to see what you build. As always, if you find an issue, have a feature request, or need to open a pull request, please &lt;a href="https://github.com/Comcast/kuberhealthy/issues">open an issue&lt;/a> on the Github project.&lt;/p></description></item><item><title>Blog: My exciting journey into Kubernetes’ history</title><link>https://kubernetes.io/blog/2020/05/my-exciting-journey-into-kubernetes-history/</link><pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/05/my-exciting-journey-into-kubernetes-history/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Sascha Grunert, SUSE Software Solutions&lt;/p>
&lt;p>&lt;em>Editor's note: Sascha is part of &lt;a href="https://github.com/kubernetes/sig-release">SIG Release&lt;/a> and is working on many other
different container runtime related topics. Feel free to reach him out on
Twitter &lt;a href="https://twitter.com/saschagrunert">@saschagrunert&lt;/a>.&lt;/em>&lt;/p>
&lt;hr>
&lt;blockquote>
&lt;p>A story of data science-ing 90,000 GitHub issues and pull requests by using
Kubeflow, TensorFlow, Prow and a fully automated CI/CD pipeline.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;a href="#introduction">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#getting-the-data">Getting the Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#exploring-the-data">Exploring the Data&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#labels-labels-labels">Labels, Labels, Labels&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#building-the-machine-learning-model">Building the Machine Learning Model&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#doing-some-first-natural-language-processing-nlp">Doing some first Natural Language Processing (NLP)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#creating-the-multi-layer-perceptron-mlp-model">Creating the Multi-Layer Perceptron (MLP) Model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#training-the-model">Training the Model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#a-first-prediction">A first Prediction&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#automate-everything">Automate Everything&lt;/a>&lt;/li>
&lt;li>&lt;a href="#automatic-labeling-of-new-prs">Automatic Labeling of new PRs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>Choosing the right steps when working in the field of data science is truly no
silver bullet. Most data scientists might have their custom workflow, which
could be more or less automated, depending on their area of work. Using
&lt;a href="https://kubernetes.io">Kubernetes&lt;/a> can be a tremendous enhancement when trying to automate
workflows on a large scale. In this blog post, I would like to take you on my
journey of doing data science while integrating the overall workflow into
Kubernetes.&lt;/p>
&lt;p>The target of the research I did in the past few months was to find any
useful information about all those thousands of GitHub issues and pull requests
(PRs) we have in the &lt;a href="https://github.com/kubernetes/kubernetes">Kubernetes repository&lt;/a>. What I ended up with was a
fully automated, in Kubernetes running Continuous Integration (CI) and
Deployment (CD) data science workflow powered by &lt;a href="https://www.kubeflow.org">Kubeflow&lt;/a> and &lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow">Prow&lt;/a>.
You may not know both of them, but we get to the point where I explain what
they’re doing in detail. The source code of my work can be found in the
&lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis">kubernetes-analysis GitHub repository&lt;/a>, which contains everything source
code-related as well as the raw data. But how to retrieve this data I’m talking
about? Well, this is where the story begins.&lt;/p>
&lt;h1 id="getting-the-data">Getting the Data&lt;/h1>
&lt;p>The foundation for my experiments is the raw GitHub API data in plain &lt;a href="https://en.wikipedia.org/wiki/JSON">JSON&lt;/a>
format. The necessary data can be retrieved via the &lt;a href="https://developer.github.com/v3/issues">GitHub issues
endpoint&lt;/a>, which returns all pull requests as well as regular issues in the
&lt;a href="https://en.wikipedia.org/wiki/Representational_state_transfer">REST&lt;/a> API. I exported roughly &lt;strong>91000&lt;/strong> issues and pull requests in
the first iteration into a massive &lt;strong>650 MiB&lt;/strong> data blob. This took me about &lt;strong>8
hours&lt;/strong> of data retrieval time because for sure, the GitHub API is &lt;a href="https://developer.github.com/apps/building-github-apps/understanding-rate-limits-for-github-apps/">rate
limited&lt;/a>. To be able to put this data into a GitHub repository, I’d chosen
to compress it via &lt;a href="https://linux.die.net/man/1/xz">&lt;code>xz(1)&lt;/code>&lt;/a>. The result was a roundabout &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/data/api.tar.xz">25 MiB sized
tarball&lt;/a>, which fits well into the repository.&lt;/p>
&lt;p>I had to find a way to regularly update the dataset because the Kubernetes
issues and pull requests are updated by the users over time as well as new ones
are created. To achieve the continuous update without having to wait 8 hours
over and over again, I now fetch the delta GitHub API data between the
&lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/.update">last update&lt;/a> and the current time. This way, a Continuous Integration job
can update the data on a regular basis, whereas I can continue my research with
the latest available set of data.&lt;/p>
&lt;p>From a tooling perspective, I’ve written an &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/main">all-in-one Python executable&lt;/a>,
which allows us to trigger the different steps during the data science
experiments separately via dedicated subcommands. For example, to run an export
of the whole data set, we can call:&lt;/p>
&lt;pre>&lt;code>&amp;gt; export GITHUB_TOKEN=&amp;lt;MY-SECRET-TOKEN&amp;gt;
&amp;gt; ./main export
INFO | Getting GITHUB_TOKEN from environment variable
INFO | Dumping all issues
INFO | Pulling 90929 items
INFO | 1: Unit test coverage in Kubelet is lousy. (~30%)
INFO | 2: Better error messages if go isn't installed, or if gcloud is old.
INFO | 3: Need real cluster integration tests
INFO | 4: kubelet should know which containers it is managing
… [just wait 8 hours] …
&lt;/code>&lt;/pre>&lt;p>To update the data between the last time stamp stored in the repository we can
run:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main export --update-api
INFO | Getting GITHUB_TOKEN from environment variable
INFO | Retrieving issues and PRs
INFO | Updating API
INFO | Got update timestamp: 2020-05-09T10:57:40.854151
INFO | 90786: Automated cherry pick of #90749: fix: azure disk dangling attach issue
INFO | 90674: Switch core master base images from debian to distroless
INFO | 90086: Handling error returned by request.Request.ParseForm()
INFO | 90544: configurable weight on the CPU and memory
INFO | 87746: Support compiling Kubelet w/o docker/docker
INFO | Using already extracted data from data/data.pickle
INFO | Loading pickle dataset
INFO | Parsed 34380 issues and 55832 pull requests (90212 items)
INFO | Updating data
INFO | Updating issue 90786 (updated at 2020-05-09T10:59:43Z)
INFO | Updating issue 90674 (updated at 2020-05-09T10:58:27Z)
INFO | Updating issue 90086 (updated at 2020-05-09T10:58:26Z)
INFO | Updating issue 90544 (updated at 2020-05-09T10:57:51Z)
INFO | Updating issue 87746 (updated at 2020-05-09T11:01:51Z)
INFO | Saving data
&lt;/code>&lt;/pre>&lt;p>This gives us an idea of how fast the project is actually moving: On a Saturday
at noon (European time), 5 issues and pull requests got updated within literally 5
minutes!&lt;/p>
&lt;p>Funnily enough, &lt;a href="https://github.com/jbeda">Joe Beda&lt;/a>, one of the founders of Kubernetes, created the
first GitHub issue &lt;a href="https://github.com/kubernetes/kubernetes/issues/1">mentioning that the unit test coverage is too low&lt;/a>. The
issue has no further description than the title, and no enhanced labeling
applied, like we know from more recent issues and pull requests. But now we have
to explore the exported data more deeply to do something useful with it.&lt;/p>
&lt;h1 id="exploring-the-data">Exploring the Data&lt;/h1>
&lt;p>Before we can start creating machine learning models and train them, we have to
get an idea about how our data is structured and what we want to achieve in
general.&lt;/p>
&lt;p>To get a better feeling about the amount of data, let’s look at how many issues
and pull requests have been created over time inside the Kubernetes repository:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main analyze --created
INFO | Using already extracted data from data/data.pickle
INFO | Loading pickle dataset
INFO | Parsed 34380 issues and 55832 pull requests (90212 items)
&lt;/code>&lt;/pre>&lt;p>The Python &lt;a href="https://matplotlib.org">matplotlib&lt;/a> module should pop up a graph which looks like this:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/created-all.svg" alt="created all">&lt;/p>
&lt;p>Okay, this looks not that spectacular but gives us an impression on how the
project has grown over the past 6 years. To get a better idea about the speed of
development of the project, we can look at the &lt;em>created-vs-closed&lt;/em> metric. This
means on our timeline, we add one to the y-axis if an issue or pull request got
created and subtracts one if closed. Now the chart looks like this:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main analyze --created-vs-closed
&lt;/code>&lt;/pre>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/created-vs-closed-all.svg" alt="created vs closed all">&lt;/p>
&lt;p>At the beginning of 2018, the Kubernetes projects introduced some more enhanced
life-cycle management via the glorious &lt;a href="https://github.com/fejta-bot">fejta-bot&lt;/a>. This automatically
closes issues and pull requests after they got stale over a longer period of
time. This resulted in a massive closing of issues, which does not apply to pull
requests in the same amount. For example, if we look at the &lt;em>created-vs-closed&lt;/em>
metric only for pull requests.&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main analyze --created-vs-closed --pull-requests
&lt;/code>&lt;/pre>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/created-vs-closed-pull-requests.svg" alt="created vs closed pull requests">&lt;/p>
&lt;p>The overall impact is not that obvious. What we can see is that the increasing
number of peaks in the PR chart indicates that the project is moving faster over
time. Usually, a candlestick chart would be a better choice for showing this kind
of volatility-related information. I’d also like to highlight that it looks like
the development of the project slowed down a bit in the beginning of 2020.&lt;/p>
&lt;p>Parsing raw JSON in every analysis iteration is not the fastest approach to do
in Python. This means that I decided to parse the more important information,
for example the content, title and creation time into dedicated &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/src/issue.py">issue&lt;/a> and
&lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/src/pull_request.py">PR classes&lt;/a>. This data will be &lt;a href="https://docs.python.org/3/library/pickle.html">pickle&lt;/a> serialized into the repository
as well, which allows an overall faster startup independently of the JSON blob.&lt;/p>
&lt;p>A pull request is more or less the same as an issue in my analysis, except that
it contains a release note.&lt;/p>
&lt;p>Release notes in Kubernetes are written in the PRs description into a separate
&lt;code>release-note&lt;/code> block like this:&lt;/p>
&lt;pre>&lt;code>```release-note
I changed something extremely important and you should note that.
```
&lt;/code>&lt;/pre>&lt;p>Those release notes are parsed by &lt;a href="https://github.com/kubernetes/release#tools">dedicated Release Engineering Tools like
&lt;code>krel&lt;/code>&lt;/a> during the release creation process and will be part of the various
&lt;a href="https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG">CHANGELOG.md&lt;/a> files and the &lt;a href="https://relnotes.k8s.io">Release Notes Website&lt;/a>. That seems like a
lot of magic, but in the end, the quality of the overall release notes is much
higher because they’re easy to edit, and the PR reviewers can ensure that we
only document real user-facing changes and nothing else.&lt;/p>
&lt;p>The quality of the input data is a key aspect when doing data science. I decided
to focus on the release notes because they seem to have the highest amount of
overall quality when comparing them to the plain descriptions in issues and PRs.
Besides that, they’re easy to parse, and we would not need to strip away
the &lt;a href="https://github.com/kubernetes/kubernetes/tree/master/.github/ISSUE_TEMPLATE">various issue&lt;/a> and &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/.github/PULL_REQUEST_TEMPLATE.md">PR template&lt;/a> text noise.&lt;/p>
&lt;h2 id="labels-labels-labels">Labels, Labels, Labels&lt;/h2>
&lt;p>Issues and pull requests in Kubernetes get different labels applied during its
life-cycle. They are usually grouped via a single slash (&lt;code>/&lt;/code>). For example, we
have &lt;code>kind/bug&lt;/code> and &lt;code>kind/api-change&lt;/code> or &lt;code>sig/node&lt;/code> and &lt;code>sig/network&lt;/code>. An easy
way to understand which label groups exist and how they’re distributed across
the repository is to plot them into a bar chart:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main analyze --labels-by-group
&lt;/code>&lt;/pre>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/labels-by-group-all-top-25.svg" alt="labels by group all top 25">&lt;/p>
&lt;p>It looks like that &lt;code>sig/&lt;/code>, &lt;code>kind/&lt;/code> and &lt;code>area/&lt;/code> labels are pretty common.
Something like &lt;code>size/&lt;/code> can be ignored for now because these labels are
automatically applied based on the amount of the code changes for a pull
request. We said that we want to focus on release notes as input data, which
means that we have to check out the distribution of the labels for the PRs. This
means that the top 25 labels on pull requests are:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main analyze --labels-by-name --pull-requests
&lt;/code>&lt;/pre>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/labels-by-name-pull-requests-top-25.svg" alt="labels by name pull requests top 25">&lt;/p>
&lt;p>Again, we can ignore labels like &lt;code>lgtm&lt;/code> (looks good to me), because every PR
which now should get merged has to look good. Pull requests containing release
notes automatically get the &lt;code>release-note&lt;/code> label applied, which enables further
filtering more easily. This does not mean that every PR containing that label
also contains the release notes block. The label could have been applied
manually and the parsing of the release notes block did not exist since the
beginning of the project. This means we will probably loose a decent amount of
input data on one hand. On the other hand we can focus on the highest possible
data quality, because applying labels the right way needs some enhanced maturity
of the project and its contributors.&lt;/p>
&lt;p>From a label group perspective I have chosen to focus on the &lt;code>kind/&lt;/code> labels.
Those labels are something which has to be applied manually by the author of the
PR, they are available on a good amount of pull requests and they’re related to
user-facing changes as well. Besides that, the &lt;code>kind/&lt;/code> choice has to be done for
every pull request because it is part of the PR template.&lt;/p>
&lt;p>Alright, how does the distribution of those labels look like when focusing only
on pull requests which have release notes?&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main analyze --release-notes-stats
&lt;/code>&lt;/pre>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/release-notes-stats.svg" alt="release notes stats">&lt;/p>
&lt;p>Interestingly, we have approximately 7,000 overall pull requests containing
release notes, but only ~5,000 have a &lt;code>kind/&lt;/code> label applied. The distribution of
the labels is not equal, and one-third of them are labeled as &lt;code>kind/bug&lt;/code>. This
brings me to the next decision in my data science journey: I will build a binary
classifier which, for the sake of simplicity, is only able to distinguish between
bugs (via &lt;code>kind/bug&lt;/code>) and non-bugs (where the label is not applied).&lt;/p>
&lt;p>The main target is now to be able to classify newly incoming release notes if
they are related to a bug or not, based on the historical data we already have
from the community.&lt;/p>
&lt;p>Before doing that, I recommend that you play around with the &lt;code>./main analyze -h&lt;/code>
subcommand as well to explore the latest set of data. You can also check out all
the &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/tree/master/assets">continuously updated assets&lt;/a> I provide within the analysis repository.
For example, those are the top 25 PR creators inside the Kubernetes repository:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/users-by-created-pull-requests-top-25.svg" alt="users by created pull request">&lt;/p>
&lt;h1 id="building-the-machine-learning-model">Building the Machine Learning Model&lt;/h1>
&lt;p>Now we have an idea what the data set is about, and we can start building a first
machine learning model. Before actually building the model, we have to
pre-process all the extracted release notes from the PRs. Otherwise, the model
would not be able to understand our input.&lt;/p>
&lt;h2 id="doing-some-first-natural-language-processing-nlp">Doing some first Natural Language Processing (NLP)&lt;/h2>
&lt;p>In the beginning, we have to define a vocabulary for which we want to train. I
decided to choose the &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">TfidfVectorizer&lt;/a> from the Python scikit-learn machine
learning library. This vectorizer is able to take our input texts and create a
single huge vocabulary out of it. This is our so-called &lt;a href="https://en.wikipedia.org/wiki/Bag-of-words_model">bag-of-words&lt;/a>,
which has a chosen n-gram range of &lt;code>(1, 2)&lt;/code> (unigrams and bigrams). Practically
this means that we always use the first word and the next one as a single
vocabulary entry (bigrams). We also use the single word as vocabulary entry
(unigram). The TfidfVectorizer is able to skip words that occur multiple times
(&lt;code>max_df&lt;/code>), and requires a minimum amount (&lt;code>min_df&lt;/code>) to add a word to the
vocabulary. I decided not to change those values in the first place, just
because I had the intuition that release notes are something unique to a
project.&lt;/p>
&lt;p>Parameters like &lt;code>min_df&lt;/code>, &lt;code>max_df&lt;/code> and the n-gram range can be seen as some of
our hyperparameters. Those parameters have to be optimized in a dedicated step
after the machine learning model has been built. This step is called
hyperparameter tuning and basically means that we train multiple times with
different parameters and compare the accuracy of the model. Afterwards, we choose
the parameters with the best accuracy.&lt;/p>
&lt;p>During the training, the vectorizer will produce a &lt;code>data/features.json&lt;/code> which
contains the whole vocabulary. This gives us a good understanding of how such a
vocabulary may look like:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">[
&lt;span style="">…&lt;/span>
&lt;span style="color:#b44">&amp;#34;hostname&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;hostname address&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;hostname and&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;hostname as&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;hostname being&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;hostname bug&amp;#34;&lt;/span>,
&lt;span style="">…&lt;/span>
]
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This produces round about 50,000 entries in the overall bag-of-words, which is
pretty much. Previous analyses between different data sets showed that it is
simply not necessary to take so many features into account. Some general data
sets state that an overall vocabulary of 20,000 is enough and higher amounts do
not influence the accuracy any more. To do so we can use the &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html">SelectKBest&lt;/a>
feature selector to strip down the vocabulary to only choose the top features.
Anyway, I still decided to stick to the top 50,000 to not negatively influence
the model accuracy. We have a relatively low amount of data (appr. 7,000
samples) and a low number of words per sample (~15) which already made me wonder
if we have enough data at all.&lt;/p>
&lt;p>The vectorizer is not only able to create our bag-of-words, but it is also able to
encode the features in &lt;a href="https://en.wikipedia.org/wiki/Tf%e2%80%93idf">term frequency–inverse document frequency (tf-idf)&lt;/a>
format. That is where the vectorizer got its name, whereas the output of that
encoding is something the machine learning model can directly consume. All the
details of the vectorization process can be found in the &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/f419ff4a3462bafc0cb067aa6973dc7280409699/src/nlp.py#L193-L235">source code&lt;/a>.&lt;/p>
&lt;h2 id="creating-the-multi-layer-perceptron-mlp-model">Creating the Multi-Layer Perceptron (MLP) Model&lt;/h2>
&lt;p>I decided to choose a simple MLP based model which is built with the help of the
popular &lt;a href="https://www.tensorflow.org/api_docs/python/tf/keras">TensorFlow&lt;/a> framework. Because we do not have that much input data,
we just use two hidden layers, so that the model basically looks like this:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/model.png" alt="model">&lt;/p>
&lt;p>There have to be &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/f419ff4a3462bafc0cb067aa6973dc7280409699/src/nlp.py#L95-L100">multiple other&lt;/a> hyperparameters to be taken into account
when creating the model. I will not discuss them in detail here, but they’re
important to be optimized also in relation to the number of classes we want to
have in the model (only two in our case).&lt;/p>
&lt;h2 id="training-the-model">Training the Model&lt;/h2>
&lt;p>Before starting the actual training, we have to split up our input data into
training and validation data sets. I’ve chosen to use ~80% of the data for
training and 20% for validation purposes. We have to shuffle our input data as
well to ensure that the model is not affected by ordering issues. The technical
details of the training process can be found in the &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/f419ff4a3462bafc0cb067aa6973dc7280409699/src/nlp.py#L91-L170">GitHub sources&lt;/a>. So now
we’re ready to finally start the training:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main train
INFO | Using already extracted data from data/data.pickle
INFO | Loading pickle dataset
INFO | Parsed 34380 issues and 55832 pull requests (90212 items)
INFO | Training for label 'kind/bug'
INFO | 6980 items selected
INFO | Using 5584 training and 1395 testing texts
INFO | Number of classes: 2
INFO | Vocabulary len: 51772
INFO | Wrote features to file data/features.json
INFO | Using units: 1
INFO | Using activation function: sigmoid
INFO | Created model with 2 layers and 64 units
INFO | Compiling model
INFO | Starting training
Train on 5584 samples, validate on 1395 samples
Epoch 1/1000
5584/5584 - 3s - loss: 0.6895 - acc: 0.6789 - val_loss: 0.6856 - val_acc: 0.6860
Epoch 2/1000
5584/5584 - 2s - loss: 0.6822 - acc: 0.6827 - val_loss: 0.6782 - val_acc: 0.6860
Epoch 3/1000
…
Epoch 68/1000
5584/5584 - 2s - loss: 0.2587 - acc: 0.9257 - val_loss: 0.4847 - val_acc: 0.7728
INFO | Confusion matrix:
[[920 32]
[291 152]]
INFO | Confusion matrix normalized:
[[0.966 0.034]
[0.657 0.343]]
INFO | Saving model to file data/model.h5
INFO | Validation accuracy: 0.7727598547935486, loss: 0.48470408514836355
&lt;/code>&lt;/pre>&lt;p>The output of the &lt;a href="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix&lt;/a> shows us that we’re pretty good on
training accuracy, but the validation accuracy could be a bit higher. We now
could start a hyperparameter tuning to see if we can optimize the output of the
model even further. I will leave that experiment up to you with the hint to the
&lt;code>./main train --tune&lt;/code> flag.&lt;/p>
&lt;p>We saved the model (&lt;code>data/model.h5&lt;/code>), the vectorizer (&lt;code>data/vectorizer.pickle&lt;/code>)
and the feature selector (&lt;code>data/selector.pickle&lt;/code>) to disk to be able to use them
later on for prediction purposes without having a need for additional training
steps.&lt;/p>
&lt;h2 id="a-first-prediction">A first Prediction&lt;/h2>
&lt;p>We are now able to test the model by loading it from disk and predicting some
input text:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main predict --test
INFO | Testing positive text:
Fix concurrent map access panic
Don't watch .mount cgroups to reduce number of inotify watches
Fix NVML initialization race condition
Fix brtfs disk metrics when using a subdirectory of a subvolume
INFO | Got prediction result: 0.9940581321716309
INFO | Matched expected positive prediction result
INFO | Testing negative text:
action required
1. Currently, if users were to explicitly specify CacheSize of 0 for
KMS provider, they would end-up with a provider that caches up to
1000 keys. This PR changes this behavior.
Post this PR, when users supply 0 for CacheSize this will result in
a validation error.
2. CacheSize type was changed from int32 to *int32. This allows
defaulting logic to differentiate between cases where users
explicitly supplied 0 vs. not supplied any value.
3. KMS Provider's endpoint (path to Unix socket) is now validated when
the EncryptionConfiguration files is loaded. This used to be handled
by the GRPCService.
INFO | Got prediction result: 0.1251964420080185
INFO | Matched expected negative prediction result
&lt;/code>&lt;/pre>&lt;p>Both tests are real-world examples which already exist. We could also try
something completely different, like this random tweet I found a couple of
minutes ago:&lt;/p>
&lt;pre>&lt;code>./main predict &amp;quot;My dudes, if you can understand SYN-ACK, you can understand consent&amp;quot;
INFO | Got prediction result: 0.1251964420080185
ERROR | Result is lower than selected threshold 0.6
&lt;/code>&lt;/pre>&lt;p>Looks like it is not classified as bug for a release note, which seems to work.
Selecting a good threshold is also not that easy, but sticking to something &amp;gt;
50% should be the bare minimum.&lt;/p>
&lt;h1 id="automate-everything">Automate Everything&lt;/h1>
&lt;p>The next step is to find some way of automation to continuously update the model
with new data. If I change any source code within my repository, then I’d like
to get feedback about the test results of the model without having a need to run
the training on my own machine. I would like to utilize the GPUs in my
Kubernetes cluster to train faster and automatically update the data set if a PR
got merged.&lt;/p>
&lt;p>With the help of &lt;a href="https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview">Kubeflow pipelines&lt;/a> we can fulfill most of these
requirements. The pipeline I built looks like this:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/kubeflow-pipeline.png" alt="pipeline">&lt;/p>
&lt;p>First, we check out the source code of the PR, which will be passed on as output
artifact to all other steps. Then we incrementally update the API and internal
data before we run the training on an always up-to-date data set. The prediction
test verifies after the training that we did not badly influence the model with
our changes.&lt;/p>
&lt;p>We also built a container image within our pipeline. &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/Dockerfile-deploy">This container image&lt;/a>
copies the previously built model, vectorizer, and selector into a container and
runs &lt;code>./main serve&lt;/code>. When doing this, we spin up a &lt;a href="https://www.kubeflow.org/docs/components/serving/kfserving">kfserving&lt;/a> web server,
which can be used for prediction purposes. Do you want to try it out by yourself? Simply
do a JSON POST request like this and run the prediction against the endpoint:&lt;/p>
&lt;pre>&lt;code>&amp;gt; curl https://kfserving.k8s.saschagrunert.de/v1/models/kubernetes-analysis:predict \
-d '{&amp;quot;text&amp;quot;: &amp;quot;my test text&amp;quot;}'
{&amp;quot;result&amp;quot;: 0.1251964420080185}
&lt;/code>&lt;/pre>&lt;p>The &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/src/kfserver.py">custom kfserving&lt;/a> implementation is pretty straightforward, whereas the
deployment utilizes &lt;a href="https://knative.dev/docs/serving">Knative Serving&lt;/a> and an &lt;a href="https://istio.io">Istio&lt;/a> ingress gateway
under the hood to correctly route the traffic into the cluster and provide the
right set of services.&lt;/p>
&lt;p>The &lt;code>commit-changes&lt;/code> and &lt;code>rollout&lt;/code> step will only run if the pipeline runs on
the &lt;code>master&lt;/code> branch. Those steps make sure that we always have the latest data
set available on the master branch as well as in the kfserving deployment. The
&lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/f419ff4a3462bafc0cb067aa6973dc7280409699/src/rollout.py#L30-L51">rollout step&lt;/a> creates a new canary deployment, which only accepts 50% of the
incoming traffic in the first place. After the canary got deployed successfully,
it will be promoted as the new main instance of the service. This is a great way
to ensure that the deployment works as intended and allows additional testing
after rolling out the canary.&lt;/p>
&lt;p>But how to trigger Kubeflow pipelines when creating a pull request? Kubeflow has
no feature for that right now. That’s why I decided to use &lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow">Prow&lt;/a>,
Kubernetes test-infrastructure project for CI/CD purposes.&lt;/p>
&lt;p>First of all, a &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/f419ff4a3462bafc0cb067aa6973dc7280409699/ci/config.yaml#L45-L61">24h periodic job&lt;/a> ensures that we have at least daily
up-to-date data available within the repository. Then, if we create a pull
request, Prow will run the whole Kubeflow pipeline without committing or rolling
out any changes. If we merge the pull request via Prow, another job runs on the
master and updates the data as well as the deployment. That’s pretty neat, isn’t
it?&lt;/p>
&lt;h1 id="automatic-labeling-of-new-prs">Automatic Labeling of new PRs&lt;/h1>
&lt;p>The prediction API is nice for testing, but now we need a real-world use case.
Prow supports external plugins which can be used to take action on any GitHub
event. I wrote &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/tree/master/pkg">a plugin&lt;/a> which uses the kfserving API to make predictions
based on new pull requests. This means if we now create a new pull request in
the kubernetes-analysis repository, we will see the following:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-1.png" alt="pr 1">&lt;/p>
&lt;hr>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-2.png" alt="pr 2">&lt;/p>
&lt;p>Okay cool, so now let’s change the release note based on a real bug from the
already existing dataset:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-3.png" alt="pr 3">&lt;/p>
&lt;hr>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-4.png" alt="pr 4">&lt;/p>
&lt;p>The bot edits its own comment, predicts it with round about 90% as &lt;code>kind/bug&lt;/code>
and automatically adds the correct label! Now, if we change it back to some
different - obviously wrong - release note:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-5.png" alt="pr 5">&lt;/p>
&lt;hr>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-6.png" alt="pr 6">&lt;/p>
&lt;p>The bot does the work for us, removes the label and informs us what it did!
Finally, if we change the release note to &lt;code>None&lt;/code>:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-7.png" alt="pr 7">&lt;/p>
&lt;hr>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-8.png" alt="pr 8">&lt;/p>
&lt;p>The bot removed the comment, which is nice and reduces the text noise on the PR.
Everything I demonstrated is running inside a single Kubernetes cluster, which
would make it unnecessary at all to expose the kfserving API to the public. This
introduces an indirect API rate limiting because the only usage would be
possible via the Prow bot user.&lt;/p>
&lt;p>If you want to try it out for yourself, feel free to open a &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/issues/new?&amp;amp;template=release-notes-test.md">new test
issue&lt;/a> in &lt;code>kubernetes-analysis&lt;/code>. This works because I enabled the plugin
also for issues rather than only for pull requests.&lt;/p>
&lt;p>So then, we have a running CI bot which is able to classify new release notes
based on a machine learning model. If the bot would run in the official
Kubernetes repository, then we could correct wrong label predictions manually.
This way, the next training iteration would pick up the correction and result in
a continuously improved model over time. All totally automated!&lt;/p>
&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>Thank you for reading down to here! This was my little data science journey
through the Kubernetes GitHub repository. There are a lot of other things to
optimize, for example introducing more classes (than just &lt;code>kind/bug&lt;/code> or nothing)
or automatic hyperparameter tuning with Kubeflows &lt;a href="https://www.kubeflow.org/docs/components/hyperparameter-tuning/hyperparameter">Katib&lt;/a>. If you have any
questions or suggestions, then feel free to get in touch with me anytime. See you
soon!&lt;/p></description></item><item><title>Blog: An Introduction to the K8s-Infrastructure Working Group</title><link>https://kubernetes.io/blog/2020/05/27/an-introduction-to-the-k8s-infrastructure-working-group/</link><pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/05/27/an-introduction-to-the-k8s-infrastructure-working-group/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: &lt;a href="https://twitter.com/kiran_oliver">Kiran &amp;quot;Rin&amp;quot; Oliver&lt;/a> Storyteller, Kubernetes Upstream Marketing Team&lt;/p>
&lt;h1 id="an-introduction-to-the-k8s-infrastructure-working-group">An Introduction to the K8s-Infrastructure Working Group&lt;/h1>
&lt;p>&lt;em>Welcome to part one of a new series introducing the K8s-Infrastructure working group!&lt;/em>&lt;/p>
&lt;p>When Kubernetes was formed in 2014, Google undertook the task of building and maintaining the infrastructure necessary for keeping the project running smoothly. The tools itself were open source, but the Google Cloud Platform project used to run the infrastructure was internal-only, preventing contributors from being able to help out. In August 2018, Google granted the Cloud Native Computing Foundation &lt;a href="https://cloud.google.com/blog/products/gcp/google-cloud-grants-9m-in-credits-for-the-operation-of-the-kubernetes-project">$9M in credits for the operation of Kubernetes&lt;/a>. The sentiment behind this was that a project such as Kubernetes should be both maintained and operated by the community itself rather than by a single vendor.&lt;/p>
&lt;p>A group of community members enthusiastically undertook the task of collaborating on the path forward, realizing that there was a &lt;a href="https://github.com/kubernetes/community/issues/2715">more formal infrastructure necessary&lt;/a>. They joined together as a cross-team working group with ownership spanning across multiple Kubernetes SIGs (Architecture, Contributor Experience, Release, and Testing). &lt;a href="https://twitter.com/spiffxp">Aaron Crickenberger&lt;/a> worked with the Kubernetes Steering Committee to enable the formation of the working group, co-drafting a charter alongside long-time collaborator &lt;a href="https://twitter.com/dims">Davanum Srinivas&lt;/a>, and by 2019 the working group was official.&lt;/p>
&lt;h2 id="what-issues-does-the-k8s-infrastructure-working-group-tackle">What Issues Does the K8s-Infrastructure Working Group Tackle?&lt;/h2>
&lt;p>The team took on the complex task of managing the many moving parts of the infrastructure that sustains Kubernetes as a project.&lt;/p>
&lt;p>The need started with necessity: the first problem they took on was a complete migration of all of the project's infrastructure from Google-owned infrastructure to the Cloud Native Computing Foundation (CNCF). This is being done so that the project is self-sustainable without the need of any direct assistance from individual vendors. This breaks down in the following ways:&lt;/p>
&lt;ul>
&lt;li>Identifying what infrastructure the Kubernetes project depends on.
&lt;ul>
&lt;li>What applications are running?&lt;/li>
&lt;li>Where does it run?&lt;/li>
&lt;li>Where is its source code?&lt;/li>
&lt;li>What is custom built?&lt;/li>
&lt;li>What is off-the-shelf?&lt;/li>
&lt;li>What services depend on each other?&lt;/li>
&lt;li>How is it administered?&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Documenting guidelines and policies for how to run the infrastructure as a community.
&lt;ul>
&lt;li>What are our access policies?&lt;/li>
&lt;li>How do we keep track of billing?&lt;/li>
&lt;li>How do we ensure privacy and security?&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Migrating infrastructure over to the CNCF as-is.
&lt;ul>
&lt;li>What is the path of least resistance to migration?&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Improving the state of the infrastructure for sustainability.
&lt;ul>
&lt;li>Moving from humans running scripts to a more automated GitOps model (YAML all the things!)&lt;/li>
&lt;li>Supporting community members who wish to develop new infrastructure&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Documenting the state of our efforts, better defining goals, and completeness indicators.
&lt;ul>
&lt;li>The project and program management necessary to communicate this work to our &lt;a href="https://kubernetes.io/blog/2020/04/21/contributor-communication/">massive community of contributors&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="the-challenge-of-k8s-infrastructure-is-documentation">The challenge of K8s-Infrastructure is documentation&lt;/h2>
&lt;p>The most crucial problem the working group is trying to tackle is that the project is all volunteer-led. This leads to contributors, chairs, and others involved in the project quickly becoming overscheduled. As a result of this, certain areas such as documentation and organization often lack information, and efforts to progress are taking longer than the group would like to complete.&lt;/p>
&lt;p>Some of the infrastructure that is being migrated over hasn't been updated in a while, and its original authors or directly responsible individuals have moved on from working on Kubernetes. While this is great from the perspective of the fact that the code was able to run untouched for a long period of time, from the perspective of trying to migrate, this makes it difficult to identify how to operate these components, and how to move these infrastructure pieces where they need to be effectively.&lt;/p>
&lt;p>The lack of documentation is being addressed head-on by group member &lt;a href="https://twitter.com/bartsmykla">Bart Smykla&lt;/a>, but there is a definite need for others to support. If you're looking for a way to &lt;a href="https://github.com/kubernetes/community/labels/wg%2Fk8s-infra">get involved&lt;/a> and learn the infrastructure, you can become a new contributor to the working group!&lt;/p>
&lt;h2 id="celebrating-some-working-group-wins">Celebrating some Working Group wins&lt;/h2>
&lt;p>The team has made progress in the last few months that is well worth celebrating.&lt;/p>
&lt;ul>
&lt;li>The K8s-Infrastructure Working Group released an automated billing report that they start every meeting off by reviewing as a group.&lt;/li>
&lt;li>DNS for k8s.io and kubernetes.io are also fully &lt;a href="https://groups.google.com/g/kubernetes-dev/c/LZTYJorGh7c/m/u-ydk-yNEgAJ">community-owned&lt;/a>, with community members able to &lt;a href="https://github.com/kubernetes/k8s.io/issues/new?assignees=&amp;amp;labels=wg%2Fk8s-infra&amp;amp;template=dns-request.md&amp;amp;title=DNS+REQUEST%3A+%3Cyour-dns-record%3E">file issues&lt;/a> to manage records.&lt;/li>
&lt;li>The container registry &lt;a href="https://github.com/kubernetes/k8s.io/tree/master/k8s.gcr.io">k8s.gcr.io&lt;/a> is also fully community-owned and available for all Kubernetes subprojects to use.&lt;/li>
&lt;li>The Kubernetes &lt;a href="https://github.com/kubernetes/publishing-bot">publishing-bot&lt;/a> responsible for keeping k8s.io/kubernetes/staging repositories published to their own top-level repos (For example: &lt;a href="https://github.com/kubernetes/api">kubernetes/api&lt;/a>) runs on a community-owned cluster.&lt;/li>
&lt;li>The gcsweb.k8s.io service used to provide anonymous access to GCS buckets for kubernetes artifacts runs on a community-owned cluster.&lt;/li>
&lt;li>There is also an automated process of promoting all our container images. This includes a fully documented infrastructure, managed by the Kubernetes community, with automated processes for provisioning permissions.&lt;/li>
&lt;/ul>
&lt;p>These are just a few of the things currently happening in the K8s Infrastructure working group.&lt;/p>
&lt;p>If you're interested in getting involved, be sure to join the &lt;a href="https://app.slack.com/client/T09NY5SBT/CCK68P2Q2">#wg-K8s-infra Slack Channel&lt;/a>. Meetings are 60 minutes long, and are held every other Wednesday at 8:30 AM PT/16:30 UTC.&lt;/p>
&lt;p>Join to help with the documentation, stay to learn about the amazing infrastructure supporting the Kubernetes community.&lt;/p></description></item><item><title>Blog: WSL+Docker: Kubernetes on the Windows Desktop</title><link>https://kubernetes.io/blog/2020/05/21/wsl-docker-kubernetes-on-the-windows-desktop/</link><pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/05/21/wsl-docker-kubernetes-on-the-windows-desktop/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: &lt;a href="https://twitter.com/nunixtech">Nuno do Carmo&lt;/a> Docker Captain and WSL Corsair; &lt;a href="https://twitter.com/idvoretskyi">Ihor Dvoretskyi&lt;/a>, Developer Advocate, Cloud Native Computing Foundation&lt;/p>
&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>New to Windows 10 and WSL2, or new to Docker and Kubernetes? Welcome to this blog post where we will install from scratch Kubernetes in Docker &lt;a href="https://kind.sigs.k8s.io/">KinD&lt;/a> and &lt;a href="https://minikube.sigs.k8s.io/docs/">Minikube&lt;/a>.&lt;/p>
&lt;h1 id="why-kubernetes-on-windows">Why Kubernetes on Windows?&lt;/h1>
&lt;p>For the last few years, Kubernetes became a de-facto standard platform for running containerized services and applications in distributed environments. While a wide variety of distributions and installers exist to deploy Kubernetes in the cloud environments (public, private or hybrid), or within the bare metal environments, there is still a need to deploy and run Kubernetes locally, for example, on the developer's workstation.&lt;/p>
&lt;p>Kubernetes has been originally designed to be deployed and used in the Linux environments. However, a good number of users (and not only application developers) use Windows OS as their daily driver. When Microsoft revealed WSL - &lt;a href="https://docs.microsoft.com/en-us/windows/wsl/">the Windows Subsystem for Linux&lt;/a>, the line between Windows and Linux environments became even less visible.&lt;/p>
&lt;p>Also, WSL brought an ability to run Kubernetes on Windows almost seamlessly!&lt;/p>
&lt;p>Below, we will cover in brief how to install and use various solutions to run Kubernetes locally.&lt;/p>
&lt;h1 id="prerequisites">Prerequisites&lt;/h1>
&lt;p>Since we will explain how to install KinD, we won't go into too much detail around the installation of KinD's dependencies.&lt;/p>
&lt;p>However, here is the list of the prerequisites needed and their version/lane:&lt;/p>
&lt;ul>
&lt;li>OS: Windows 10 version 2004, Build 19041&lt;/li>
&lt;li>&lt;a href="https://docs.microsoft.com/en-us/windows/wsl/wsl2-install">WSL2 enabled&lt;/a>
&lt;ul>
&lt;li>In order to install the distros as WSL2 by default, once WSL2 installed, run the command &lt;code>wsl.exe --set-default-version 2&lt;/code> in Powershell&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>WSL2 distro installed from the Windows Store - the distro used is Ubuntu-18.04&lt;/li>
&lt;li>&lt;a href="https://hub.docker.com/editions/community/docker-ce-desktop-windows">Docker Desktop for Windows&lt;/a>, stable channel - the version used is 2.2.0.4&lt;/li>
&lt;li>[Optional] Microsoft Terminal installed from the Windows Store
&lt;ul>
&lt;li>Open the Windows store and type &amp;quot;Terminal&amp;quot; in the search, it will be (normally) the first option&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-windows-store-terminal.png" alt="Windows Store Terminal">&lt;/p>
&lt;p>And that's actually it. For Docker Desktop for Windows, no need to configure anything yet as we will explain it in the next section.&lt;/p>
&lt;h1 id="wsl2-first-contact">WSL2: First contact&lt;/h1>
&lt;p>Once everything is installed, we can launch the WSL2 terminal from the Start menu, and type &amp;quot;Ubuntu&amp;quot; for searching the applications and documents:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-start-menu-search.png" alt="Start Menu Search">&lt;/p>
&lt;p>Once found, click on the name and it will launch the default Windows console with the Ubuntu bash shell running.&lt;/p>
&lt;p>Like for any normal Linux distro, you need to create a user and set a password:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-user-password.png" alt="User-Password">&lt;/p>
&lt;h2 id="optional-update-the-sudoers">[Optional] Update the &lt;code>sudoers&lt;/code>&lt;/h2>
&lt;p>As we are working, normally, on our local computer, it might be nice to update the &lt;code>sudoers&lt;/code> and set the group &lt;code>%sudo&lt;/code> to be password-less:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Edit the sudoers with the visudo command&lt;/span>
sudo visudo
&lt;span style="color:#080;font-style:italic"># Change the %sudo group to be password-less&lt;/span>
%sudo &lt;span style="color:#b8860b">ALL&lt;/span>&lt;span style="color:#666">=(&lt;/span>ALL:ALL&lt;span style="color:#666">)&lt;/span> NOPASSWD: ALL
&lt;span style="color:#080;font-style:italic"># Press CTRL+X to exit&lt;/span>
&lt;span style="color:#080;font-style:italic"># Press Y to save&lt;/span>
&lt;span style="color:#080;font-style:italic"># Press Enter to confirm&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-visudo.png" alt="visudo">&lt;/p>
&lt;h2 id="update-ubuntu">Update Ubuntu&lt;/h2>
&lt;p>Before we move to the Docker Desktop settings, let's update our system and ensure we start in the best conditions:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Update the repositories and list of the packages available&lt;/span>
sudo apt update
&lt;span style="color:#080;font-style:italic"># Update the system based on the packages installed &amp;gt; the &amp;#34;-y&amp;#34; will approve the change automatically&lt;/span>
sudo apt upgrade -y
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-apt-update-upgrade.png" alt="apt-update-upgrade">&lt;/p>
&lt;h1 id="docker-desktop-faster-with-wsl2">Docker Desktop: faster with WSL2&lt;/h1>
&lt;p>Before we move into the settings, let's do a small test, it will display really how cool the new integration with Docker Desktop is:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Try to see if the docker cli and daemon are installed&lt;/span>
docker version
&lt;span style="color:#080;font-style:italic"># Same for kubectl&lt;/span>
kubectl version
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-kubectl-error.png" alt="kubectl-error">&lt;/p>
&lt;p>You got an error? Perfect! It's actually good news, so let's now move on to the settings.&lt;/p>
&lt;h2 id="docker-desktop-settings-enable-wsl2-integration">Docker Desktop settings: enable WSL2 integration&lt;/h2>
&lt;p>First let's start Docker Desktop for Windows if it's not still the case. Open the Windows start menu and type &amp;quot;docker&amp;quot;, click on the name to start the application:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-start.png" alt="docker-start">&lt;/p>
&lt;p>You should now see the Docker icon with the other taskbar icons near the clock:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-taskbar.png" alt="docker-taskbar">&lt;/p>
&lt;p>Now click on the Docker icon and choose settings. A new window will appear:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-settings-general.png" alt="docker-settings-general">&lt;/p>
&lt;p>By default, the WSL2 integration is not active, so click the &amp;quot;Enable the experimental WSL 2 based engine&amp;quot; and click &amp;quot;Apply &amp;amp; Restart&amp;quot;:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-settings-wsl2-activated.png" alt="docker-settings-wsl2">&lt;/p>
&lt;p>What this feature did behind the scenes was to create two new distros in WSL2, containing and running all the needed backend sockets, daemons and also the CLI tools (read: docker and kubectl command).&lt;/p>
&lt;p>Still, this first setting is still not enough to run the commands inside our distro. If we try, we will have the same error as before.&lt;/p>
&lt;p>In order to fix it, and finally be able to use the commands, we need to tell the Docker Desktop to &amp;quot;attach&amp;quot; itself to our distro also:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-resources-wsl-integration.png" alt="docker-resources-wsl">&lt;/p>
&lt;p>Let's now switch back to our WSL2 terminal and see if we can (finally) launch the commands:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Try to see if the docker cli and daemon are installed&lt;/span>
docker version
&lt;span style="color:#080;font-style:italic"># Same for kubectl&lt;/span>
kubectl version
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-kubectl-success.png" alt="docker-kubectl-success">&lt;/p>
&lt;blockquote>
&lt;p>Tip: if nothing happens, restart Docker Desktop and restart the WSL process in Powershell: &lt;code>Restart-Service LxssManager&lt;/code> and launch a new Ubuntu session&lt;/p>
&lt;/blockquote>
&lt;p>And success! The basic settings are now done and we move to the installation of KinD.&lt;/p>
&lt;h1 id="kind-kubernetes-made-easy-in-a-container">KinD: Kubernetes made easy in a container&lt;/h1>
&lt;p>Right now, we have Docker that is installed, configured and the last test worked fine.&lt;/p>
&lt;p>However, if we look carefully at the &lt;code>kubectl&lt;/code> command, it found the &amp;quot;Client Version&amp;quot; (1.15.5), but it didn't find any server.&lt;/p>
&lt;p>This is normal as we didn't enable the Docker Kubernetes cluster. So let's install KinD and create our first cluster.&lt;/p>
&lt;p>And as sources are always important to mention, we will follow (partially) the how-to on the &lt;a href="https://kind.sigs.k8s.io/docs/user/quick-start/">official KinD website&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Download the latest version of KinD&lt;/span>
curl -Lo ./kind https://github.com/kubernetes-sigs/kind/releases/download/v0.7.0/kind-&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>uname&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>-amd64
&lt;span style="color:#080;font-style:italic"># Make the binary executable&lt;/span>
chmod +x ./kind
&lt;span style="color:#080;font-style:italic"># Move the binary to your executable path&lt;/span>
sudo mv ./kind /usr/local/bin/
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-install.png" alt="kind-install">&lt;/p>
&lt;h2 id="kind-the-first-cluster">KinD: the first cluster&lt;/h2>
&lt;p>We are ready to create our first cluster:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Check if the KUBECONFIG is not set&lt;/span>
&lt;span style="color:#a2f">echo&lt;/span> &lt;span style="color:#b8860b">$KUBECONFIG&lt;/span>
&lt;span style="color:#080;font-style:italic"># Check if the .kube directory is created &amp;gt; if not, no need to create it&lt;/span>
ls &lt;span style="color:#b8860b">$HOME&lt;/span>/.kube
&lt;span style="color:#080;font-style:italic"># Create the cluster and give it a name (optional)&lt;/span>
kind create cluster --name wslkind
&lt;span style="color:#080;font-style:italic"># Check if the .kube has been created and populated with files&lt;/span>
ls &lt;span style="color:#b8860b">$HOME&lt;/span>/.kube
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-cluster-create.png" alt="kind-cluster-create">&lt;/p>
&lt;blockquote>
&lt;p>Tip: as you can see, the Terminal was changed so the nice icons are all displayed&lt;/p>
&lt;/blockquote>
&lt;p>The cluster has been successfully created, and because we are using Docker Desktop, the network is all set for us to use &amp;quot;as is&amp;quot;.&lt;/p>
&lt;p>So we can open the &lt;code>Kubernetes master&lt;/code> URL in our Windows browser:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-k8s-master.png" alt="kind-browser-k8s-master">&lt;/p>
&lt;p>And this is the real strength from Docker Desktop for Windows with the WSL2 backend. Docker really did an amazing integration.&lt;/p>
&lt;h2 id="kind-counting-1-2-3">KinD: counting 1 - 2 - 3&lt;/h2>
&lt;p>Our first cluster was created and it's the &amp;quot;normal&amp;quot; one node cluster:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Check how many nodes it created&lt;/span>
kubectl get nodes
&lt;span style="color:#080;font-style:italic"># Check the services for the whole cluster&lt;/span>
kubectl get all --all-namespaces
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-list-nodes-services.png" alt="kind-list-nodes-services">&lt;/p>
&lt;p>While this will be enough for most people, let's leverage one of the coolest feature, multi-node clustering:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Delete the existing cluster&lt;/span>
kind delete cluster --name wslkind
&lt;span style="color:#080;font-style:italic"># Create a config file for a 3 nodes cluster&lt;/span>
cat &lt;span style="color:#b44">&amp;lt;&amp;lt; EOF &amp;gt; kind-3nodes.yaml
&lt;/span>&lt;span style="color:#b44">kind: Cluster
&lt;/span>&lt;span style="color:#b44">apiVersion: kind.x-k8s.io/v1alpha4
&lt;/span>&lt;span style="color:#b44">nodes:
&lt;/span>&lt;span style="color:#b44"> - role: control-plane
&lt;/span>&lt;span style="color:#b44"> - role: worker
&lt;/span>&lt;span style="color:#b44"> - role: worker
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;span style="color:#080;font-style:italic"># Create a new cluster with the config file&lt;/span>
kind create cluster --name wslkindmultinodes --config ./kind-3nodes.yaml
&lt;span style="color:#080;font-style:italic"># Check how many nodes it created&lt;/span>
kubectl get nodes
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-cluster-create-multinodes.png" alt="kind-cluster-create-multinodes">&lt;/p>
&lt;blockquote>
&lt;p>Tip: depending on how fast we run the &amp;quot;get nodes&amp;quot; command, it can be that not all the nodes are ready, wait few seconds and run it again, everything should be ready&lt;/p>
&lt;/blockquote>
&lt;p>And that's it, we have created a three-node cluster, and if we look at the services one more time, we will see several that have now three replicas:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Check the services for the whole cluster&lt;/span>
kubectl get all --all-namespaces
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-list-services-multinodes.png" alt="wsl2-kind-list-services-multinodes">&lt;/p>
&lt;h2 id="kind-can-i-see-a-nice-dashboard">KinD: can I see a nice dashboard?&lt;/h2>
&lt;p>Working on the command line is always good and very insightful. However, when dealing with Kubernetes we might want, at some point, to have a visual overview.&lt;/p>
&lt;p>For that, the &lt;a href="https://github.com/kubernetes/dashboard">Kubernetes Dashboard&lt;/a> project has been created. The installation and first connection test is quite fast, so let's do it:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Install the Dashboard application into our cluster&lt;/span>
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc6/aio/deploy/recommended.yaml
&lt;span style="color:#080;font-style:italic"># Check the resources it created based on the new namespace created&lt;/span>
kubectl get all -n kubernetes-dashboard
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-install-dashboard.png" alt="kind-install-dashboard">&lt;/p>
&lt;p>As it created a service with a ClusterIP (read: internal network address), we cannot reach it if we type the URL in our Windows browser:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-dashboard-error.png" alt="kind-browse-dashboard-error">&lt;/p>
&lt;p>That's because we need to create a temporary proxy:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Start a kubectl proxy&lt;/span>
kubectl proxy
&lt;span style="color:#080;font-style:italic"># Enter the URL on your browser: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-dashboard-success.png" alt="kind-browse-dashboard-success">&lt;/p>
&lt;p>Finally to login, we can either enter a Token, which we didn't create, or enter the &lt;code>kubeconfig&lt;/code> file from our Cluster.&lt;/p>
&lt;p>If we try to login with the &lt;code>kubeconfig&lt;/code>, we will get the error &amp;quot;Internal error (500): Not enough data to create auth info structure&amp;quot;. This is due to the lack of credentials in the &lt;code>kubeconfig&lt;/code> file.&lt;/p>
&lt;p>So to avoid you ending with the same error, let's follow the &lt;a href="https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md">recommended RBAC approach&lt;/a>.&lt;/p>
&lt;p>Let's open a new WSL2 session:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Create a new ServiceAccount&lt;/span>
kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;span style="color:#b44">apiVersion: v1
&lt;/span>&lt;span style="color:#b44">kind: ServiceAccount
&lt;/span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;span style="color:#b44"> name: admin-user
&lt;/span>&lt;span style="color:#b44"> namespace: kubernetes-dashboard
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;span style="color:#080;font-style:italic"># Create a ClusterRoleBinding for the ServiceAccount&lt;/span>
kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;span style="color:#b44">apiVersion: rbac.authorization.k8s.io/v1
&lt;/span>&lt;span style="color:#b44">kind: ClusterRoleBinding
&lt;/span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;span style="color:#b44"> name: admin-user
&lt;/span>&lt;span style="color:#b44">roleRef:
&lt;/span>&lt;span style="color:#b44"> apiGroup: rbac.authorization.k8s.io
&lt;/span>&lt;span style="color:#b44"> kind: ClusterRole
&lt;/span>&lt;span style="color:#b44"> name: cluster-admin
&lt;/span>&lt;span style="color:#b44">subjects:
&lt;/span>&lt;span style="color:#b44">- kind: ServiceAccount
&lt;/span>&lt;span style="color:#b44"> name: admin-user
&lt;/span>&lt;span style="color:#b44"> namespace: kubernetes-dashboard
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-dashboard-rbac-serviceaccount.png" alt="kind-browse-dashboard-rbac-serviceaccount">&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Get the Token for the ServiceAccount&lt;/span>
kubectl -n kubernetes-dashboard describe secret &lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>kubectl -n kubernetes-dashboard get secret | grep admin-user | awk &lt;span style="color:#b44">&amp;#39;{print $1}&amp;#39;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>
&lt;span style="color:#080;font-style:italic"># Copy the token and copy it into the Dashboard login and press &amp;#34;Sign in&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-dashboard-login-success.png" alt="kind-browse-dashboard-login-success">&lt;/p>
&lt;p>Success! And let's see our nodes listed also:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-dashboard-browse-nodes.png" alt="kind-browse-dashboard-browse-nodes">&lt;/p>
&lt;p>A nice and shiny three nodes appear.&lt;/p>
&lt;h1 id="minikube-kubernetes-from-everywhere">Minikube: Kubernetes from everywhere&lt;/h1>
&lt;p>Right now, we have Docker that is installed, configured and the last test worked fine.&lt;/p>
&lt;p>However, if we look carefully at the &lt;code>kubectl&lt;/code> command, it found the &amp;quot;Client Version&amp;quot; (1.15.5), but it didn't find any server.&lt;/p>
&lt;p>This is normal as we didn't enable the Docker Kubernetes cluster. So let's install Minikube and create our first cluster.&lt;/p>
&lt;p>And as sources are always important to mention, we will follow (partially) the how-to from the &lt;a href="https://kubernetes.io/docs/tasks/tools/install-minikube/">Kubernetes.io website&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Download the latest version of Minikube&lt;/span>
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
&lt;span style="color:#080;font-style:italic"># Make the binary executable&lt;/span>
chmod +x ./minikube
&lt;span style="color:#080;font-style:italic"># Move the binary to your executable path&lt;/span>
sudo mv ./minikube /usr/local/bin/
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-install.png" alt="minikube-install">&lt;/p>
&lt;h2 id="minikube-updating-the-host">Minikube: updating the host&lt;/h2>
&lt;p>If we follow the how-to, it states that we should use the &lt;code>--driver=none&lt;/code> flag in order to run Minikube directly on the host and Docker.&lt;/p>
&lt;p>Unfortunately, we will get an error about &amp;quot;conntrack&amp;quot; being required to run Kubernetes v 1.18:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Create a minikube one node cluster&lt;/span>
minikube start --driver&lt;span style="color:#666">=&lt;/span>none
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-start-error.png" alt="minikube-start-error">&lt;/p>
&lt;blockquote>
&lt;p>Tip: as you can see, the Terminal was changed so the nice icons are all displayed&lt;/p>
&lt;/blockquote>
&lt;p>So let's fix the issue by installing the missing package:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Install the conntrack package&lt;/span>
sudo apt install -y conntrack
&lt;/code>&lt;/pre>&lt;/div>&lt;p>![minikube-install-conntrack](/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-install conntrack.png)&lt;/p>
&lt;p>Let's try to launch it again:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Create a minikube one node cluster&lt;/span>
minikube start --driver&lt;span style="color:#666">=&lt;/span>none
&lt;span style="color:#080;font-style:italic"># We got a permissions error &amp;gt; try again with sudo&lt;/span>
sudo minikube start --driver&lt;span style="color:#666">=&lt;/span>none
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-start-error-systemd.png" alt="minikube-start-error-systemd">&lt;/p>
&lt;p>Ok, this error cloud be problematic ... in the past. Luckily for us, there's a solution&lt;/p>
&lt;h2 id="minikube-enabling-systemd">Minikube: enabling SystemD&lt;/h2>
&lt;p>In order to enable SystemD on WSL2, we will apply the &lt;a href="https://forum.snapcraft.io/t/running-snaps-on-wsl2-insiders-only-for-now/13033">scripts&lt;/a> from &lt;a href="https://twitter.com/diddledan">Daniel Llewellyn&lt;/a>.&lt;/p>
&lt;p>I invite you to read the full blog post and how he came to the solution, and the various iterations he did to fix several issues.&lt;/p>
&lt;p>So in a nutshell, here are the commands:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Install the needed packages&lt;/span>
sudo apt install -yqq daemonize dbus-user-session fontconfig
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-systemd-packages.png" alt="minikube-systemd-packages">&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Create the start-systemd-namespace script&lt;/span>
sudo vi /usr/sbin/start-systemd-namespace
&lt;span style="color:#080;font-style:italic">#!/bin/bash&lt;/span>
&lt;span style="color:#b8860b">SYSTEMD_PID&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>ps -ef | grep &lt;span style="color:#b44">&amp;#39;/lib/systemd/systemd --system-unit=basic.target$&amp;#39;&lt;/span> | grep -v unshare | awk &lt;span style="color:#b44">&amp;#39;{print $2}&amp;#39;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> &lt;span style="color:#666">[&lt;/span> -z &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span> &lt;span style="color:#666">||&lt;/span> &lt;span style="color:#666">[&lt;/span> &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> !&lt;span style="color:#666">=&lt;/span> &lt;span style="color:#b44">&amp;#34;1&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span>; &lt;span style="color:#a2f;font-weight:bold">then&lt;/span>
&lt;span style="color:#a2f">export&lt;/span> &lt;span style="color:#b8860b">PRE_NAMESPACE_PATH&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$PATH&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>
&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#a2f">set&lt;/span> -o posix; &lt;span style="color:#a2f">set&lt;/span>&lt;span style="color:#666">)&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^BASH&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^DIRSTACK=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^EUID=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^GROUPS=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^HOME=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^HOSTNAME=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^HOSTTYPE=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^IFS=&amp;#39;.*&amp;#34;&lt;/span>&lt;span style="color:#b44">$&amp;#39;\n&amp;#39;&lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;#39;&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^LANG=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^LOGNAME=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^MACHTYPE=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^NAME=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^OPTERR=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^OPTIND=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^OSTYPE=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^PIPESTATUS=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^POSIXLY_CORRECT=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^PPID=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^PS1=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^PS4=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^SHELL=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^SHELLOPTS=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^SHLVL=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^SYSTEMD_PID=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^UID=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^USER=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^_=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> cat - &amp;gt; &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$HOME&lt;/span>&lt;span style="color:#b44">/.systemd-env&amp;#34;&lt;/span>
&lt;span style="color:#a2f">echo&lt;/span> &lt;span style="color:#b44">&amp;#34;PATH=&amp;#39;&lt;/span>&lt;span style="color:#b8860b">$PATH&lt;/span>&lt;span style="color:#b44">&amp;#39;&amp;#34;&lt;/span> &amp;gt;&amp;gt; &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$HOME&lt;/span>&lt;span style="color:#b44">/.systemd-env&amp;#34;&lt;/span>
&lt;span style="color:#a2f">exec&lt;/span> sudo /usr/sbin/enter-systemd-namespace &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$BASH_EXECUTION_STRING&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">fi&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> &lt;span style="color:#666">[&lt;/span> -n &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$PRE_NAMESPACE_PATH&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span>; &lt;span style="color:#a2f;font-weight:bold">then&lt;/span>
&lt;span style="color:#a2f">export&lt;/span> &lt;span style="color:#b8860b">PATH&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$PRE_NAMESPACE_PATH&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">fi&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Create the enter-systemd-namespace&lt;/span>
sudo vi /usr/sbin/enter-systemd-namespace
&lt;span style="color:#080;font-style:italic">#!/bin/bash&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> &lt;span style="color:#666">[&lt;/span> &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$UID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> !&lt;span style="color:#666">=&lt;/span> &lt;span style="color:#666">0&lt;/span> &lt;span style="color:#666">]&lt;/span>; &lt;span style="color:#a2f;font-weight:bold">then&lt;/span>
&lt;span style="color:#a2f">echo&lt;/span> &lt;span style="color:#b44">&amp;#34;You need to run &lt;/span>&lt;span style="color:#b8860b">$0&lt;/span>&lt;span style="color:#b44"> through sudo&amp;#34;&lt;/span>
&lt;span style="color:#a2f">exit&lt;/span> &lt;span style="color:#666">1&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">fi&lt;/span>
&lt;span style="color:#b8860b">SYSTEMD_PID&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>ps -ef | grep &lt;span style="color:#b44">&amp;#39;/lib/systemd/systemd --system-unit=basic.target$&amp;#39;&lt;/span> | grep -v unshare | awk &lt;span style="color:#b44">&amp;#39;{print $2}&amp;#39;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> &lt;span style="color:#666">[&lt;/span> -z &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span>; &lt;span style="color:#a2f;font-weight:bold">then&lt;/span>
/usr/sbin/daemonize /usr/bin/unshare --fork --pid --mount-proc /lib/systemd/systemd --system-unit&lt;span style="color:#666">=&lt;/span>basic.target
&lt;span style="color:#a2f;font-weight:bold">while&lt;/span> &lt;span style="color:#666">[&lt;/span> -z &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span>; &lt;span style="color:#a2f;font-weight:bold">do&lt;/span>
&lt;span style="color:#b8860b">SYSTEMD_PID&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>ps -ef | grep &lt;span style="color:#b44">&amp;#39;/lib/systemd/systemd --system-unit=basic.target$&amp;#39;&lt;/span> | grep -v unshare | awk &lt;span style="color:#b44">&amp;#39;{print $2}&amp;#39;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">done&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">fi&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> &lt;span style="color:#666">[&lt;/span> -n &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span> &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#666">[&lt;/span> &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> !&lt;span style="color:#666">=&lt;/span> &lt;span style="color:#b44">&amp;#34;1&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span>; &lt;span style="color:#a2f;font-weight:bold">then&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> &lt;span style="color:#666">[&lt;/span> -n &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$1&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span> &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#666">[&lt;/span> &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$1&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> !&lt;span style="color:#666">=&lt;/span> &lt;span style="color:#b44">&amp;#34;bash --login&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span> &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#666">[&lt;/span> &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$1&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> !&lt;span style="color:#666">=&lt;/span> &lt;span style="color:#b44">&amp;#34;/bin/bash --login&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span>; &lt;span style="color:#a2f;font-weight:bold">then&lt;/span>
&lt;span style="color:#a2f">exec&lt;/span> /usr/bin/nsenter -t &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> -a &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> /usr/bin/sudo -H -u &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SUDO_USER&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> /bin/bash -c &lt;span style="color:#b44">&amp;#39;set -a; source &amp;#34;$HOME/.systemd-env&amp;#34;; set +a; exec bash -c &amp;#39;&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>&lt;span style="color:#a2f">printf&lt;/span> &lt;span style="color:#b44">&amp;#34;%q&amp;#34;&lt;/span> &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$@&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">else&lt;/span>
&lt;span style="color:#a2f">exec&lt;/span> /usr/bin/nsenter -t &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> -a &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> /bin/login -p -f &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SUDO_USER&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> &lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>/bin/cat &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$HOME&lt;/span>&lt;span style="color:#b44">/.systemd-env&amp;#34;&lt;/span> | grep -v &lt;span style="color:#b44">&amp;#34;^PATH=&amp;#34;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">fi&lt;/span>
&lt;span style="color:#a2f">echo&lt;/span> &lt;span style="color:#b44">&amp;#34;Existential crisis&amp;#34;&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">fi&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Edit the permissions of the enter-systemd-namespace script&lt;/span>
sudo chmod +x /usr/sbin/enter-systemd-namespace
&lt;span style="color:#080;font-style:italic"># Edit the bash.bashrc file&lt;/span>
sudo sed -i 2a&lt;span style="color:#b44">&amp;#34;# Start or enter a PID namespace in WSL2\nsource /usr/sbin/start-systemd-namespace\n&amp;#34;&lt;/span> /etc/bash.bashrc
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-systemd-files.png" alt="minikube-systemd-files">&lt;/p>
&lt;p>Finally, exit and launch a new session. You &lt;strong>do not&lt;/strong> need to stop WSL2, a new session is enough:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-systemd-enabled.png" alt="minikube-systemd-enabled">&lt;/p>
&lt;h2 id="minikube-the-first-cluster">Minikube: the first cluster&lt;/h2>
&lt;p>We are ready to create our first cluster:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Check if the KUBECONFIG is not set&lt;/span>
&lt;span style="color:#a2f">echo&lt;/span> &lt;span style="color:#b8860b">$KUBECONFIG&lt;/span>
&lt;span style="color:#080;font-style:italic"># Check if the .kube directory is created &amp;gt; if not, no need to create it&lt;/span>
ls &lt;span style="color:#b8860b">$HOME&lt;/span>/.kube
&lt;span style="color:#080;font-style:italic"># Check if the .minikube directory is created &amp;gt; if yes, delete it&lt;/span>
ls &lt;span style="color:#b8860b">$HOME&lt;/span>/.minikube
&lt;span style="color:#080;font-style:italic"># Create the cluster with sudo&lt;/span>
sudo minikube start --driver&lt;span style="color:#666">=&lt;/span>none
&lt;/code>&lt;/pre>&lt;/div>&lt;p>In order to be able to use &lt;code>kubectl&lt;/code> with our user, and not &lt;code>sudo&lt;/code>, Minikube recommends running the &lt;code>chown&lt;/code> command:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Change the owner of the .kube and .minikube directories&lt;/span>
sudo chown -R &lt;span style="color:#b8860b">$USER&lt;/span> &lt;span style="color:#b8860b">$HOME&lt;/span>/.kube &lt;span style="color:#b8860b">$HOME&lt;/span>/.minikube
&lt;span style="color:#080;font-style:italic"># Check the access and if the cluster is running&lt;/span>
kubectl cluster-info
&lt;span style="color:#080;font-style:italic"># Check the resources created&lt;/span>
kubectl get all --all-namespaces
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-start-fixed.png" alt="minikube-start-fixed">&lt;/p>
&lt;p>The cluster has been successfully created, and Minikube used the WSL2 IP, which is great for several reasons, and one of them is that we can open the &lt;code>Kubernetes master&lt;/code> URL in our Windows browser:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-browse-k8s-master.png" alt="minikube-browse-k8s-master">&lt;/p>
&lt;p>And the real strength of WSL2 integration, the port &lt;code>8443&lt;/code> once open on WSL2 distro, it actually forwards it to Windows, so instead of the need to remind the IP address, we can also reach the &lt;code>Kubernetes master&lt;/code> URL via &lt;code>localhost&lt;/code>:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-browse-k8s-master-localhost.png" alt="minikube-browse-k8s-master-localhost">&lt;/p>
&lt;h2 id="minikube-can-i-see-a-nice-dashboard">Minikube: can I see a nice dashboard?&lt;/h2>
&lt;p>Working on the command line is always good and very insightful. However, when dealing with Kubernetes we might want, at some point, to have a visual overview.&lt;/p>
&lt;p>For that, Minikube embeded the &lt;a href="https://github.com/kubernetes/dashboard">Kubernetes Dashboard&lt;/a>. Thanks to it, running and accessing the Dashboard is very simple:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Enable the Dashboard service&lt;/span>
sudo minikube dashboard
&lt;span style="color:#080;font-style:italic"># Access the Dashboard from a browser on Windows side&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-browse-dashboard.png" alt="minikube-browse-dashboard">&lt;/p>
&lt;p>The command creates also a proxy, which means that once we end the command, by pressing &lt;code>CTRL+C&lt;/code>, the Dashboard will no more be accessible.&lt;/p>
&lt;p>Still, if we look at the namespace &lt;code>kubernetes-dashboard&lt;/code>, we will see that the service is still created:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Get all the services from the dashboard namespace&lt;/span>
kubectl get all --namespace kubernetes-dashboard
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-dashboard-get-all.png" alt="minikube-dashboard-get-all">&lt;/p>
&lt;p>Let's edit the service and change it's type to &lt;code>LoadBalancer&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Edit the Dashoard service&lt;/span>
kubectl edit service/kubernetes-dashboard --namespace kubernetes-dashboard
&lt;span style="color:#080;font-style:italic"># Go to the very end and remove the last 2 lines&lt;/span>
status:
loadBalancer: &lt;span style="color:#666">{}&lt;/span>
&lt;span style="color:#080;font-style:italic"># Change the type from ClusterIO to LoadBalancer&lt;/span>
type: LoadBalancer
&lt;span style="color:#080;font-style:italic"># Save the file&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-dashboard-type-loadbalancer.png" alt="minikube-dashboard-type-loadbalancer">&lt;/p>
&lt;p>Check again the Dashboard service and let's access the Dashboard via the LoadBalancer:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Get all the services from the dashboard namespace&lt;/span>
kubectl get all --namespace kubernetes-dashboard
&lt;span style="color:#080;font-style:italic"># Access the Dashboard from a browser on Windows side with the URL: localhost:&amp;lt;port exposed&amp;gt;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-browse-dashboard-loadbalancer.png" alt="minikube-browse-dashboard-loadbalancer">&lt;/p>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>It's clear that we are far from done as we could have some LoadBalancing implemented and/or other services (storage, ingress, registry, etc...).&lt;/p>
&lt;p>Concerning Minikube on WSL2, as it needed to enable SystemD, we can consider it as an intermediate level to be implemented.&lt;/p>
&lt;p>So with two solutions, what could be the &amp;quot;best for you&amp;quot;? Both bring their own advantages and inconveniences, so here an overview from our point of view solely:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Criteria&lt;/th>
&lt;th>KinD&lt;/th>
&lt;th>Minikube&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Installation on WSL2&lt;/td>
&lt;td>Very Easy&lt;/td>
&lt;td>Medium&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Multi-node&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Plugins&lt;/td>
&lt;td>Manual install&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Persistence&lt;/td>
&lt;td>Yes, however not designed for&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Alternatives&lt;/td>
&lt;td>K3d&lt;/td>
&lt;td>Microk8s&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>We hope you could have a real taste of the integration between the different components: WSL2 - Docker Desktop - KinD/Minikube. And that gave you some ideas or, even better, some answers to your Kubernetes workflows with KinD and/or Minikube on Windows and WSL2.&lt;/p>
&lt;p>See you soon for other adventures in the Kubernetes ocean.&lt;/p>
&lt;p>&lt;a href="https://twitter.com/nunixtech">Nuno&lt;/a> &amp;amp; &lt;a href="https://twitter.com/idvoretskyi">Ihor&lt;/a>&lt;/p></description></item><item><title>Blog: How Docs Handle Third Party and Dual Sourced Content</title><link>https://kubernetes.io/blog/2020/05/third-party-dual-sourced-content/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/05/third-party-dual-sourced-content/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Zach Corleissen, Cloud Native Computing Foundation&lt;/p>
&lt;p>&lt;em>Editor's note: Zach is one of the chairs for the Kubernetes documentation special interest group (SIG Docs).&lt;/em>&lt;/p>
&lt;p>Late last summer, SIG Docs started a community conversation about third party content in Kubernetes docs. This conversation became a &lt;a href="https://github.com/kubernetes/enhancements/pull/1327">Kubernetes Enhancement Proposal&lt;/a> (KEP) and, after five months for review and comment, SIG Architecture approved the KEP as a &lt;a href="https://kubernetes.io/docs/contribute/style/content-guide/">content guide&lt;/a> for Kubernetes docs.&lt;/p>
&lt;p>Here's how Kubernetes docs handle third party content now:&lt;/p>
&lt;blockquote>
&lt;p>Links to active content in the Kubernetes project (projects in the kubernetes and kubernetes-sigs GitHub orgs) are always allowed.&lt;/p>
&lt;p>Kubernetes requires some third party content to function. Examples include container runtimes (containerd, CRI-O, Docker), networking policy (CNI plugins), Ingress controllers, and logging.&lt;/p>
&lt;p>Docs can link to third party open source software (OSS) outside the Kubernetes project if it’s necessary for Kubernetes to function.&lt;/p>
&lt;/blockquote>
&lt;p>These common sense guidelines make sure that Kubernetes docs document Kubernetes.&lt;/p>
&lt;h2 id="keeping-the-docs-focused">Keeping the docs focused&lt;/h2>
&lt;p>Our goal is for Kubernetes docs to be a trustworthy guide to Kubernetes features. To achieve this goal, SIG Docs is &lt;a href="https://github.com/kubernetes/website/issues/20232">tracking third party content&lt;/a> and removing any third party content that isn't both in the Kubernetes project &lt;em>and&lt;/em> required for Kubernetes to function.&lt;/p>
&lt;h3 id="re-homing-content">Re-homing content&lt;/h3>
&lt;p>Some content will be removed that readers may find helpful. To make sure readers have continous access to information, we're giving stakeholders until the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.19">1.19 release deadline for docs&lt;/a>, &lt;strong>July 9th, 2020&lt;/strong> to re-home any content slated for removal.&lt;/p>
&lt;p>Over the next few months you'll see less third party content in the docs as contributors open PRs to remove content.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>Over time, SIG Docs observed increasing vendor content in the docs. Some content took the form of vendor-specific implementations that aren't required for Kubernetes to function in-project. Other content was thinly-disguised advertising with minimal to no feature content. Some vendor content was new; other content had been in the docs for years. It became clear that the docs needed clear, well-bounded guidelines for what kind of third party content is and isn't allowed. The &lt;a href="https://kubernetes.io/docs/contribute/content-guide/">content guide&lt;/a> emerged from an extensive period for review and comment from the community.&lt;/p>
&lt;p>Docs work best when they're accurate, helpful, trustworthy, and remain focused on features. In our experience, vendor content dilutes trust and accuracy.&lt;/p>
&lt;p>Put simply: feature docs aren't a place for vendors to advertise their products. Our content policy keeps the docs focused on helping developers and cluster admins, not on marketing.&lt;/p>
&lt;h2 id="dual-sourced-content">Dual sourced content&lt;/h2>
&lt;p>Less impactful but also important is how Kubernetes docs handle &lt;em>dual-sourced content&lt;/em>. Dual-sourced content is content published in more than one location, or from a non-canonical source.&lt;/p>
&lt;p>From the &lt;a href="https://kubernetes.io/docs/contribute/style/content-guide/#dual-sourced-content">Kubernetes content guide&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>Wherever possible, Kubernetes docs link to canonical sources instead of hosting dual-sourced content.&lt;/p>
&lt;/blockquote>
&lt;p>Minimizing dual-sourced content streamlines the docs and makes content across the Web more searchable. We're working to consolidate and redirect dual-sourced content in the Kubernetes docs as well.&lt;/p>
&lt;h2 id="ways-to-contribute">Ways to contribute&lt;/h2>
&lt;p>We're tracking third-party content in an &lt;a href="https://github.com/kubernetes/website/issues/20232">issue in the Kubernetes website repository&lt;/a>. If you see third party content that's out of project and isn't required for Kubernetes to function, please comment on the tracking issue.&lt;/p>
&lt;p>Feel free to open a PR that removes non-conforming content once you've identified it!&lt;/p>
&lt;h2 id="want-to-know-more">Want to know more?&lt;/h2>
&lt;p>For more information, read the issue description for &lt;a href="https://github.com/kubernetes/website/issues/20232">tracking third party content&lt;/a>.&lt;/p></description></item><item><title>Blog: Introducing PodTopologySpread</title><link>https://kubernetes.io/blog/2020/05/introducing-podtopologyspread/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/05/introducing-podtopologyspread/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Wei Huang (IBM), Aldo Culquicondor (Google)&lt;/p>
&lt;p>Managing Pods distribution across a cluster is hard. The well-known Kubernetes
features for Pod affinity and anti-affinity, allow some control of Pod placement
in different topologies. However, these features only resolve part of Pods
distribution use cases: either place unlimited Pods to a single topology, or
disallow two Pods to co-locate in the same topology. In between these two
extreme cases, there is a common need to distribute the Pods evenly across the
topologies, so as to achieve better cluster utilization and high availability of
applications.&lt;/p>
&lt;p>The PodTopologySpread scheduling plugin (originally proposed as EvenPodsSpread)
was designed to fill that gap. We promoted it to beta in 1.18.&lt;/p>
&lt;h2 id="api-changes">API changes&lt;/h2>
&lt;p>A new field &lt;code>topologySpreadConstraints&lt;/code> is introduced in the Pod's spec API:&lt;/p>
&lt;pre>&lt;code>spec:
topologySpreadConstraints:
- maxSkew: &amp;lt;integer&amp;gt;
topologyKey: &amp;lt;string&amp;gt;
whenUnsatisfiable: &amp;lt;string&amp;gt;
labelSelector: &amp;lt;object&amp;gt;
&lt;/code>&lt;/pre>&lt;p>As this API is embedded in Pod's spec, you can use this feature in all the
high-level workload APIs, such as Deployment, DaemonSet, StatefulSet, etc.&lt;/p>
&lt;p>Let's see an example of a cluster to understand this API.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-05-introducing-podtopologyspread/api.png" alt="API">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>labelSelector&lt;/strong> is used to find matching Pods. For each topology, we count
the number of Pods that match this label selector. In the above example, given
the labelSelector as &amp;quot;app: foo&amp;quot;, the matching number in &amp;quot;zone1&amp;quot; is 2; while
the number in &amp;quot;zone2&amp;quot; is 0.&lt;/li>
&lt;li>&lt;strong>topologyKey&lt;/strong> is the key that defines a topology in the Nodes' labels. In
the above example, some Nodes are grouped into &amp;quot;zone1&amp;quot; if they have the label
&amp;quot;zone=zone1&amp;quot; label; while other ones are grouped into &amp;quot;zone2&amp;quot;.&lt;/li>
&lt;li>&lt;strong>maxSkew&lt;/strong> describes the maximum degree to which Pods can be unevenly
distributed. In the above example:
&lt;ul>
&lt;li>if we put the incoming Pod to &amp;quot;zone1&amp;quot;, the skew on &amp;quot;zone1&amp;quot; will become 3 (3
Pods matched in &amp;quot;zone1&amp;quot;; global minimum of 0 Pods matched on &amp;quot;zone2&amp;quot;), which
violates the &amp;quot;maxSkew: 1&amp;quot; constraint.&lt;/li>
&lt;li>if the incoming Pod is placed to &amp;quot;zone2&amp;quot;, the skew on &amp;quot;zone2&amp;quot; is 0 (1 Pod
matched in &amp;quot;zone2&amp;quot;; global minimum of 1 Pod matched on &amp;quot;zone2&amp;quot; itself),
which satisfies the &amp;quot;maxSkew: 1&amp;quot; constraint. Note that the skew is
calculated per each qualified Node, instead of a global skew.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>whenUnsatisfiable&lt;/strong> specifies, when &amp;quot;maxSkew&amp;quot; can't be satisfied, what
action should be taken:
&lt;ul>
&lt;li>&lt;code>DoNotSchedule&lt;/code> (default) tells the scheduler not to schedule it. It's a
hard constraint.&lt;/li>
&lt;li>&lt;code>ScheduleAnyway&lt;/code> tells the scheduler to still schedule it while prioritizing
Nodes that reduce the skew. It's a soft constraint.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="advanced-usage">Advanced usage&lt;/h2>
&lt;p>As the feature name &amp;quot;PodTopologySpread&amp;quot; implies, the basic usage of this feature
is to run your workload with an absolute even manner (maxSkew=1), or relatively
even manner (maxSkew&amp;gt;=2). See the &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/">official
document&lt;/a>
for more details.&lt;/p>
&lt;p>In addition to this basic usage, there are some advanced usage examples that
enable your workloads to benefit on high availability and cluster utilization.&lt;/p>
&lt;h3 id="usage-along-with-nodeselector-nodeaffinity">Usage along with NodeSelector / NodeAffinity&lt;/h3>
&lt;p>You may have found that we didn't have a &amp;quot;topologyValues&amp;quot; field to limit which
topologies the Pods are going to be scheduled to. By default, it is going to
search all Nodes and group them by &amp;quot;topologyKey&amp;quot;. Sometimes this may not be the
ideal case. For instance, suppose there is a cluster with Nodes tagged with
&amp;quot;env=prod&amp;quot;, &amp;quot;env=staging&amp;quot; and &amp;quot;env=qa&amp;quot;, and now you want to evenly place Pods to
the &amp;quot;qa&amp;quot; environment across zones, is it possible?&lt;/p>
&lt;p>The answer is yes. You can leverage the NodeSelector or NodeAffinity API spec.
Under the hood, the PodTopologySpread feature will &lt;strong>honor&lt;/strong> that and calculate
the spread constraints among the nodes that satisfy the selectors.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-05-introducing-podtopologyspread/advanced-usage-1.png" alt="Advanced-Usage-1">&lt;/p>
&lt;p>As illustrated above, you can specify &lt;code>spec.affinity.nodeAffinity&lt;/code> to limit the
&amp;quot;searching scope&amp;quot; to be &amp;quot;qa&amp;quot; environment, and within that scope, the Pod will be
scheduled to one zone which satisfies the topologySpreadConstraints. In this
case, it's &amp;quot;zone2&amp;quot;.&lt;/p>
&lt;h3 id="multiple-topologyspreadconstraints">Multiple TopologySpreadConstraints&lt;/h3>
&lt;p>It's intuitive to understand how one single TopologySpreadConstraint works.
What's the case for multiple TopologySpreadConstraints? Internally, each
TopologySpreadConstraint is calculated independently, and the result sets will
be merged to generate the eventual result set - i.e., suitable Nodes.&lt;/p>
&lt;p>In the following example, we want to schedule a Pod to a cluster with 2
requirements at the same time:&lt;/p>
&lt;ul>
&lt;li>place the Pod evenly with Pods across zones&lt;/li>
&lt;li>place the Pod evenly with Pods across nodes&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-05-introducing-podtopologyspread/advanced-usage-2.png" alt="Advanced-Usage-2">&lt;/p>
&lt;p>For the first constraint, there are 3 Pods in zone1 and 2 Pods in zone2, so the
incoming Pod can be only put to zone2 to satisfy the &amp;quot;maxSkew=1&amp;quot; constraint. In
other words, the result set is nodeX and nodeY.&lt;/p>
&lt;p>For the second constraint, there are too many Pods in nodeB and nodeX, so the
incoming Pod can be only put to nodeA and nodeY.&lt;/p>
&lt;p>Now we can conclude the only qualified Node is nodeY - from the intersection of
the sets {nodeX, nodeY} (from the first constraint) and {nodeA, nodeY} (from the
second constraint).&lt;/p>
&lt;p>Multiple TopologySpreadConstraints is powerful, but be sure to understand the
difference with the preceding &amp;quot;NodeSelector/NodeAffinity&amp;quot; example: one is to
calculate result set independently and then interjoined; while the other is to
calculate topologySpreadConstraints based on the filtering results of node
constraints.&lt;/p>
&lt;p>Instead of using &amp;quot;hard&amp;quot; constraints in all topologySpreadConstraints, you can
also combine using &amp;quot;hard&amp;quot; constraints and &amp;quot;soft&amp;quot; constraints to adhere to more
diverse cluster situations.&lt;/p>
&lt;blockquote class="note">
&lt;div>&lt;strong>Note:&lt;/strong> If two TopologySpreadConstraints are being applied for the same {topologyKey,
whenUnsatisfiable} tuple, the Pod creation will be blocked returning a
validation error.&lt;/div>
&lt;/blockquote>
&lt;h2 id="podtopologyspread-defaults">PodTopologySpread defaults&lt;/h2>
&lt;p>PodTopologySpread is a Pod level API. As such, to use the feature, workload
authors need to be aware of the underlying topology of the cluster, and then
specify proper &lt;code>topologySpreadConstraints&lt;/code> in the Pod spec for every workload.
While the Pod-level API gives the most flexibility it is also possible to
specify cluster-level defaults.&lt;/p>
&lt;p>The default PodTopologySpread constraints allow you to specify spreading for all
the workloads in the cluster, tailored for its topology. The constraints can be
specified by an operator/admin as PodTopologySpread plugin arguments in the
&lt;a href="https://kubernetes.io/docs/reference/scheduling/profiles/">scheduling profile configuration
API&lt;/a> when starting
kube-scheduler.&lt;/p>
&lt;p>A sample configuration could look like this:&lt;/p>
&lt;pre>&lt;code>apiVersion: kubescheduler.config.k8s.io/v1alpha2
kind: KubeSchedulerConfiguration
profiles:
pluginConfig:
- name: PodTopologySpread
args:
defaultConstraints:
- maxSkew: 1
topologyKey: example.com/rack
whenUnsatisfiable: ScheduleAnyway
&lt;/code>&lt;/pre>&lt;p>When configuring default constraints, label selectors must be left empty.
kube-scheduler will deduce the label selectors from the membership of the Pod to
Services, ReplicationControllers, ReplicaSets or StatefulSets. Pods can
always override the default constraints by providing their own through the
PodSpec.&lt;/p>
&lt;blockquote class="note">
&lt;div>&lt;strong>Note:&lt;/strong> When using default PodTopologySpread constraints, it is recommended to disable
the old DefaultTopologySpread plugin.&lt;/div>
&lt;/blockquote>
&lt;h2 id="wrap-up">Wrap-up&lt;/h2>
&lt;p>PodTopologySpread allows you to define spreading constraints for your workloads
with a flexible and expressive Pod-level API. In the past, workload authors used
Pod AntiAffinity rules to force or hint the scheduler to run a single Pod per
topology domain. In contrast, the new PodTopologySpread constraints allow Pods
to specify skew levels that can be required (hard) or desired (soft). The
feature can be paired with Node selectors and Node affinity to limit the
spreading to specific domains. Pod spreading constraints can be defined for
different topologies such as hostnames, zones, regions, racks, etc.&lt;/p>
&lt;p>Lastly, cluster operators can define default constraints to be applied to all
Pods. This way, Pods don't need to be aware of the underlying topology of the
cluster.&lt;/p></description></item><item><title>Blog: Two-phased Canary Rollout with Open Source Gloo</title><link>https://kubernetes.io/blog/2020/04/two-phased-canary-rollout-with-gloo/</link><pubDate>Wed, 22 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/two-phased-canary-rollout-with-gloo/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Rick Ducott | &lt;a href="https://github.com/rickducott/">GitHub&lt;/a> | &lt;a href="https://twitter.com/ducott">Twitter&lt;/a>&lt;/p>
&lt;p>Every day, my colleagues and I are talking to platform owners, architects, and engineers who are using &lt;a href="https://github.com/solo-io/gloo">Gloo&lt;/a> as an API gateway
to expose their applications to end users. These applications may span legacy monoliths, microservices, managed cloud services, and Kubernetes
clusters. Fortunately, Gloo makes it easy to set up routes to manage, secure, and observe application traffic while
supporting a flexible deployment architecture to meet the varying production needs of our users.&lt;/p>
&lt;p>Beyond the initial set up, platform owners frequently ask us to help design the operational workflows within their organization:
How do we bring a new application online? How do we upgrade an application? How do we divide responsibilities across our
platform, ops, and development teams?&lt;/p>
&lt;p>In this post, we're going to use Gloo to design a two-phased canary rollout workflow for application upgrades:&lt;/p>
&lt;ul>
&lt;li>In the first phase, we'll do canary testing by shifting a small subset of traffic to the new version. This allows you to safely perform smoke and correctness tests.&lt;/li>
&lt;li>In the second phase, we'll progressively shift traffic to the new version, allowing us to monitor the new version under load, and eventually, decommission the old version.&lt;/li>
&lt;/ul>
&lt;p>To keep it simple, we're going to focus on designing the workflow using &lt;a href="https://github.com/solo-io/gloo">open source Gloo&lt;/a>, and we're going to deploy the gateway and
application to Kubernetes. At the end, we'll talk about a few extensions and advanced topics that could be interesting to explore in a follow up.&lt;/p>
&lt;h2 id="initial-setup">Initial setup&lt;/h2>
&lt;p>To start, we need a Kubernetes cluster. This example doesn't take advantage of any cloud specific
features, and can be run against a local test cluster such as &lt;a href="https://kubernetes.io/docs/tasks/tools/install-minikube/">minikube&lt;/a>.
This post assumes a basic understanding of Kubernetes and how to interact with it using &lt;code>kubectl&lt;/code>.&lt;/p>
&lt;p>We'll install the latest &lt;a href="https://github.com/solo-io/gloo">open source Gloo&lt;/a> to the &lt;code>gloo-system&lt;/code> namespace and deploy
version &lt;code>v1&lt;/code> of an example application to the &lt;code>echo&lt;/code> namespace. We'll expose this application outside the cluster
by creating a route in Gloo, to end up with a picture like this:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/setup.png" alt="Setup">&lt;/p>
&lt;h3 id="deploying-gloo">Deploying Gloo&lt;/h3>
&lt;p>We'll install gloo with the &lt;code>glooctl&lt;/code> command line tool, which we can download and add to the &lt;code>PATH&lt;/code> with the following
commands:&lt;/p>
&lt;pre>&lt;code>curl -sL https://run.solo.io/gloo/install | sh
export PATH=$HOME/.gloo/bin:$PATH
&lt;/code>&lt;/pre>&lt;p>Now, you should be able to run &lt;code>glooctl version&lt;/code> to see that it is installed correctly:&lt;/p>
&lt;pre>&lt;code>➜ glooctl version
Client: {&amp;quot;version&amp;quot;:&amp;quot;1.3.15&amp;quot;}
Server: version undefined, could not find any version of gloo running
&lt;/code>&lt;/pre>&lt;p>Now we can install the gateway to our cluster with a simple command:&lt;/p>
&lt;pre>&lt;code>glooctl install gateway
&lt;/code>&lt;/pre>&lt;p>The console should indicate the install finishes successfully:&lt;/p>
&lt;pre>&lt;code>Creating namespace gloo-system... Done.
Starting Gloo installation...
Gloo was successfully installed!
&lt;/code>&lt;/pre>&lt;p>Before long, we can see all the Gloo pods running in the &lt;code>gloo-system&lt;/code> namespace:&lt;/p>
&lt;pre>&lt;code>➜ kubectl get pod -n gloo-system
NAME READY STATUS RESTARTS AGE
discovery-58f8856bd7-4fftg 1/1 Running 0 13s
gateway-66f86bc8b4-n5crc 1/1 Running 0 13s
gateway-proxy-5ff99b8679-tbp65 1/1 Running 0 13s
gloo-66b8dc8868-z5c6r 1/1 Running 0 13s
&lt;/code>&lt;/pre>&lt;h3 id="deploying-the-application">Deploying the application&lt;/h3>
&lt;p>Our &lt;code>echo&lt;/code> application is a simple container (thanks to our friends at HashiCorp) that will
respond with the application version, to help demonstrate our canary workflows as we start testing and
shifting traffic to a &lt;code>v2&lt;/code> version of the application.&lt;/p>
&lt;p>Kubernetes gives us a lot of flexibility in terms of modeling this application. We'll adopt the following
conventions:&lt;/p>
&lt;ul>
&lt;li>We'll include the version in the deployment name so we can run two versions of the application
side-by-side and manage their lifecycle differently.&lt;/li>
&lt;li>We'll label pods with an app label (&lt;code>app: echo&lt;/code>) and a version label (&lt;code>version: v1&lt;/code>) to help with our canary rollout.&lt;/li>
&lt;li>We'll deploy a single Kubernetes &lt;code>Service&lt;/code> for the application to set up networking. Instead of updating
this or using multiple services to manage routing to different versions, we'll manage the rollout with Gloo configuration.&lt;/li>
&lt;/ul>
&lt;p>The following is our &lt;code>v1&lt;/code> echo application:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>apps/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Deployment&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo-v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">replicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Shout out to our friends at Hashi for this useful test server&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>hashicorp/http-echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">args&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;-text=version:v1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- -listen=:&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">imagePullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Always&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo-v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">containerPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And here is the &lt;code>echo&lt;/code> Kubernetes &lt;code>Service&lt;/code> object:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Service&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">80&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">targetPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">protocol&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>TCP&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For convenience, we've published this yaml in a repo so we can deploy it with the following command:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/1-setup/echo.yaml
&lt;/code>&lt;/pre>&lt;p>We should see the following output:&lt;/p>
&lt;pre>&lt;code>namespace/echo created
deployment.apps/echo-v1 created
service/echo created
&lt;/code>&lt;/pre>&lt;p>And we should be able to see all the resources healthy in the &lt;code>echo&lt;/code> namespace:&lt;/p>
&lt;pre>&lt;code>➜ kubectl get all -n echo
NAME READY STATUS RESTARTS AGE
pod/echo-v1-66dbfffb79-287s5 1/1 Running 0 6s
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
service/echo ClusterIP 10.55.252.216 &amp;lt;none&amp;gt; 80/TCP 6s
NAME READY UP-TO-DATE AVAILABLE AGE
deployment.apps/echo-v1 1/1 1 1 7s
NAME DESIRED CURRENT READY AGE
replicaset.apps/echo-v1-66dbfffb79 1 1 1 7s
&lt;/code>&lt;/pre>&lt;h3 id="exposing-outside-the-cluster-with-gloo">Exposing outside the cluster with Gloo&lt;/h3>
&lt;p>We can now expose this service outside the cluster with Gloo. First, we'll model the application as a Gloo
&lt;a href="https://docs.solo.io/gloo/latest/introduction/architecture/concepts/#upstreams">Upstream&lt;/a>, which is Gloo's abstraction
for a traffic destination:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Upstream&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kube&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">serviceName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">serviceNamespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">servicePort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subsetSpec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">selectors&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">keys&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- version&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here, we're setting up subsets based on the &lt;code>version&lt;/code> label. We don't have to use this in our routes, but later
we'll start to use it to support our canary workflow.&lt;/p>
&lt;p>We can now create a route to this upstream in Gloo by defining a
&lt;a href="https://docs.solo.io/gloo/latest/introduction/architecture/concepts/#virtual-services">Virtual Service&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VirtualService&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">virtualHost&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">domains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can apply these resources with the following commands:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/1-setup/upstream.yaml
kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/1-setup/vs.yaml
&lt;/code>&lt;/pre>&lt;p>Once we apply these two resources, we can start to send traffic to the application through Gloo:&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/
version:v1
&lt;/code>&lt;/pre>&lt;p>Our setup is complete, and our cluster now looks like this:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/setup.png" alt="Setup">&lt;/p>
&lt;h2 id="two-phased-rollout-strategy">Two-Phased Rollout Strategy&lt;/h2>
&lt;p>Now we have a new version &lt;code>v2&lt;/code> of the echo application that we wish to roll out. We know that when the
rollout is complete, we are going to end up with this picture:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/end-state.png" alt="End State">&lt;/p>
&lt;p>However, to get there, we may want to perform a few rounds of testing to ensure the new version of the application
meets certain correctness and/or performance acceptance criteria. In this post, we'll introduce a two-phased approach to
canary rollout with Gloo, that could be used to satisfy the vast majority of acceptance tests.&lt;/p>
&lt;p>In the first phase, we'll perform smoke and correctness tests by routing a small segment of the traffic to the new version
of the application. In this demo, we'll use a header &lt;code>stage: canary&lt;/code> to trigger routing to the new service, though in
practice it may be desirable to make this decision based on another part of the request, such as a claim in a verified JWT.&lt;/p>
&lt;p>In the second phase, we've already established correctness, so we are ready to shift all of the traffic over to the new
version of the application. We'll configure weighted destinations, and shift the traffic while monitoring certain business
metrics to ensure the service quality remains at acceptable levels. Once 100% of the traffic is shifted to the new version,
the old version can be decommissioned.&lt;/p>
&lt;p>In practice, it may be desirable to only use one of the phases for testing, in which case the other phase can be
skipped.&lt;/p>
&lt;h2 id="phase-1-initial-canary-rollout-of-v2">Phase 1: Initial canary rollout of v2&lt;/h2>
&lt;p>In this phase, we'll deploy &lt;code>v2&lt;/code>, and then use a header &lt;code>stage: canary&lt;/code> to start routing a small amount of specific
traffic to the new version. We'll use this header to perform some basic smoke testing and make sure &lt;code>v2&lt;/code> is working the
way we'd expect:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/subset-routing.png" alt="Subset Routing">&lt;/p>
&lt;h3 id="setting-up-subset-routing">Setting up subset routing&lt;/h3>
&lt;p>Before deploying our &lt;code>v2&lt;/code> service, we'll update our virtual service to only route to pods that have the subset label
&lt;code>version: v1&lt;/code>, using a Gloo feature called &lt;a href="https://docs.solo.io/gloo/latest/guides/traffic_management/destination_types/subsets/">subset routing&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VirtualService&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">virtualHost&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">domains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can apply them to the cluster with the following commands:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/2-initial-subset-routing-to-v2/vs-1.yaml
&lt;/code>&lt;/pre>&lt;p>The application should continue to function as before:&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/
version:v1
&lt;/code>&lt;/pre>&lt;h3 id="deploying-echo-v2">Deploying echo v2&lt;/h3>
&lt;p>Now we can safely deploy &lt;code>v2&lt;/code> of the echo application:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>apps/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Deployment&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo-v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">replicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>hashicorp/http-echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">args&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;-text=version:v2&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- -listen=:&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">imagePullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Always&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo-v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">containerPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can deploy with the following command:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/2-initial-subset-routing-to-v2/echo-v2.yaml
&lt;/code>&lt;/pre>&lt;p>Since our gateway is configured to route specifically to the &lt;code>v1&lt;/code> subset, this should have no effect. However, it does enable
&lt;code>v2&lt;/code> to be routable from the gateway if the &lt;code>v2&lt;/code> subset is configured for a route.&lt;/p>
&lt;p>Make sure &lt;code>v2&lt;/code> is running before moving on:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">➜ kubectl get pod -n &lt;span style="color:#a2f">echo&lt;/span>
NAME READY STATUS RESTARTS AGE
echo-v1-66dbfffb79-2qw86 1/1 Running &lt;span style="color:#666">0&lt;/span> 5m25s
echo-v2-86584fbbdb-slp44 1/1 Running &lt;span style="color:#666">0&lt;/span> 93s
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The application should continue to function as before:&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/
version:v1
&lt;/code>&lt;/pre>&lt;h3 id="adding-a-route-to-v2-for-canary-testing">Adding a route to v2 for canary testing&lt;/h3>
&lt;p>We'll route to the &lt;code>v2&lt;/code> subset when the &lt;code>stage: canary&lt;/code> header is supplied on the request. If the header isn't
provided, we'll continue to route to the &lt;code>v1&lt;/code> subset as before.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VirtualService&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">virtualHost&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">domains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">headers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>stage&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>canary&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can deploy with the following command:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/2-initial-subset-routing-to-v2/vs-2.yaml
&lt;/code>&lt;/pre>&lt;h3 id="canary-testing">Canary testing&lt;/h3>
&lt;p>Now that we have this route, we can do some testing. First let's ensure that the existing route is working as expected:&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/
version:v1
&lt;/code>&lt;/pre>&lt;p>And now we can start to canary test our new application version:&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/ -H &amp;quot;stage: canary&amp;quot;
version:v2
&lt;/code>&lt;/pre>&lt;h3 id="advanced-use-cases-for-subset-routing">Advanced use cases for subset routing&lt;/h3>
&lt;p>We may decide that this approach, using user-provided request headers, is too open. Instead, we may
want to restrict canary testing to a known, authorized user.&lt;/p>
&lt;p>A common implementation of this that we've seen is for the canary route to require a valid JWT that contains
a specific claim to indicate the subject is authorized for canary testing. Enterprise Gloo has out of the box
support for verifying JWTs, updating the request headers based on the JWT claims, and recomputing the
routing destination based on the updated headers. We'll save that for a future post covering more advanced use
cases in canary testing.&lt;/p>
&lt;h2 id="phase-2-shifting-all-traffic-to-v2-and-decommissioning-v1">Phase 2: Shifting all traffic to v2 and decommissioning v1&lt;/h2>
&lt;p>At this point, we've deployed &lt;code>v2&lt;/code>, and created a route for canary testing. If we are satisfied with the
results of the testing, we can move on to phase 2 and start shifting the load from &lt;code>v1&lt;/code> to &lt;code>v2&lt;/code>. We'll use
&lt;a href="https://docs.solo.io/gloo/latest/guides/traffic_management/destination_types/multi_destination/">weighted destinations&lt;/a>
in Gloo to manage the load during the migration.&lt;/p>
&lt;h3 id="setting-up-the-weighted-destinations">Setting up the weighted destinations&lt;/h3>
&lt;p>We can change the Gloo route to route to both of these destinations, with weights to decide how much of the traffic should
go to the &lt;code>v1&lt;/code> versus the &lt;code>v2&lt;/code> subset. To start, we're going to set it up so 100% of the traffic continues to get routed to the
&lt;code>v1&lt;/code> subset, unless the &lt;code>stage: canary&lt;/code> header was provided as before.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VirtualService&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">virtualHost&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">domains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># We&amp;#39;ll keep our route from before if we want to continue testing with this header&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">headers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>stage&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>canary&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Now we&amp;#39;ll route the rest of the traffic to the upstream, load balanced across the two subsets.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">multi&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">destinations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">destination&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">100&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">destination&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can apply this virtual service update to the cluster with the following commands:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/3-progressive-traffic-shift-to-v2/vs-1.yaml
&lt;/code>&lt;/pre>&lt;p>Now the cluster looks like this, for any request that doesn't have the &lt;code>stage: canary&lt;/code> header:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/init-traffic-shift.png" alt="Initialize Traffic Shift">&lt;/p>
&lt;p>With the initial weights, we should see the gateway continue to serve &lt;code>v1&lt;/code> for all traffic.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">➜ curl &lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>glooctl proxy url&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>/
version:v1
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="commence-rollout">Commence rollout&lt;/h3>
&lt;p>To simulate a load test, let's shift half the traffic to &lt;code>v2&lt;/code>:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/load-test.png" alt="Load Test">&lt;/p>
&lt;p>This can be expressed on our virtual service by adjusting the weights:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VirtualService&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">virtualHost&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">domains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">headers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>stage&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>canary&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">multi&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">destinations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">destination&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Update the weight so 50% of the traffic hits v1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">50&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">destination&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># And 50% is routed to v2&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">50&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can apply this to the cluster with the following command:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/3-progressive-traffic-shift-to-v2/vs-2.yaml
&lt;/code>&lt;/pre>&lt;p>Now when we send traffic to the gateway, we should see half of the requests return &lt;code>version:v1&lt;/code> and the
other half return &lt;code>version:v2&lt;/code>.&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/
version:v1
➜ curl $(glooctl proxy url)/
version:v2
➜ curl $(glooctl proxy url)/
version:v1
&lt;/code>&lt;/pre>&lt;p>In practice, during this process it's likely you'll be monitoring some performance and business metrics
to ensure the traffic shift isn't resulting in a decline in the overall quality of service. We can even
leverage operators like &lt;a href="https://github.com/weaveworks/flagger">Flagger&lt;/a> to help automate this Gloo
workflow. Gloo Enterprise integrates with your metrics backend and provides out of the box and dynamic,
upstream-based dashboards that can be used to monitor the health of the rollout.
We will save these topics for a future post on advanced canary testing use cases with Gloo.&lt;/p>
&lt;h3 id="finishing-the-rollout">Finishing the rollout&lt;/h3>
&lt;p>We will continue adjusting weights until eventually, all of the traffic is now being routed to &lt;code>v2&lt;/code>:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/final-shift.png" alt="Final Shift">&lt;/p>
&lt;p>Our virtual service will look like this:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VirtualService&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">virtualHost&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">domains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">headers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>stage&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>canary&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">multi&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">destinations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">destination&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># No traffic will be sent to v1 anymore&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">destination&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Now all the traffic will be routed to v2&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">100&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can apply that to the cluster with the following command:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/3-progressive-traffic-shift-to-v2/vs-3.yaml
&lt;/code>&lt;/pre>&lt;p>Now when we send traffic to the gateway, we should see all of the requests return &lt;code>version:v2&lt;/code>.&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/
version:v2
➜ curl $(glooctl proxy url)/
version:v2
➜ curl $(glooctl proxy url)/
version:v2
&lt;/code>&lt;/pre>&lt;h3 id="decommissioning-v1">Decommissioning v1&lt;/h3>
&lt;p>At this point, we have deployed the new version of our application, conducted correctness tests using subset routing,
conducted load and performance tests by progressively shifting traffic to the new version, and finished
the rollout. The only remaining task is to clean up our &lt;code>v1&lt;/code> resources.&lt;/p>
&lt;p>First, we'll clean up our routes. We'll leave the subset specified on the route so we are all setup for future upgrades.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VirtualService&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">virtualHost&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">domains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can apply this update with the following command:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/4-decommissioning-v1/vs.yaml
&lt;/code>&lt;/pre>&lt;p>And we can delete the &lt;code>v1&lt;/code> deployment, which is no longer serving any traffic.&lt;/p>
&lt;pre>&lt;code>kubectl delete deploy -n echo echo-v1
&lt;/code>&lt;/pre>&lt;p>Now our cluster looks like this:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/end-state.png" alt="End State">&lt;/p>
&lt;p>And requests to the gateway return this:&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/
version:v2
&lt;/code>&lt;/pre>&lt;p>We have now completed our two-phased canary rollout of an application update using Gloo!&lt;/p>
&lt;h2 id="other-advanced-topics">Other Advanced Topics&lt;/h2>
&lt;p>Over the course of this post, we collected a few topics that could be a good starting point for advanced exploration:&lt;/p>
&lt;ul>
&lt;li>Using the &lt;strong>JWT&lt;/strong> filter to verify JWTs, extract claims onto headers, and route to canary versions depending on a claim value.&lt;/li>
&lt;li>Looking at &lt;strong>Prometheus metrics&lt;/strong> and &lt;strong>Grafana dashboards&lt;/strong> created by Gloo to monitor the health of the rollout.&lt;/li>
&lt;li>Automating the rollout by integrating &lt;strong>Flagger&lt;/strong> with &lt;strong>Gloo&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;p>A few other topics that warrant further exploration:&lt;/p>
&lt;ul>
&lt;li>Supporting &lt;strong>self-service&lt;/strong> upgrades by giving teams ownership over their upstream and route configuration&lt;/li>
&lt;li>Utilizing Gloo's &lt;strong>delegation&lt;/strong> feature and Kubernetes &lt;strong>RBAC&lt;/strong> to decentralize the configuration management safely&lt;/li>
&lt;li>Fully automating the continuous delivery process by applying &lt;strong>GitOps&lt;/strong> principles and using tools like &lt;strong>Flux&lt;/strong> to push config to the cluster&lt;/li>
&lt;li>Supporting &lt;strong>hybrid&lt;/strong> or &lt;strong>non-Kubernetes&lt;/strong> application use-cases by setting up Gloo with a different deployment pattern&lt;/li>
&lt;li>Utilizing &lt;strong>traffic shadowing&lt;/strong> to begin testing the new version with realistic data before shifting production traffic to it&lt;/li>
&lt;/ul>
&lt;h2 id="get-involved-in-the-gloo-community">Get Involved in the Gloo Community&lt;/h2>
&lt;p>Gloo has a large and growing community of open source users, in addition to an enterprise customer base. To learn more about
Gloo:&lt;/p>
&lt;ul>
&lt;li>Check out the &lt;a href="https://github.com/solo-io/gloo">repo&lt;/a>, where you can see the code and file issues&lt;/li>
&lt;li>Check out the &lt;a href="https://docs.solo.io/gloo/latest">docs&lt;/a>, which have an extensive collection of guides and examples&lt;/li>
&lt;li>Join the &lt;a href="http://slack.solo.io/">slack channel&lt;/a> and start chatting with the Solo engineering team and user community&lt;/li>
&lt;/ul>
&lt;p>If you'd like to get in touch with me (feedback is always appreciated!), you can find me on the
&lt;a href="http://slack.solo.io/">Solo slack&lt;/a> or email me at &lt;strong>&lt;a href="mailto:rick.ducott@solo.io">rick.ducott@solo.io&lt;/a>&lt;/strong>.&lt;/p></description></item><item><title>Blog: Cluster API v1alpha3 Delivers New Features and an Improved User Experience</title><link>https://kubernetes.io/blog/2020/04/21/cluster-api-v1alpha3-delivers-new-features-and-an-improved-user-experience/</link><pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/21/cluster-api-v1alpha3-delivers-new-features-and-an-improved-user-experience/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Daniel Lipovetsky (D2IQ)&lt;/p>
&lt;img src="https://kubernetes.io/images/blog/2020-04-21-Cluster-API-v1alpha3-Delivers-New-Features-and-an-Improved-User-Experience/kubernetes-cluster-logos_final-02.svg" align="right" width="25%" alt="Cluster API Logo: Turtles All The Way Down">
&lt;p>The Cluster API is a Kubernetes project to bring declarative, Kubernetes-style APIs to cluster creation, configuration, and management. It provides optional, additive functionality on top of core Kubernetes to manage the lifecycle of a Kubernetes cluster.&lt;/p>
&lt;p>Following the v1alpha2 release in October 2019, many members of the Cluster API community met in San Francisco, California, to plan the next release. The project had just gone through a major transformation, delivering a new architecture that promised to make the project easier for users to adopt, and faster for the community to build. Over the course of those two days, we found our common goals: To implement the features critical to managing production clusters, to make its user experience more intuitive, and to make it a joy to develop.&lt;/p>
&lt;p>The v1alpha3 release of Cluster API brings significant features for anyone running Kubernetes in production and at scale. Among the highlights:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#declarative-control-plane-management">Declarative Control Plane Management&lt;/a>&lt;/li>
&lt;li>&lt;a href="#distributing-control-plane-nodes-to-reduce-risk">Support for Distributing Control Plane Nodes Across Failure Domains To Reduce Risk&lt;/a>&lt;/li>
&lt;li>&lt;a href="#automated-replacement-of-unhealthy-nodes">Automated Replacement of Unhealthy Nodes&lt;/a>&lt;/li>
&lt;li>&lt;a href="#infrastructure-managed-node-groups">Support for Infrastructure-Managed Node Groups&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>For anyone who wants to understand the API, or prizes a simple, but powerful, command-line interface, the new release brings:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#clusterctl">Redesigned clusterctl, a command-line tool (and go library) for installing and managing the lifecycle of Cluster API&lt;/a>&lt;/li>
&lt;li>&lt;a href="#the-cluster-api-book">Extensive and up-to-date documentation in The Cluster API Book&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Finally, for anyone extending the Cluster API for their custom infrastructure or software needs:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#end-to-end-test-framework">New End-to-End (e2e) Test Framework&lt;/a>&lt;/li>
&lt;li>&lt;a href="#provider-implementer-s-guide">Documentation for integrating Cluster API into your cluster lifecycle stack&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>All this was possible thanks to the hard work of many contributors.&lt;/p>
&lt;h2 id="declarative-control-plane-management">Declarative Control Plane Management&lt;/h2>
&lt;p>&lt;em>Special thanks to &lt;a href="https://github.com/detiber/">Jason DeTiberus&lt;/a>, &lt;a href="https://github.com/randomvariable">Naadir Jeewa&lt;/a>, and &lt;a href="https://github.com/chuckha">Chuck Ha&lt;/a>&lt;/em>&lt;/p>
&lt;p>The Kubeadm-based Control Plane (KCP) provides a declarative API to deploy and scale the Kubernetes control plane, including etcd. This is the feature many Cluster API users have been waiting for! Until now, to deploy and scale up the control plane, users had to create specially-crafted Machine resources. To scale down the control plane, they had to manually remove members from the etcd cluster. KCP automates deployment, scaling, and upgrades.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>What is the Kubernetes Control Plane?&lt;/strong>
The Kubernetes control plane is, at its core, kube-apiserver and etcd. If either of these are unavailable, no API requests can be handled. This impacts not only core Kubernetes APIs, but APIs implemented with CRDs. Other components, like kube-scheduler and kube-controller-manager, are also important, but do not have the same impact on availability.&lt;/p>
&lt;p>The control plane was important in the beginning because it scheduled workloads. However, some workloads could continue to run during a control plane outage. Today, workloads depend on operators, service meshes, and API gateways, which all use the control plane as a platform. Therefore, the control plane's availability is more important than ever.&lt;/p>
&lt;p>Managing the control plane is one of the most complex parts of cluster operation. Because the typical control plane includes etcd, it is stateful, and operations must be done in the correct sequence. Control plane replicas can and do fail, and maintaining control plane availability means being able to replace failed nodes.&lt;/p>
&lt;p>The control plane can suffer a complete outage (e.g. permanent loss of quorum in etcd), and recovery (along with regular backups) is sometimes the only feasible option.&lt;/p>
&lt;p>For more details, read about &lt;a href="https://kubernetes.io/docs/concepts/overview/components/">Kubernetes Components&lt;/a> in the Kubernetes documentation.&lt;/p>
&lt;/blockquote>
&lt;p>Here's an example of a 3-replica control plane for the Cluster API Docker Infrastructure, which the project maintains for testing and development. For brevity, other required resources, like Cluster, and Infrastructure Template, referenced by its name and namespace, are not shown.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>controlplane.cluster.x-k8s.io/v1alpha3&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>KubeadmControlPlane&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">infrastructureTemplate&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>infrastructure.cluster.x-k8s.io/v1alpha3&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DockerMachineTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kubeadmConfigSpec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">clusterConfiguration&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">replicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">3&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1.16.3&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Deploy this control plane with kubectl:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl apply -f example-docker-control-plane.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Scale the control plane the same way you scale other Kubernetes resources:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl scale kubeadmcontrolplane example --replicas&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">5&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>kubeadmcontrolplane.controlplane.cluster.x-k8s.io/example scaled
&lt;/code>&lt;/pre>&lt;p>Upgrade the control plane to a newer patch of the Kubernetes release:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl patch kubeadmcontrolplane example --type&lt;span style="color:#666">=&lt;/span>json -p &lt;span style="color:#b44">&amp;#39;[{&amp;#34;op&amp;#34;: &amp;#34;replace&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/spec/version&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;1.16.4&amp;#34;}]&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>Number of Control Plane Replicas&lt;/strong>
By default, KCP is configured to manage etcd, and requires an odd number of replicas. If KCP is configured to not manage etcd, an odd number is recommended, but not required. An odd number of replicas ensures optimal etcd configuration. To learn why your etcd cluster should have an odd number of members, see the &lt;a href="https://etcd.io/docs/v3.4.0/faq/#why-an-odd-number-of-cluster-members">etcd FAQ&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>Because it is a core Cluster API component, KCP can be used with any v1alpha3-compatible Infrastructure Provider that provides a fixed control plane endpoint, i.e., a load balancer or virtual IP. This endpoint enables requests to reach multiple control plane replicas.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>What is an Infrastructure Provider?&lt;/strong>
A source of computational resources (e.g. machines, networking, etc.). The community maintains providers for AWS, Azure, Google Cloud, and VMWare. For details, see the &lt;a href="https://cluster-api.sigs.k8s.io/reference/providers.html">list of providers&lt;/a> in the Cluster API Book.&lt;/p>
&lt;/blockquote>
&lt;h2 id="distributing-control-plane-nodes-to-reduce-risk">Distributing Control Plane Nodes To Reduce Risk&lt;/h2>
&lt;p>&lt;em>Special thanks to &lt;a href="https://github.com/vincepri/">Vince Prignano&lt;/a>, and &lt;a href="https://github.com/chuckha">Chuck Ha&lt;/a>&lt;/em>&lt;/p>
&lt;p>Cluster API users can now deploy nodes in different failure domains, reducing the risk of a cluster failing due to a domain outage. This is especially important for the control plane: If nodes in one domain fail, the cluster can continue to operate as long as the control plane is available to nodes in other domains.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>What is a Failure Domain?&lt;/strong>
A failure domain is a way to group the resources that would be made unavailable by some failure. For example, in many public clouds, an &amp;quot;availability zone&amp;quot; is the default failure domain. A zone corresponds to a data center. So, if a specific data center is brought down by a power outage or natural disaster, all resources in that zone become unavailable. If you run Kubernetes on your own hardware, your failure domain might be a rack, a network switch, or power distribution unit.&lt;/p>
&lt;/blockquote>
&lt;p>The Kubeadm-based ControlPlane distributes nodes across failure domains. To minimize the chance of losing multiple nodes in the event of a domain outage, it tries to distribute them evenly: it deploys a new node in the failure domain with the fewest existing nodes, and it removes an existing node in the failure domain with the most existing nodes.&lt;/p>
&lt;p>MachineDeployments and MachineSets do not distribute nodes across failure domains. To deploy your worker nodes across multiple failure domains, create a MachineDeployment or MachineSet for each failure domain.&lt;/p>
&lt;p>The Failure Domain API works on any infrastructure. That's because every Infrastructure Provider maps failure domains in its own way. The API is optional, so if your infrastructure is not complex enough to need failure domains, you do not need to support it. This example is for the Cluster API Docker Infrastructure Provider. Note that two of the domains are marked as suitable for control plane nodes, while a third is not. The Kubeadm-based ControlPlane will only deploy nodes to domains marked suitable.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>infrastructure.cluster.x-k8s.io/v1alpha3&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DockerCluster&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">controlPlaneEndpoint&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">host&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">172.17.0.4&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">6443&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">failureDomains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">domain-one&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">controlPlane&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">domain-two&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">controlPlane&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">domain-three&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">controlPlane&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">false&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws">AWS Infrastructure Provider&lt;/a> (CAPA), maintained by the Cluster API project, maps failure domains to AWS Availability Zones. Using CAPA, you can deploy a cluster across multiple Availability Zones. First, define subnets for multiple Availability Zones. The CAPA controller will define a failure domain for each Availability Zone. Deploy the control plane with the KubeadmControlPlane: it will distribute replicas across the failure domains. Finally, create a separate MachineDeployment for each failure domain.&lt;/p>
&lt;h2 id="automated-replacement-of-unhealthy-nodes">Automated Replacement of Unhealthy Nodes&lt;/h2>
&lt;p>&lt;em>Special thanks to &lt;a href="https://github.com/enxebre">Alberto García Lamela&lt;/a>, and &lt;a href="http://github.com/joelspeed">Joel Speed&lt;/a>&lt;/em>&lt;/p>
&lt;p>There are many reasons why a node might be unhealthy. The kubelet process may stop. The container runtime might have a bug. The kernel might have a memory leak. The disk may run out of space. CPU, disk, or memory hardware may fail. A power outage may happen. Failures like these are especially common in larger clusters.&lt;/p>
&lt;p>Kubernetes is designed to tolerate them, and to help your applications tolerate them as well. Nevertheless, only a finite number of nodes can be unhealthy before the cluster runs out of resources, and Pods are evicted or not scheduled in the first place. Unhealthy nodes should be repaired or replaced at the earliest opportunity.&lt;/p>
&lt;p>The Cluster API now includes a MachineHealthCheck resource, and a controller that monitors node health. When it detects an unhealthy node, it removes it. (Another Cluster API controller detects the node has been removed and replaces it.) You can configure the controller to suit your needs. You can configure how long to wait before removing the node. You can also set a threshold for the number of unhealthy nodes. When the threshold is reached, no more nodes are removed. The wait can be used to tolerate short-lived outages, and the threshold to prevent too many nodes from being replaced at the same time.&lt;/p>
&lt;p>The controller will remove only nodes managed by a Cluster API MachineSet. The controller does not remove control plane nodes, whether managed by the Kubeadm-based Control Plane, or by the user, as in v1alpha2. For more, see &lt;a href="https://cluster-api.sigs.k8s.io/tasks/healthcheck.html#limitations-and-caveats-of-a-machinehealthcheck">Limits and Caveats of a MachineHealthCheck&lt;/a>.&lt;/p>
&lt;p>Here is an example of a MachineHealthCheck. For more details, see &lt;a href="https://cluster-api.sigs.k8s.io/tasks/healthcheck.html">Configure a MachineHealthCheck&lt;/a> in the Cluster API book.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cluster.x-k8s.io/v1alpha3&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>MachineHealthCheck&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-node-unhealthy-5m&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">clusterName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">maxUnhealthy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">33&lt;/span>%&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">nodeStartupTimeout&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>10m&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">nodepool&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nodepool&lt;span style="color:#666">-0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">unhealthyConditions&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Ready&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">status&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Unknown&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">timeout&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>300s&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Ready&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">status&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;False&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">timeout&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>300s&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="infrastructure-managed-node-groups">Infrastructure-Managed Node Groups&lt;/h2>
&lt;p>&lt;em>Special thanks to &lt;a href="https://github.com/juan-lee">Juan-Lee Pang&lt;/a> and &lt;a href="https://github.com/CecileRobertMichon">Cecile Robert-Michon&lt;/a>&lt;/em>&lt;/p>
&lt;p>If you run large clusters, you need to create and destroy hundreds of nodes, sometimes in minutes. Although public clouds make it possible to work with large numbers of nodes, having to make a separate API request to create or delete every node may scale poorly. For example, API requests may have to be delayed to stay within rate limits.&lt;/p>
&lt;p>Some public clouds offer APIs to manage groups of nodes as one single entity. For example, AWS has AutoScaling Groups, Azure has Virtual Machine Scale Sets, and GCP has Managed Instance Groups. With this release of Cluster API, Infrastructure Providers can add support for these APIs, and users can deploy groups of Cluster API Machines by using the MachinePool Resource. For more information, see the &lt;a href="https://github.com/kubernetes-sigs/cluster-api/blob/bf51a2502f9007b531f6a9a2c1a4eae1586fb8ca/docs/proposals/20190919-machinepool-api.md">proposal&lt;/a> in the Cluster API repository.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Experimental Feature&lt;/strong>
The MachinePool API is an experimental feature that is not enabled by default. Users are encouraged to try it and report on how well it meets their needs.&lt;/p>
&lt;/blockquote>
&lt;h2 id="the-cluster-api-user-experience-reimagined">The Cluster API User Experience, Reimagined&lt;/h2>
&lt;h3 id="clusterctl">clusterctl&lt;/h3>
&lt;p>&lt;em>Special thanks to &lt;a href="https://github.com/fabriziopandini">Fabrizio Pandini&lt;/a>&lt;/em>&lt;/p>
&lt;p>If you are new to Cluster API, your first experience will probably be with the project's command-line tool, clusterctl. And with the new Cluster API release, it has been re-designed to be more pleasing to use than before. The tool is all you need to deploy your first &lt;a href="https://cluster-api.sigs.k8s.io/reference/glossary.html?highlight=pool#workload-cluster">workload cluster&lt;/a> in just a few steps.&lt;/p>
&lt;p>First, use &lt;code>clusterctl init&lt;/code> to &lt;a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/init.html">fetch the configuration&lt;/a> for your Infrastructure and Bootstrap Providers and deploy all of the components that make up the Cluster API. Second, use &lt;code>clusterctl config cluster&lt;/code> to &lt;a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/config-cluster.html">create the workload cluster manifest&lt;/a>. This manifest is just a collection of Kubernetes objects. To create the workload cluster, just &lt;code>kubectl apply&lt;/code> the manifest. Don't be surprised if this workflow looks familiar: Deploying a workload cluster with Cluster API is just like deploying an application workload with Kubernetes!&lt;/p>
&lt;p>Clusterctl also helps with the &amp;quot;day 2&amp;quot; operations. Use &lt;code>clusterctl move&lt;/code> to &lt;a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/move.html">migrate Cluster API custom resources&lt;/a>, such as Clusters, and Machines, from one &lt;a href="https://cluster-api.sigs.k8s.io/reference/glossary.html#management-cluster">Management Cluster&lt;/a> to another. This step--also known as a &lt;a href="https://cluster-api.sigs.k8s.io/reference/glossary.html#pivot">pivot&lt;/a>--is necessary to create a workload cluster that manages itself with Cluster API. Finally, use &lt;code>clusterctl upgrade&lt;/code> to &lt;a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/upgrade.html">upgrade all of the installed components&lt;/a> when a new Cluster API release becomes available.&lt;/p>
&lt;p>One more thing! Clusterctl is not only a command-line tool. It is also a Go library! Think of the library as an integration point for projects that build on top of Cluster API. All of clusterctl's command-line functionality is available in the library, making it easy to integrate into your stack. To get started with the library, please read its &lt;a href="https://pkg.go.dev/sigs.k8s.io/cluster-api@v0.3.1/cmd/clusterctl/client?tab=doc">documentation&lt;/a>.&lt;/p>
&lt;h3 id="the-cluster-api-book">The Cluster API Book&lt;/h3>
&lt;p>&lt;em>Thanks to many contributors!&lt;/em>&lt;/p>
&lt;p>The &lt;a href="https://cluster-api.sigs.k8s.io/">project's documentation&lt;/a> is extensive. New users should get some background on the &lt;a href="https://cluster-api.sigs.k8s.io/user/concepts.html">architecture&lt;/a>, and then create a cluster of their own with the &lt;a href="https://cluster-api.sigs.k8s.io/user/quick-start.html">Quick Start&lt;/a>. The clusterctl tool has its own &lt;a href="https://cluster-api.sigs.k8s.io/clusterctl/overview.html">reference&lt;/a>. The &lt;a href="https://cluster-api.sigs.k8s.io/developer/guide.html">Developer Guide&lt;/a> has plenty of information for anyone interested in contributing to the project.&lt;/p>
&lt;p>Above and beyond the content itself, the project's documentation site is a pleasure to use. It is searchable, has an outline, and even supports different color themes. If you think the site a lot like the documentation for a different community project, &lt;a href="https://book.kubebuilder.io/">Kubebuilder&lt;/a>, that is no coincidence! Many thanks to Kubebuilder authors for creating a great example of documentation. And many thanks to the &lt;a href="https://github.com/rust-lang/mdBook">mdBook&lt;/a> authors for creating a great tool for building documentation.&lt;/p>
&lt;h2 id="integrate-customize">Integrate &amp;amp; Customize&lt;/h2>
&lt;h3 id="end-to-end-test-framework">End-to-End Test Framework&lt;/h3>
&lt;p>&lt;em>Special thanks to &lt;a href="https://github.com/chuckha">Chuck Ha&lt;/a>&lt;/em>&lt;/p>
&lt;p>The Cluster API project is designed to be extensible. For example, anyone can develop their own Infrastructure and Bootstrap Providers. However, it's important that Providers work in a uniform way. And, because the project is still evolving, it takes work to make sure that Providers are up-to-date with new releases of the core.&lt;/p>
&lt;p>The End-to-End Test Framework provides a set of standard tests for developers to verify that their Providers integrate correctly with the current release of Cluster API, and help identify any regressions that happen after a new release of the Cluster API, or the Provider.&lt;/p>
&lt;p>For more details on the Framework, see &lt;a href="https://cluster-api.sigs.k8s.io/developer/testing.html?highlight=e2e#running-the-end-to-end-tests">Testing&lt;/a> in the Cluster API Book, and the &lt;a href="https://github.com/kubernetes-sigs/cluster-api/tree/master/test/framework">README&lt;/a> in the repository.&lt;/p>
&lt;h3 id="provider-implementer-s-guide">Provider Implementer's Guide&lt;/h3>
&lt;p>&lt;em>Thanks to many contributors!&lt;/em>&lt;/p>
&lt;p>The community maintains &lt;a href="https://cluster-api.sigs.k8s.io/reference/providers.html">Infrastructure Providers&lt;/a> for a many popular infrastructures. However, if you want to build your own Infrastructure or Bootstrap Provider, the &lt;a href="https://cluster-api.sigs.k8s.io/developer/providers/implementers-guide/overview.html">Provider Implementer's&lt;/a> guide explains the entire process, from creating a git repository, to creating CustomResourceDefinitions for your Providers, to designing, implementing, and testing the controllers.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Under Active Development&lt;/strong>
The Provider Implementer's Guide is actively under development, and may not yet reflect all of the changes in the v1alpha3 release.&lt;/p>
&lt;/blockquote>
&lt;h2 id="join-us">Join Us!&lt;/h2>
&lt;p>The Cluster API project is a very active project, and covers many areas of interest. If you are an infrastructure expert, you can contribute to one of the Infrastructure Providers. If you like building controllers, you will find opportunities to innovate. If you're curious about testing distributed systems, you can help develop the project's end-to-end test framework. Whatever your interests and background, you can make a real impact on the project.&lt;/p>
&lt;p>Come introduce yourself to the community at our weekly meeting, where we dedicate a block of time for a Q&amp;amp;A session. You can also find maintainers and users on the Kubernetes Slack, and in the Kubernetes forum. Please check out the links below. We look forward to seeing you!&lt;/p>
&lt;ul>
&lt;li>Chat with us on the Kubernetes &lt;a href="http://slack.k8s.io/">Slack&lt;/a>:&lt;a href="https://kubernetes.slack.com/archives/C8TSNPY4T"> #cluster-api&lt;/a>&lt;/li>
&lt;li>Join the &lt;a href="https://groups.google.com/forum/">sig-cluster-lifecycle&lt;/a> Google Group to receive calendar invites and gain access to documents&lt;/li>
&lt;li>Join our &lt;a href="https://zoom.us/j/861487554">Zoom meeting&lt;/a>, every Wednesday at 10:00 Pacific Time&lt;/li>
&lt;li>Post to the &lt;a href="https://discuss.kubernetes.io/c/contributors/cluster-api">Cluster API community forum&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: How Kubernetes contributors are building a better communication process</title><link>https://kubernetes.io/blog/2020/04/21/contributor-communication/</link><pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/21/contributor-communication/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Paris Pittman&lt;/p>
&lt;blockquote>
&lt;p>&amp;quot;Perhaps we just need to use a different word. We may need to use community development or project advocacy as a word in the open source realm as opposed to marketing, and perhaps then people will realize that they need to do it.&amp;quot;
~ &lt;a href="https://todogroup.org/www.linkedin.com/in/nithyaruff/">&lt;em>Nithya Ruff&lt;/em>&lt;/a> (from &lt;a href="https://todogroup.org/guides/marketing-open-source-projects/">&lt;em>TODO Group&lt;/em>&lt;/a>)&lt;/p>
&lt;/blockquote>
&lt;p>A common way to participate in the Kubernetes contributor community is
to be everywhere.&lt;/p>
&lt;p>We have an active &lt;a href="https://slack.k8s.io">Slack&lt;/a>, many mailing lists, Twitter account(s), and
dozens of community-driven podcasts and newsletters that highlight all
end-user, contributor, and ecosystem topics. And to add on to that, we also have &lt;a href="http://github.com/kubernetes/community">repositories of amazing documentation&lt;/a>, tons of &lt;a href="https://calendar.google.com/calendar/embed?src=cgnt364vd8s86hr2phapfjc6uk%40group.calendar.google.com&amp;amp;ctz=America%2FLos_Angeles">meetings&lt;/a> that drive the project forward, and &lt;a href="https://www.youtube.com/watch?v=yqB_le-N6EE">recorded code deep dives&lt;/a>. All of this information is incredibly valuable,
but it can be too much.&lt;/p>
&lt;p>Keeping up with thousands of contributors can be a challenge for anyone,
but this task of consuming information straight from the firehose is
particularly challenging for new community members. It's no secret that
the project is vast for contributors and users alike.&lt;/p>
&lt;p>To paint a picture with numbers:&lt;/p>
&lt;ul>
&lt;li>43,000 contributors&lt;/li>
&lt;li>6,579 members in #kubernetes-dev slack channel&lt;/li>
&lt;li>52 mailing lists (kubernetes-dev@ has thousands of members; sig-networking@ has 1000 alone)&lt;/li>
&lt;li>40 &lt;a href="https://github.com/kubernetes/community/blob/master/governance.md#community-groups">community groups&lt;/a>&lt;/li>
&lt;li>30 &lt;a href="https://calendar.google.com/calendar/embed?src=cgnt364vd8s86hr2phapfjc6uk%40group.calendar.google.com&amp;amp;ctz=America%2FLos_Angeles">upstream meetings&lt;/a> &lt;em>this&lt;/em> week alone&lt;/li>
&lt;/ul>
&lt;p>All of these numbers are only growing in scale, and with that comes the need to simplify how contributors get the information right information front-and-center.&lt;/p>
&lt;h2 id="how-we-got-here">How we got here&lt;/h2>
&lt;p>Kubernetes (K8s for short) communication grew out of a need for people
to connect in our growing community. With the best of intentions, the
community spun up channels for people to connect. This energy was part
of what helped Kubernetes grow so fast, and it also had us in sprawling out far and wide. As adoption grew, &lt;a href="https://github.com/kubernetes/community/issues/2466">contributors knew there was a need for standardization&lt;/a>.&lt;/p>
&lt;p>This new attention to how the community communicates led to the discovery
of a complex web of options. There were so many options, and it was a
challenge for anyone to be sure they were in the right place to receive
the right information. We started taking immediate action combining communication streams and thinking about how to reach out best to serve our community. We also asked for feedback from all our
contributors directly via &lt;a href="https://github.com/kubernetes/community/tree/master/sig-contributor-experience/surveys">&lt;strong>annual surveys&lt;/strong>&lt;/a>
to see where folks were actually reading the news that influences their
experiences here in our community.&lt;/p>
&lt;p>&lt;img src="https://user-images.githubusercontent.com/1744971/79478603-3a3a1980-7fd1-11ea-8b7a-d36aac7a097b.png" alt="Kubernetes channel access">&lt;/p>
&lt;p>With over 43,000 contributors, our contributor base is larger than many enterprise companies. You can imagine what it's like getting important messages across to make sure they are landing and folks are taking action.&lt;/p>
&lt;h2 id="contributing-to-better-communication">Contributing to better communication&lt;/h2>
&lt;p>Think about how your company/employer solves for this kind of communication challenge. Many have done so
by building internal marketing and communication focus areas in
marketing departments. So that's what we are doing. This has also been
applied &lt;a href="https://fedoraproject.org/wiki/Marketing">at Fedora&lt;/a> and at a smaller
scale in our very &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">own release&lt;/a> and &lt;a href="https://github.com/kubernetes/community/blob/d0fd6c16f7ee754b08082cc15658eb8db7afeaf8/events/events-team/marketing/README.md">contributor summit&lt;/a> planning
teams as roles.&lt;/p>
&lt;p>We have hit the accelerator on an &lt;strong>upstream marketing group&lt;/strong> under SIG
Contributor Experience and we want to tackle this challenge straight on.
We've learned in other contributor areas that creating roles for
contributors is super helpful - onboarding, breaking down work, and
ownership. &lt;a href="https://github.com/kubernetes/community/tree/master/communication/marketing-team">Here's our team charting the course&lt;/a>.&lt;/p>
&lt;p>Journey your way through our other documents like our &lt;a href="https://github.com/kubernetes/community/blob/master/communication/marketing-team/CHARTER.md">charter&lt;/a> if you are
interested in our mission and scope.&lt;/p>
&lt;p>Many of you close to the ecosystem might be scratching your head - isn't
this what CNCF does?&lt;/p>
&lt;p>Yes and no. The CNCF has 40+ other projects that need to be marketed to
a countless number of different types of community members in distinct
ways and they aren't responsible for the day to day operations of their
projects. They absolutely do partner with us to highlight what we need
and when we need it, and they do a fantastic job of it (one example is
the &lt;a href="https://twitter.com/kubernetesio">&lt;em>@kubernetesio Twitter account&lt;/em>&lt;/a> and its 200,000
followers).&lt;/p>
&lt;p>Where this group differs is in its scope: we are entirely
focused on elevating the hard work being done throughout the Kubernetes
community by its contributors.&lt;/p>
&lt;h2 id="what-to-expect-from-us">What to expect from us&lt;/h2>
&lt;p>You can expect to see us on the Kubernetes &lt;a href="https://github.com/kubernetes/community/tree/master/communication">communication channels&lt;/a> supporting you by:&lt;/p>
&lt;ul>
&lt;li>Finding ways of adding our human touch to potentially overwhelming
quantities of info by storytelling and other methods - we want to
highlight the work you are doing and provide useful information!&lt;/li>
&lt;li>Keeping you in the know of the comings and goings of contributor
community events, activities, mentoring initiatives, KEPs, and more.&lt;/li>
&lt;li>Creating a presence on Twitter specifically for contributors via
@k8scontributors that is all about being a contributor in all its
forms.&lt;/li>
&lt;/ul>
&lt;p>What does this look like in the wild? Our &lt;a href="https://kubernetes.io/blog/2020/03/19/join-sig-scalability/">first post&lt;/a> in a series about our 36 community groups landed recently. Did you see it?
More articles like this and additional themes of stories to flow through
&lt;a href="https://github.com/kubernetes/community/tree/master/communication/marketing-team#purpose">our storytellers&lt;/a>.&lt;/p>
&lt;p>We will deliver this with an &lt;a href="https://github.com/kubernetes/community/blob/master/communication/marketing-team/CHARTER.md#ethosvision">ethos&lt;/a> behind us aligned to the Kubernetes project as a whole, and we're
committed to using the same tools as all the other SIGs to do so. Check out our &lt;a href="https://github.com/orgs/kubernetes/projects/39">project board&lt;/a> to view our roadmap of upcoming work.&lt;/p>
&lt;h2 id="join-us-and-be-part-of-the-story">Join us and be part of the story&lt;/h2>
&lt;p>This initiative is in an early phase and we still have important roles to fill to make it successful.&lt;/p>
&lt;p>If you are interested in open sourcing marketing functions – it's a fun
ride – join us! Specific immediate roles include storytelling through
blogs and as a designer. We also have plenty of work in progress on our project board.
Add a comment to any open issue to let us know you're interested in getting involved.&lt;/p>
&lt;p>Also, if you're reading this, you're exactly the type of person we are
here to support. We would love to hear about how to improve, feedback,
or how we can work together.&lt;/p>
&lt;p>Reach out at one of the contact methods listed on &lt;a href="https://github.com/kubernetes/community/tree/master/communication/marketing-team#contact-us">our README&lt;/a>. We would love to hear from you.&lt;/p></description></item><item><title>Blog: API Priority and Fairness Alpha</title><link>https://kubernetes.io/blog/2020/04/06/kubernetes-1-18-feature-api-priority-and-fairness-alpha/</link><pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/06/kubernetes-1-18-feature-api-priority-and-fairness-alpha/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Min Kim (Ant Financial), Mike Spreitzer (IBM), Daniel Smith (Google)&lt;/p>
&lt;p>This blog describes “API Priority And Fairness”, a new alpha feature in Kubernetes 1.18. API Priority And Fairness permits cluster administrators to divide the concurrency of the control plane into different weighted priority levels. Every request arriving at a kube-apiserver will be categorized into one of the priority levels and get its fair share of the control plane’s throughput.&lt;/p>
&lt;h2 id="what-problem-does-this-solve">What problem does this solve?&lt;/h2>
&lt;p>Today the apiserver has a simple mechanism for protecting itself against CPU and memory overloads: max-in-flight limits for mutating and for readonly requests. Apart from the distinction between mutating and readonly, no other distinctions are made among requests; consequently, there can be undesirable scenarios where one subset of the requests crowds out other requests.&lt;/p>
&lt;p>In short, it is far too easy for Kubernetes workloads to accidentally DoS the apiservers, causing other important traffic--like system controllers or leader elections---to fail intermittently. In the worst cases, a few broken nodes or controllers can push a busy cluster over the edge, turning a local problem into a control plane outage.&lt;/p>
&lt;h2 id="how-do-we-solve-the-problem">How do we solve the problem?&lt;/h2>
&lt;p>The new feature “API Priority and Fairness” is about generalizing the existing max-in-flight request handler in each apiserver, to make the behavior more intelligent and configurable. The overall approach is as follows.&lt;/p>
&lt;ol>
&lt;li>Each request is matched by a &lt;em>Flow Schema&lt;/em>. The Flow Schema states the Priority Level for requests that match it, and assigns a “flow identifier” to these requests. Flow identifiers are how the system determines whether requests are from the same source or not.&lt;/li>
&lt;li>Priority Levels may be configured to behave in several ways. Each Priority Level gets its own isolated concurrency pool. Priority levels also introduce the concept of queuing requests that cannot be serviced immediately.&lt;/li>
&lt;li>To prevent any one user or namespace from monopolizing a Priority Level, they may be configured to have multiple queues. &lt;a href="https://aws.amazon.com/builders-library/workload-isolation-using-shuffle-sharding/#What_is_shuffle_sharding.3F">“Shuffle Sharding”&lt;/a> is used to assign each flow of requests to a subset of the queues.&lt;/li>
&lt;li>Finally, when there is capacity to service a request, a &lt;a href="https://en.wikipedia.org/wiki/Fair_queuing">“Fair Queuing”&lt;/a> algorithm is used to select the next request. Within each priority level the queues compete with even fairness.&lt;/li>
&lt;/ol>
&lt;p>Early results have been very promising! Take a look at this &lt;a href="https://github.com/kubernetes/kubernetes/pull/88177#issuecomment-588945806">analysis&lt;/a>.&lt;/p>
&lt;h2 id="how-do-i-try-this-out">How do I try this out?&lt;/h2>
&lt;p>You are required to prepare the following things in order to try out the feature:&lt;/p>
&lt;ul>
&lt;li>Download and install a kubectl greater than v1.18.0 version&lt;/li>
&lt;li>Enabling the new API groups with the command line flag &lt;code>--runtime-config=&amp;quot;flowcontrol.apiserver.k8s.io/v1alpha1=true&amp;quot;&lt;/code> on the kube-apiservers&lt;/li>
&lt;li>Switch on the feature gate with the command line flag &lt;code>--feature-gates=APIPriorityAndFairness=true&lt;/code> on the kube-apiservers&lt;/li>
&lt;/ul>
&lt;p>After successfully starting your kube-apiservers, you will see a few default FlowSchema and PriorityLevelConfiguration resources in the cluster. These default configurations are designed for a general protection and traffic management for your cluster.
You can examine and customize the default configuration by running the usual tools, e.g.:&lt;/p>
&lt;ul>
&lt;li>&lt;code>kubectl get flowschemas&lt;/code>&lt;/li>
&lt;li>&lt;code>kubectl get prioritylevelconfigurations&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="how-does-this-work-under-the-hood">How does this work under the hood?&lt;/h2>
&lt;p>Upon arrival at the handler, a request is assigned to exactly one priority level and exactly one flow within that priority level. Hence understanding how FlowSchema and PriorityLevelConfiguration works will be helping you manage the request traffic going through your kube-apiservers.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>FlowSchema: FlowSchema will identify a PriorityLevelConfiguration object and the way to compute the request’s “flow identifier”. Currently we support matching requests according to: the identity making the request, the verb, and the target object. The identity can match in terms of: a username, a user group name, or a ServiceAccount. And as for the target objects, we can match by apiGroup, resource[/subresource], and namespace.&lt;/p>
&lt;ul>
&lt;li>The flow identifier is used for shuffle sharding, so it’s important that requests have the same flow identifier if they are from the same source! We like to consider scenarios with “elephants” (which send many/heavy requests) vs “mice” (which send few/light requests): it is important to make sure the elephant’s requests all get the same flow identifier, otherwise they will look like many different mice to the system!&lt;/li>
&lt;li>See the API Documentation &lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#flowschema-v1alpha1-flowcontrol-apiserver-k8s-io">here&lt;/a>!&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>PriorityLevelConfiguration: Defines a priority level.&lt;/p>
&lt;ul>
&lt;li>For apiserver self requests, and any reentrant traffic (e.g., admission webhooks which themselves make API requests), a Priority Level can be marked “exempt”, which means that no queueing or limiting of any sort is done. This is to prevent priority inversions.&lt;/li>
&lt;li>Each non-exempt Priority Level is configured with a number of &amp;quot;concurrency shares&amp;quot; and gets an isolated pool of concurrency to use. Requests of that Priority Level run in that pool when it is not full, never anywhere else. Each apiserver is configured with a total concurrency limit (taken to be the sum of the old limits on mutating and readonly requests), and this is then divided among the Priority Levels in proportion to their concurrency shares.&lt;/li>
&lt;li>A non-exempt Priority Level may select a number of queues and a &amp;quot;hand size&amp;quot; to use for the shuffle sharding. Shuffle sharding maps flows to queues in a way that is better than consistent hashing. A given flow has access to a small collection of queues, and for each incoming request the shortest queue is chosen. When a Priority Level has queues, it also sets a limit on queue length. There is also a limit placed on how long a request can wait in its queue; this is a fixed fraction of the apiserver's request timeout. A request that cannot be executed and cannot be queued (any longer) is rejected.&lt;/li>
&lt;li>Alternatively, a non-exempt Priority Level may select immediate rejection instead of waiting in a queue.&lt;/li>
&lt;li>See the &lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#prioritylevelconfiguration-v1alpha1-flowcontrol-apiserver-k8s-io">API documentation&lt;/a> for this feature.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="what-s-missing-when-will-there-be-a-beta">What’s missing? When will there be a beta?&lt;/h2>
&lt;p>We’re already planning a few enhancements based on alpha and there will be more as users send feedback to our community. Here’s a list of them:&lt;/p>
&lt;ul>
&lt;li>Traffic management for WATCH and EXEC requests&lt;/li>
&lt;li>Adjusting and improving the default set of FlowSchema/PriorityLevelConfiguration&lt;/li>
&lt;li>Enhancing observability on how this feature works&lt;/li>
&lt;li>Join the discussion &lt;a href="https://github.com/kubernetes/enhancements/pull/1632">here&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Possibly treat LIST requests differently depending on an estimate of how big their result will be.&lt;/p>
&lt;h2 id="how-can-i-get-involved">How can I get involved?&lt;/h2>
&lt;p>As always! Reach us on slack &lt;a href="https://kubernetes.slack.com/messages/sig-api-machinery">#sig-api-machinery&lt;/a>, or through the &lt;a href="https://groups.google.com/forum/#!forum/kubernetes-sig-api-machinery">mailing list&lt;/a>. We have lots of exciting features to build and can use all sorts of help.&lt;/p>
&lt;p>Many thanks to the contributors that have gotten this feature this far: Aaron Prindle, Daniel Smith, Jonathan Tomer, Mike Spreitzer, Min Kim, Bruce Ma, Yu Liao, Mengyi Zhou!&lt;/p></description></item><item><title>Blog: Introducing Windows CSI support alpha for Kubernetes</title><link>https://kubernetes.io/blog/2020/04/03/kubernetes-1-18-feature-windows-csi-support-alpha/</link><pubDate>Fri, 03 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/03/kubernetes-1-18-feature-windows-csi-support-alpha/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Authors: Deep Debroy [Docker], Jing Xu [Google], Krishnakumar R (KK) [Microsoft]&lt;/p>
&lt;p>&lt;em>The alpha version of &lt;a href="https://github.com/kubernetes-csi/csi-proxy">CSI Proxy&lt;/a> for Windows is being released with Kubernetes 1.18. CSI proxy enables CSI Drivers on Windows by allowing containers in Windows to perform privileged storage operations.&lt;/em>&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>Container Storage Interface (CSI) for Kubernetes went GA in the Kubernetes 1.13 release. CSI has become the standard for exposing block and file storage to containerized workloads on Container Orchestration systems (COs) like Kubernetes. It enables third-party storage providers to write and deploy plugins without the need to alter the core Kubernetes codebase. All new storage features will utilize CSI, therefore it is important to get CSI drivers to work on Windows.&lt;/p>
&lt;p>A CSI driver in Kubernetes has two main components: a controller plugin and a node plugin. The controller plugin generally does not need direct access to the host and can perform all its operations through the Kubernetes API and external control plane services (e.g. cloud storage service). The node plugin, however, requires direct access to the host for making block devices and/or file systems available to the Kubernetes kubelet. This was previously not possible for containers on Windows. With the release of &lt;a href="https://github.com/kubernetes-csi/csi-proxy">CSIProxy&lt;/a>, CSI drivers can now perform storage operations on the node. This inturn enables containerized CSI Drivers to run on Windows.&lt;/p>
&lt;h2 id="csi-support-for-windows-clusters">CSI support for Windows clusters&lt;/h2>
&lt;p>CSI drivers (e.g. AzureDisk, GCE PD, etc.) are recommended to be deployed as containers. CSI driver’s node plugin typically runs on every worker node in the cluster (as a DaemonSet). Node plugin containers need to run with elevated privileges to perform storage related operations. However, Windows currently does not support privileged containers. To solve this problem, &lt;a href="https://github.com/kubernetes-csi/csi-proxy">CSIProxy&lt;/a> makes it so that node plugins can now be deployed as unprivileged pods and then use the proxy to perform privileged storage operations on the node.&lt;/p>
&lt;h2 id="node-plugin-interactions-with-csiproxy">Node plugin interactions with CSIProxy&lt;/h2>
&lt;p>The design of the CSI proxy is captured in this &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-windows/20190714-windows-csi-support.md">KEP&lt;/a>. The following diagram depicts the interactions with the CSI node plugin and CSI proxy.&lt;/p>
&lt;p align="center">
&lt;img src="https://kubernetes.io/images/blog/2020-04-03-Introducing-Windows-CSI-support-alpha-for-Kubernetes/CSIProxyOverview.png">
&lt;/p>
&lt;p>The CSI proxy runs as a process directly on the host on every windows node - very similar to kubelet. The CSI code in kubelet interacts with the &lt;a href="https://kubernetes-csi.github.io/docs/node-driver-registrar.html">node driver registrar&lt;/a> component and the CSI node plugin. The node driver registrar is a community maintained CSI project which handles the registration of vendor specific node plugins. The kubelet initiates CSI gRPC calls like NodeStageVolume/NodePublishVolume on the node plugin as described in the figure. Node plugins interface with the CSIProxy process to perform local host OS storage related operations such as creation/enumeration of volumes, mounting/unmounting, etc.&lt;/p>
&lt;h2 id="csi-proxy-architecture-and-implementation">CSI proxy architecture and implementation&lt;/h2>
&lt;p align="center">
&lt;img src="https://kubernetes.io/images/blog/2020-04-03-Introducing-Windows-CSI-support-alpha-for-Kubernetes/CSIProxyArchitecture.png">
&lt;/p>
&lt;p>In the alpha release, CSIProxy supports the following API groups:&lt;/p>
&lt;ol>
&lt;li>Filesystem&lt;/li>
&lt;li>Disk&lt;/li>
&lt;li>Volume&lt;/li>
&lt;li>SMB&lt;/li>
&lt;/ol>
&lt;p>CSI proxy exposes each API group via a Windows named pipe. The communication is performed using gRPC over these pipes. The client library from the CSI proxy project uses these pipes to interact with the CSI proxy APIs. For example, the filesystem APIs are exposed via a pipe like &lt;code>\.\pipe\csi-proxy-filesystem-v1alpha1&lt;/code> and volume APIs under the &lt;code>\.\pipe\csi-proxy-volume-v1alpha1&lt;/code>, and so on.&lt;/p>
&lt;p>From each API group service, the calls are routed to the host API layer. The host API calls into the host Windows OS by either Powershell or Go standard library calls. For example, when the filesystem API &lt;a href="https://github.com/kubernetes-csi/csi-proxy/blob/master/client/api/filesystem/v1alpha1/api.proto">Rmdir&lt;/a> is called the API group service would decode the grpc structure &lt;a href="https://github.com/kubernetes-csi/csi-proxy/blob/master/client/api/filesystem/v1alpha1/api.pb.go">RmdirRequest&lt;/a> and find the directory to be removed and call into the Host APIs layer. This would result in a call to &lt;a href="https://github.com/kubernetes-csi/csi-proxy/blob/master/internal/os/filesystem/api.go">os.Remove&lt;/a>, a Go standard library call, to perform the remove operation.&lt;/p>
&lt;h2 id="control-flow-details">Control flow details&lt;/h2>
&lt;p>The following figure uses CSI call NodeStageVolume as an example to explain the interaction between kubelet, CSI plugin, and CSI proxy for provisioning a fresh volume. After the node plugin receives a CSI RPC call, it makes a few calls to CSIproxy accordingly. As a result of the NodeStageVolume call, first the required disk is identified using either of the Disk API calls: ListDiskLocations (in AzureDisk driver) or GetDiskNumberByName (in GCE PD driver). If the disk is not partitioned, then the PartitionDisk (Disk API group) is called. Subsequently, Volume API calls such as ListVolumesOnDisk, FormatVolume and MountVolume are called to perform the rest of the required operations. Similar operations are performed in case of NodeUnstageVolume, NodePublishVolume, NodeUnpublishedVolume, etc.&lt;/p>
&lt;p align="center">
&lt;img src="https://kubernetes.io/images/blog/2020-04-03-Introducing-Windows-CSI-support-alpha-for-Kubernetes/CSIProxyControlFlow.png">
&lt;/p>
&lt;h2 id="current-support">Current support&lt;/h2>
&lt;p>CSI proxy is now available as alpha. You can find more details on the &lt;a href="https://github.com/kubernetes-csi/csi-proxy">CSIProxy&lt;/a> GitHub repository. There are currently two cloud providers that provide alpha support for CSI drivers on Windows: Azure and GCE.&lt;/p>
&lt;h2 id="future-plans">Future plans&lt;/h2>
&lt;p>One key area of focus in beta is going to be Windows based build and CI/CD setup to improve the stability and quality of the code base. Another area is using Go based calls directly instead of Powershell commandlets to improve performance. Enhancing debuggability and adding more tests are other areas which the team will be looking into.&lt;/p>
&lt;h2 id="how-to-get-involved">How to get involved?&lt;/h2>
&lt;p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. Those interested in getting involved with the design and development of CSI Proxy, or any part of the Kubernetes Storage system, may join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group&lt;/a> (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p>
&lt;p>For those interested in more details, the &lt;a href="https://github.com/kubernetes-csi/csi-proxy">CSIProxy&lt;/a> GitHub repository is a good place to start. In addition, the &lt;a href="https://kubernetes.slack.com/messages/csi-windows">#csi-windows&lt;/a> channel on kubernetes slack is available for discussions specific to the CSI on Windows.&lt;/p>
&lt;h2 id="acknowledgments">Acknowledgments&lt;/h2>
&lt;p>We would like to thank Michelle Au for guiding us throughout this journey to alpha. We would like to thank Jean Rougé for contributions during the initial CSI proxy effort. We would like to thank Saad Ali for all the guidance with respect to the project and review/feedback on a draft of this blog. We would like to thank Patrick Lang and Mark Rossetti for helping us with Windows specific questions and details. Special thanks to Andy Zhang for reviews and guidance with respect to Azuredisk and Azurefile work. A big thank you to Paul Burt and Karen Chu for the review and suggestions on improving this blog post.&lt;/p>
&lt;p>Last but not the least, we would like to thank the broader Kubernetes community who contributed at every step of the project.&lt;/p></description></item><item><title>Blog: Improvements to the Ingress API in Kubernetes 1.18</title><link>https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/</link><pubDate>Thu, 02 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Rob Scott (Google), Christopher M Luciano (IBM)&lt;/p>
&lt;p>The Ingress API in Kubernetes has enabled a large number of controllers to provide simple and powerful ways to manage inbound network traffic to Kubernetes workloads. In Kubernetes 1.18, we've made 3 significant additions to this API:&lt;/p>
&lt;ul>
&lt;li>A new &lt;code>pathType&lt;/code> field that can specify how Ingress paths should be matched.&lt;/li>
&lt;li>A new &lt;code>IngressClass&lt;/code> resource that can specify how Ingresses should be implemented by controllers.&lt;/li>
&lt;li>Support for wildcards in hostnames.&lt;/li>
&lt;/ul>
&lt;h2 id="better-path-matching-with-path-types">Better Path Matching With Path Types&lt;/h2>
&lt;p>The new concept of a path type allows you to specify how a path should be matched. There are three supported types:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>ImplementationSpecific (default):&lt;/strong> With this path type, matching is up to the controller implementing the &lt;code>IngressClass&lt;/code>. Implementations can treat this as a separate &lt;code>pathType&lt;/code> or treat it identically to the &lt;code>Prefix&lt;/code> or &lt;code>Exact&lt;/code> path types.&lt;/li>
&lt;li>&lt;strong>Exact:&lt;/strong> Matches the URL path exactly and with case sensitivity.&lt;/li>
&lt;li>&lt;strong>Prefix:&lt;/strong> Matches based on a URL path prefix split by &lt;code>/&lt;/code>. Matching is case sensitive and done on a path element by element basis.&lt;/li>
&lt;/ul>
&lt;h2 id="extended-configuration-with-ingress-classes">Extended Configuration With Ingress Classes&lt;/h2>
&lt;p>The Ingress resource was designed with simplicity in mind, providing a simple set of fields that would be applicable in all use cases. Over time, as use cases evolved, implementations began to rely on a long list of custom annotations for further configuration. The new &lt;code>IngressClass&lt;/code> resource provides a way to replace some of those annotations.&lt;/p>
&lt;p>Each &lt;code>IngressClass&lt;/code> specifies which controller should implement Ingresses of the class and can reference a custom resource with additional parameters.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;networking.k8s.io/v1beta1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;IngressClass&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;external-lb&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">controller&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;example.com/ingress-controller&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;k8s.example.com/v1alpha&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;IngressParameters&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;external-lb&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="specifying-the-class-of-an-ingress">Specifying the Class of an Ingress&lt;/h3>
&lt;p>A new &lt;code>ingressClassName&lt;/code> field has been added to the Ingress spec that is used to reference the &lt;code>IngressClass&lt;/code> that should be used to implement this Ingress.&lt;/p>
&lt;h3 id="deprecating-the-ingress-class-annotation">Deprecating the Ingress Class Annotation&lt;/h3>
&lt;p>Before the &lt;code>IngressClass&lt;/code> resource was added in Kubernetes 1.18, a similar concept of Ingress class was often specified with a &lt;code>kubernetes.io/ingress.class&lt;/code> annotation on the Ingress. Although this annotation was never formally defined, it was widely supported by Ingress controllers, and should now be considered formally deprecated.&lt;/p>
&lt;h3 id="setting-a-default-ingressclass">Setting a Default IngressClass&lt;/h3>
&lt;p>It’s possible to mark a specific &lt;code>IngressClass&lt;/code> as default in a cluster. Setting the
&lt;code>ingressclass.kubernetes.io/is-default-class&lt;/code> annotation to true on an
IngressClass resource will ensure that new Ingresses without an &lt;code>ingressClassName&lt;/code> specified will be assigned this default &lt;code>IngressClass&lt;/code>.&lt;/p>
&lt;h2 id="support-for-hostname-wildcards">Support for Hostname Wildcards&lt;/h2>
&lt;p>Many Ingress providers have supported wildcard hostname matching like &lt;code>*.foo.com&lt;/code> matching &lt;code>app1.foo.com&lt;/code>, but until now the spec assumed an exact FQDN match of the host. Hosts can now be precise matches (for example “&lt;code>foo.bar.com&lt;/code>”) or a wildcard (for example “&lt;code>*.foo.com&lt;/code>”). Precise matches require that the http host header matches the Host setting. Wildcard matches require the http host header is equal to the suffix of the wildcard rule.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Host&lt;/th>
&lt;th>Host header&lt;/th>
&lt;th>Match?&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>*.foo.com&lt;/code>&lt;/td>
&lt;td>&lt;code>bar.foo.com&lt;/code>&lt;/td>
&lt;td>Matches based on shared suffix&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>*.foo.com&lt;/code>&lt;/td>
&lt;td>&lt;code>baz.bar.foo.com&lt;/code>&lt;/td>
&lt;td>No match, wildcard only covers a single DNS label&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>*.foo.com&lt;/code>&lt;/td>
&lt;td>&lt;code>foo.com&lt;/code>&lt;/td>
&lt;td>No match, wildcard only covers a single DNS label&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="putting-it-all-together">Putting it All Together&lt;/h3>
&lt;p>These new Ingress features allow for much more configurability. Here’s an example of an Ingress that makes use of pathType, &lt;code>ingressClassName&lt;/code>, and a hostname wildcard:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;networking.k8s.io/v1beta1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Ingress&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;example-ingress&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">ingressClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;external-lb&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">rules&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">host&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;*.example.com&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">http&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">paths&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;/example&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">pathType&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Prefix&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">backend&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">serviceName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;example-service&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">servicePort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">80&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="ingress-controller-support">Ingress Controller Support&lt;/h3>
&lt;p>Since these features are new in Kubernetes 1.18, each Ingress controller implementation will need some time to develop support for these new features. Check the documentation for your preferred Ingress controllers to see when they will support this new functionality.&lt;/p>
&lt;h2 id="the-future-of-ingress">The Future of Ingress&lt;/h2>
&lt;p>The Ingress API is on pace to graduate from beta to a stable API in Kubernetes 1.19. It will continue to provide a simple way to manage inbound network traffic for Kubernetes workloads. This API has intentionally been kept simple and lightweight, but there has been a desire for greater configurability for more advanced use cases.&lt;/p>
&lt;p>Work is currently underway on a new highly configurable set of APIs that will provide an alternative to Ingress in the future. These APIs are being referred to as the new “Service APIs”. They are not intended to replace any existing APIs, but instead provide a more configurable alternative for complex use cases. For more information, check out the &lt;a href="http://github.com/kubernetes-sigs/service-apis">Service APIs repo on GitHub&lt;/a>.&lt;/p></description></item><item><title>Blog: Kubernetes 1.18 Feature Server-side Apply Beta 2</title><link>https://kubernetes.io/blog/2020/04/01/kubernetes-1.18-feature-server-side-apply-beta-2/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/01/kubernetes-1.18-feature-server-side-apply-beta-2/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Antoine Pelisse (Google)&lt;/p>
&lt;h2 id="what-is-server-side-apply">What is Server-side Apply?&lt;/h2>
&lt;p>Server-side Apply is an important effort to migrate “kubectl apply” to the apiserver. It was started in 2018 by the Apply working group.&lt;/p>
&lt;p>The use of kubectl to declaratively apply resources has exposed the following challenges:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>One needs to use the kubectl go code, or they have to shell out to kubectl.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Strategic merge-patch, the patch format used by kubectl, grew organically and was challenging to fix while maintaining compatibility with various api-server versions.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Some features are hard to implement directly on the client, for example, unions.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Server-side Apply is a new merging algorithm, as well as tracking of field ownership, running on the Kubernetes api-server. Server-side Apply enables new features like conflict detection, so the system knows when two actors are trying to edit the same field.&lt;/p>
&lt;h2 id="how-does-it-work-what-s-managedfields">How does it work, what’s managedFields?&lt;/h2>
&lt;p>Server-side Apply works by keeping track of which actor of the system has changed each field of an object. It does so by diffing all updates to objects, and recording all the fields that have changed as well the time of the operation. All this information is stored in the managedFields in the metadata of objects. Since objects can have many fields, this field can be quite large.&lt;/p>
&lt;p>When someone applies, we can then use the information stored within managedFields to report relevant conflicts and help the merge algorithm to do the right thing.&lt;/p>
&lt;h2 id="wasn-t-it-already-beta-before-1-18">Wasn’t it already Beta before 1.18?&lt;/h2>
&lt;p>Yes, Server-side Apply has been Beta since 1.16, but it didn’t track the owner for fields associated with objects that had not been applied. This means that most objects didn’t have the managedFields metadata stored, and conflicts for these objects cannot be resolved. With Kubernetes 1.18, all new objects will have the managedFields attached to them and provide accurate information on conflicts.&lt;/p>
&lt;h2 id="how-do-i-use-it">How do I use it?&lt;/h2>
&lt;p>The most common way to use this is through kubectl: &lt;code>kubectl apply --server-side&lt;/code>. This is likely to show conflicts with other actors, including client-side apply. When that happens, conflicts can be forced by using the &lt;code>--force-conflicts&lt;/code> flag, which will grab the ownership for the fields that have changed.&lt;/p>
&lt;h2 id="current-limitations">Current limitations&lt;/h2>
&lt;p>We have two important limitations right now, especially with sub-resources. The first is that if you apply with a status, the status is going to be ignored. We are still going to try and acquire the fields, which may lead to invalid conflicts. The other is that we do not update the managedFields on some sub-resources, including scale, so you may not see information about a horizontal pod autoscaler changing the number of replicas.&lt;/p>
&lt;h2 id="what-s-next">What’s next?&lt;/h2>
&lt;p>We are working hard to improve the experience of using server-side apply with kubectl, and we are trying to make it the default. As part of that, we want to improve the migration from client-side to server-side.&lt;/p>
&lt;h2 id="can-i-help">Can I help?&lt;/h2>
&lt;p>Of course! The working-group apply is available on slack #wg-apply, through the &lt;a href="https://groups.google.com/forum/#!forum/kubernetes-wg-apply">mailing list&lt;/a> and we also meet every other Tuesday at 9.30 PT on Zoom. We have lots of exciting features to build and can use all sorts of help.&lt;/p>
&lt;p>We would also like to use the opportunity to thank the hard work of all the contributors involved in making this new beta possible:&lt;/p>
&lt;ul>
&lt;li>Daniel Smith&lt;/li>
&lt;li>Jenny Buckley&lt;/li>
&lt;li>Joe Betz&lt;/li>
&lt;li>Julian Modesto&lt;/li>
&lt;li>Kevin Wiesmüller&lt;/li>
&lt;li>Maria Ntalla&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes Topology Manager Moves to Beta - Align Up!</title><link>https://kubernetes.io/blog/2020/04/01/kubernetes-1-18-feature-topoloy-manager-beta/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/01/kubernetes-1-18-feature-topoloy-manager-beta/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Kevin Klues (NVIDIA), Victor Pickard (Red Hat), Conor Nolan (Intel)&lt;/p>
&lt;p>This blog post describes the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>, a beta feature of Kubernetes in release 1.18. The &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> feature enables NUMA alignment of CPUs and peripheral devices (such as SR-IOV VFs and GPUs), allowing your workload to run in an environment optimized for low-latency.&lt;/p>
&lt;p>Prior to the introduction of the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>, the CPU and Device Manager would make resource allocation decisions independent of each other. This could result in undesirable allocations on multi-socket systems, causing degraded performance on latency critical applications. With the introduction of the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>, we now have a way to avoid this.&lt;/p>
&lt;p>This blog post covers:&lt;/p>
&lt;ol>
&lt;li>A brief introduction to NUMA and why it is important&lt;/li>
&lt;li>The policies available to end-users to ensure NUMA alignment of CPUs and devices&lt;/li>
&lt;li>The internal details of how the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> works&lt;/li>
&lt;li>Current limitations of the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>&lt;/li>
&lt;li>Future directions of the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>&lt;/li>
&lt;/ol>
&lt;h2 id="so-what-is-numa-and-why-do-i-care">So, what is NUMA and why do I care?&lt;/h2>
&lt;p>The term NUMA stands for Non-Uniform Memory Access. It is a technology available on multi-cpu systems that allows different CPUs to access different parts of memory at different speeds. Any memory directly connected to a CPU is considered &amp;quot;local&amp;quot; to that CPU and can be accessed very fast. Any memory not directly connected to a CPU is considered &amp;quot;non-local&amp;quot; and will have variable access times depending on how many interconnects must be passed through in order to reach it. On modern systems, the idea of having &amp;quot;local&amp;quot; vs. &amp;quot;non-local&amp;quot; memory can also be extended to peripheral devices such as NICs or GPUs. For high performance, CPUs and devices should be allocated such that they have access to the same local memory.&lt;/p>
&lt;p>All memory on a NUMA system is divided into a set of &amp;quot;NUMA nodes&amp;quot;, with each node representing the local memory for a set of CPUs or devices. We talk about an individual CPU as being part of a NUMA node if its local memory is associated with that NUMA node.&lt;/p>
&lt;p>We talk about a peripheral device as being part of a NUMA node based on the shortest number of interconnects that must be passed through in order to reach it.&lt;/p>
&lt;p>For example, in Figure 1, CPUs 0-3 are said to be part of NUMA node 0, whereas CPUs 4-7 are part of NUMA node 1. Likewise GPU 0 and NIC 0 are said to be part of NUMA node 0 because they are attached to Socket 0, whose CPUs are all part of NUMA node 0. The same is true for GPU 1 and NIC 1 on NUMA node 1.&lt;/p>
&lt;p align="center">
&lt;img height="300" src="https://kubernetes.io/images/blog/2020-03-25-kubernetes-1.18-release-announcement/example-numa-system.png">
&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> An example system with 2 NUMA nodes, 2 Sockets with 4 CPUs each, 2 GPUs, and 2 NICs. CPUs on Socket 0, GPU 0, and NIC 0 are all part of NUMA node 0. CPUs on Socket 1, GPU 1, and NIC 1 are all part of NUMA node 1.&lt;/p>
&lt;p>Although the example above shows a 1-1 mapping of NUMA Node to Socket, this is not necessarily true in the general case. There may be multiple sockets on a single NUMA node, or individual CPUs of a single socket may be connected to different NUMA nodes. Moreover, emerging technologies such as Sub-NUMA Clustering (&lt;a href="https://software.intel.com/en-us/articles/intel-xeon-processor-scalable-family-technical-overview">available on recent intel CPUs&lt;/a>) allow single CPUs to be associated with multiple NUMA nodes so long as their memory access times to both nodes are the same (or have a negligible difference).&lt;/p>
&lt;p>The &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> has been built to handle all of these scenarios.&lt;/p>
&lt;h2 id="align-up-it-s-a-team-effort">Align Up! It's a TeaM Effort!&lt;/h2>
&lt;p>As previously stated, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> allows users to align their CPU and peripheral device allocations by NUMA node. There are several policies available for this:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&lt;code>none:&lt;/code>&lt;/strong> this policy will not attempt to do any alignment of resources. It will act the same as if the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> were not present at all. This is the default policy.&lt;/li>
&lt;li>&lt;strong>&lt;code>best-effort:&lt;/code>&lt;/strong> with this policy, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> will attempt to align allocations on NUMA nodes as best it can, but will always allow the pod to start even if some of the allocated resources are not aligned on the same NUMA node.&lt;/li>
&lt;li>&lt;strong>&lt;code>restricted:&lt;/code>&lt;/strong> this policy is the same as the &lt;strong>&lt;code>best-effort&lt;/code>&lt;/strong> policy, except it will fail pod admission if allocated resources cannot be aligned properly. Unlike with the &lt;strong>&lt;code>single-numa-node&lt;/code>&lt;/strong> policy, some allocations may come from multiple NUMA nodes if it is impossible to &lt;em>ever&lt;/em> satisfy the allocation request on a single NUMA node (e.g. 2 devices are requested and the only 2 devices on the system are on different NUMA nodes).&lt;/li>
&lt;li>&lt;strong>&lt;code>single-numa-node:&lt;/code>&lt;/strong> this policy is the most restrictive and will only allow a pod to be admitted if &lt;em>all&lt;/em> requested CPUs and devices can be allocated from exactly one NUMA node.&lt;/li>
&lt;/ul>
&lt;p>It is important to note that the selected policy is applied to each container in a pod spec individually, rather than aligning resources across all containers together.&lt;/p>
&lt;p>Moreover, a single policy is applied to &lt;em>all&lt;/em> pods on a node via a global &lt;strong>&lt;code>kubelet&lt;/code>&lt;/strong> flag, rather than allowing users to select different policies on a pod-by-pod basis (or a container-by-container basis). We hope to relax this restriction in the future.&lt;/p>
&lt;p>The &lt;strong>&lt;code>kubelet&lt;/code>&lt;/strong> flag to set one of these policies can be seen below:&lt;/p>
&lt;pre>&lt;code>--topology-manager-policy=
[none | best-effort | restricted | single-numa-node]
&lt;/code>&lt;/pre>&lt;p>Additionally, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> is protected by a feature gate. This feature gate has been available since Kubernetes 1.16, but has only been enabled by default since 1.18.&lt;/p>
&lt;p>The feature gate can be enabled or disabled as follows (as described in more detail &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">here&lt;/a>):&lt;/p>
&lt;pre>&lt;code>--feature-gates=&amp;quot;...,TopologyManager=&amp;lt;true|false&amp;gt;&amp;quot;
&lt;/code>&lt;/pre>&lt;p>In order to trigger alignment according to the selected policy, a user must request CPUs and peripheral devices in their pod spec, according to a certain set of requirements.&lt;/p>
&lt;p>For peripheral devices, this means requesting devices from the available resources provided by a device plugin (e.g. &lt;strong>&lt;code>intel.com/sriov&lt;/code>&lt;/strong>, &lt;strong>&lt;code>nvidia.com/gpu&lt;/code>&lt;/strong>, etc.). This will only work if the device plugin has been extended to integrate properly with the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>. Currently, the only plugins known to have this extension are the &lt;a href="https://github.com/NVIDIA/k8s-device-plugin/blob/5cb45d52afdf5798a40f8d0de049bce77f689865/nvidia.go#L74">Nvidia GPU device plugin&lt;/a>, and the &lt;a href="https://github.com/intel/sriov-network-device-plugin/blob/30e33f1ce2fc7b45721b6de8c8207e65dbf2d508/pkg/resources/pciNetDevice.go#L80">Intel SRIOV network device plugin&lt;/a>. Details on how to extend a device plugin to integrate with the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> can be found &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-integration-with-the-topology-manager">here&lt;/a>.&lt;/p>
&lt;p>For CPUs, this requires that the &lt;strong>&lt;code>CPUManager&lt;/code>&lt;/strong> has been configured with its &lt;strong>&lt;code>--static&lt;/code>&lt;/strong> policy enabled and that the pod is running in the Guaranteed QoS class (i.e. all CPU and memory &lt;strong>&lt;code>limits&lt;/code>&lt;/strong> are equal to their respective CPU and memory &lt;strong>&lt;code>requests&lt;/code>&lt;/strong>). CPUs must also be requested in whole number values (e.g. &lt;strong>&lt;code>1&lt;/code>&lt;/strong>, &lt;strong>&lt;code>2&lt;/code>&lt;/strong>, &lt;strong>&lt;code>1000m&lt;/code>&lt;/strong>, etc). Details on how to set the &lt;strong>&lt;code>CPUManager&lt;/code>&lt;/strong> policy can be found &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#cpu-management-policies">here&lt;/a>.&lt;/p>
&lt;p>For example, assuming the &lt;strong>&lt;code>CPUManager&lt;/code>&lt;/strong> is running with its &lt;strong>&lt;code>--static&lt;/code>&lt;/strong> policy enabled and the device plugins for &lt;strong>&lt;code>gpu-vendor.com&lt;/code>&lt;/strong>, and &lt;strong>&lt;code>nic-vendor.com&lt;/code>&lt;/strong> have been extended to integrate with the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> properly, the pod spec below is sufficient to trigger the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> to run its selected policy:&lt;/p>
&lt;pre>&lt;code>spec:
containers:
- name: numa-aligned-container
image: alpine
resources:
limits:
cpu: 2
memory: 200Mi
gpu-vendor.com/gpu: 1
nic-vendor.com/nic: 1
&lt;/code>&lt;/pre>&lt;p>Following Figure 1 from the previous section, this would result in one of the following aligned allocations:&lt;/p>
&lt;pre>&lt;code>{cpu: {0, 1}, gpu: 0, nic: 0}
{cpu: {0, 2}, gpu: 0, nic: 0}
{cpu: {0, 3}, gpu: 0, nic: 0}
{cpu: {1, 2}, gpu: 0, nic: 0}
{cpu: {1, 3}, gpu: 0, nic: 0}
{cpu: {2, 3}, gpu: 0, nic: 0}
{cpu: {4, 5}, gpu: 1, nic: 1}
{cpu: {4, 6}, gpu: 1, nic: 1}
{cpu: {4, 7}, gpu: 1, nic: 1}
{cpu: {5, 6}, gpu: 1, nic: 1}
{cpu: {5, 7}, gpu: 1, nic: 1}
{cpu: {6, 7}, gpu: 1, nic: 1}
&lt;/code>&lt;/pre>&lt;p>And that’s it! Just follow this pattern to have the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> ensure NUMA alignment across containers that request topology-aware devices and exclusive CPUs.&lt;/p>
&lt;p>&lt;strong>NOTE:&lt;/strong> if a pod is rejected by one of the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> policies, it will be placed in a &lt;strong>&lt;code>Terminated&lt;/code>&lt;/strong> state with a pod admission error and a reason of &amp;quot;&lt;strong>&lt;code>TopologyAffinityError&lt;/code>&lt;/strong>&amp;quot;. Once a pod is in this state, the Kubernetes scheduler will not attempt to reschedule it. It is therefore recommended to use a &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment">&lt;strong>&lt;code>Deployment&lt;/code>&lt;/strong>&lt;/a> with replicas to trigger a redeploy of the pod on such a failure. An &lt;a href="https://kubernetes.io/docs/concepts/architecture/controller/">external control loop&lt;/a> can also be implemented to trigger a redeployment of pods that have a &lt;strong>&lt;code>TopologyAffinityError&lt;/code>&lt;/strong>.&lt;/p>
&lt;h2 id="this-is-great-so-how-does-it-work-under-the-hood">This is great, so how does it work under the hood?&lt;/h2>
&lt;p>Pseudocode for the primary logic carried out by the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> can be seen below:&lt;/p>
&lt;pre>&lt;code>for container := range append(InitContainers, Containers...) {
for provider := range HintProviders {
hints += provider.GetTopologyHints(container)
}
bestHint := policy.Merge(hints)
for provider := range HintProviders {
provider.Allocate(container, bestHint)
}
}
&lt;/code>&lt;/pre>&lt;p>The following diagram summarizes the steps taken during this loop:&lt;/p>
&lt;p align="center">
&lt;img weight="200" height="200" src="https://kubernetes.io/images/blog/2020-03-25-kubernetes-1.18-release-announcement/numa-steps-during-loop.png">
&lt;/p>
&lt;p>The steps themselves are:&lt;/p>
&lt;ol>
&lt;li>Loop over all containers in a pod.&lt;/li>
&lt;li>For each container, gather &amp;quot;&lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong>&amp;quot; from a set of &amp;quot;&lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong>&amp;quot; for each topology-aware resource type requested by the container (e.g. &lt;strong>&lt;code>gpu-vendor.com/gpu&lt;/code>&lt;/strong>, &lt;strong>&lt;code>nic-vendor.com/nic&lt;/code>&lt;/strong>, &lt;strong>&lt;code>cpu&lt;/code>&lt;/strong>, etc.).&lt;/li>
&lt;li>Using the selected policy, merge the gathered &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> to find the &amp;quot;best&amp;quot; hint that aligns resource allocations across all resource types.&lt;/li>
&lt;li>Loop back over the set of hint providers, instructing them to allocate the resources they control using the merged hint as a guide.&lt;/li>
&lt;li>This loop runs at pod admission time and will fail to admit the pod if any of these steps fail or alignment cannot be satisfied according to the selected policy. Any resources allocated before the failure are cleaned up accordingly.&lt;/li>
&lt;/ol>
&lt;p>The following sections go into more detail on the exact structure of &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> and &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong>, as well as some details on the merge strategies used by each policy.&lt;/p>
&lt;h3 id="topologyhints">TopologyHints&lt;/h3>
&lt;p>A &lt;strong>&lt;code>TopologyHint&lt;/code>&lt;/strong> encodes a set of constraints from which a given resource request can be satisfied. At present, the only constraint we consider is NUMA alignment. It is defined as follows:&lt;/p>
&lt;pre>&lt;code>type TopologyHint struct {
NUMANodeAffinity bitmask.BitMask
Preferred bool
}
&lt;/code>&lt;/pre>&lt;p>The &lt;strong>&lt;code>NUMANodeAffinity&lt;/code>&lt;/strong> field contains a bitmask of NUMA nodes where a resource request can be satisfied. For example, the possible masks on a system with 2 NUMA nodes include:&lt;/p>
&lt;pre>&lt;code>{00}, {01}, {10}, {11}
&lt;/code>&lt;/pre>&lt;p>The &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> field contains a boolean that encodes whether the given hint is &amp;quot;preferred&amp;quot; or not. With the &lt;strong>&lt;code>best-effort&lt;/code>&lt;/strong> policy, preferred hints will be given preference over non-preferred hints when generating a &amp;quot;best&amp;quot; hint. With the &lt;strong>&lt;code>restricted&lt;/code>&lt;/strong> and &lt;strong>&lt;code>single-numa-node&lt;/code>&lt;/strong> policies, non-preferred hints will be rejected.&lt;/p>
&lt;p>In general, &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> generate &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> by looking at the set of currently available resources that can satisfy a resource request. More specifically, they generate one &lt;strong>&lt;code>TopologyHint&lt;/code>&lt;/strong> for every possible mask of NUMA nodes where that resource request can be satisfied. If a mask cannot satisfy the request, it is omitted. For example, a &lt;strong>&lt;code>HintProvider&lt;/code>&lt;/strong> might provide the following hints on a system with 2 NUMA nodes when being asked to allocate 2 resources. These hints encode that both resources could either come from a single NUMA node (either 0 or 1), or they could each come from different NUMA nodes (but we prefer for them to come from just one).&lt;/p>
&lt;pre>&lt;code>{01: True}, {10: True}, {11: False}
&lt;/code>&lt;/pre>&lt;p>At present, all &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> set the &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> field to &lt;strong>&lt;code>True&lt;/code>&lt;/strong> if and only if the &lt;strong>&lt;code>NUMANodeAffinity&lt;/code>&lt;/strong> encodes a &lt;em>minimal&lt;/em> set of NUMA nodes that can satisfy the resource request. Normally, this will only be &lt;strong>&lt;code>True&lt;/code>&lt;/strong> for &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> with a single NUMA node set in their bitmask. However, it may also be &lt;strong>&lt;code>True&lt;/code>&lt;/strong> if the only way to &lt;em>ever&lt;/em> satisfy the resource request is to span multiple NUMA nodes (e.g. 2 devices are requested and the only 2 devices on the system are on different NUMA nodes):&lt;/p>
&lt;pre>&lt;code>{0011: True}, {0111: False}, {1011: False}, {1111: False}
&lt;/code>&lt;/pre>&lt;p>&lt;strong>NOTE:&lt;/strong> Setting of the &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> field in this way is &lt;em>not&lt;/em> based on the set of currently available resources. It is based on the ability to physically allocate the number of requested resources on some minimal set of NUMA nodes.&lt;/p>
&lt;p>In this way, it is possible for a &lt;strong>&lt;code>HintProvider&lt;/code>&lt;/strong> to return a list of hints with &lt;em>all&lt;/em> &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> fields set to &lt;strong>&lt;code>False&lt;/code>&lt;/strong> if an actual preferred allocation cannot be satisfied until other containers release their resources. For example, consider the following scenario from the system in Figure 1:&lt;/p>
&lt;ol>
&lt;li>All but 2 CPUs are currently allocated to containers&lt;/li>
&lt;li>The 2 remaining CPUs are on different NUMA nodes&lt;/li>
&lt;li>A new container comes along asking for 2 CPUs&lt;/li>
&lt;/ol>
&lt;p>In this case, the only generated hint would be &lt;strong>&lt;code>{11: False}&lt;/code>&lt;/strong> and not &lt;strong>&lt;code>{11: True}&lt;/code>&lt;/strong>. This happens because it &lt;em>is&lt;/em> possible to allocate 2 CPUs from the same NUMA node on this system (just not right now, given the current allocation state). The idea being that it is better to fail pod admission and retry the deployment when the minimal alignment can be satisfied than to allow a pod to be scheduled with sub-optimal alignment.&lt;/p>
&lt;h3 id="hintproviders">HintProviders&lt;/h3>
&lt;p>A &lt;strong>&lt;code>HintProvider&lt;/code>&lt;/strong> is a component internal to the &lt;strong>&lt;code>kubelet&lt;/code>&lt;/strong> that coordinates aligned resource allocations with the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>. At present, the only &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> in Kubernetes are the &lt;strong>&lt;code>CPUManager&lt;/code>&lt;/strong> and the &lt;strong>&lt;code>DeviceManager&lt;/code>&lt;/strong>. We plan to add support for &lt;strong>&lt;code>HugePages&lt;/code>&lt;/strong> soon.&lt;/p>
&lt;p>As discussed previously, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> both gathers &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> from &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> as well as triggers aligned resource allocations on them using a merged &amp;quot;best&amp;quot; hint. As such, &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> implement the following interface:&lt;/p>
&lt;pre>&lt;code>type HintProvider interface {
GetTopologyHints(*v1.Pod, *v1.Container) map[string][]TopologyHint
Allocate(*v1.Pod, *v1.Container) error
}
&lt;/code>&lt;/pre>&lt;p>Notice that the call to &lt;strong>&lt;code>GetTopologyHints()&lt;/code>&lt;/strong> returns a &lt;strong>&lt;code>map[string][]TopologyHint&lt;/code>&lt;/strong>. This allows a single &lt;strong>&lt;code>HintProvider&lt;/code>&lt;/strong> to provide hints for multiple resource types instead of just one. For example, the &lt;strong>&lt;code>DeviceManager&lt;/code>&lt;/strong> requires this in order to pass hints back for every resource type registered by its plugins.&lt;/p>
&lt;p>As &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> generate their hints, they only consider how alignment could be satisfied for &lt;em>currently&lt;/em> available resources on the system. Any resources already allocated to other containers are not considered.&lt;/p>
&lt;p>For example, consider the system in Figure 1, with the following two containers requesting resources from it:&lt;/p>
&lt;table>
&lt;tr>
&lt;td align="center">&lt;strong>&lt;code>Container0&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>Container1&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;pre>
spec:
containers:
- name: numa-aligned-container0
image: alpine
resources:
limits:
cpu: 2
memory: 200Mi
gpu-vendor.com/gpu: 1
nic-vendor.com/nic: 1
&lt;/pre>
&lt;/td>
&lt;td>
&lt;pre>
spec:
containers:
- name: numa-aligned-container1
image: alpine
resources:
limits:
cpu: 2
memory: 200Mi
gpu-vendor.com/gpu: 1
nic-vendor.com/nic: 1
&lt;/pre>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>If &lt;strong>&lt;code>Container0&lt;/code>&lt;/strong> is the first container considered for allocation on the system, the following set of hints will be generated for the three topology-aware resource types in the spec.&lt;/p>
&lt;pre>&lt;code> cpu: {{01: True}, {10: True}, {11: False}}
gpu-vendor.com/gpu: {{01: True}, {10: True}}
nic-vendor.com/nic: {{01: True}, {10: True}}
&lt;/code>&lt;/pre>&lt;p>With a resulting aligned allocation of:&lt;/p>
&lt;pre>&lt;code>{cpu: {0, 1}, gpu: 0, nic: 0}
&lt;/code>&lt;/pre>&lt;p align="center">
&lt;img height="300" src="https://kubernetes.io/images/blog/2020-03-25-kubernetes-1.18-release-announcement/numa-hint-provider1.png">
&lt;/p>
&lt;p>When considering &lt;strong>&lt;code>Container1&lt;/code>&lt;/strong> these resources are then presumed to be unavailable, and thus only the following set of hints will be generated:&lt;/p>
&lt;pre>&lt;code> cpu: {{01: True}, {10: True}, {11: False}}
gpu-vendor.com/gpu: {{10: True}}
nic-vendor.com/nic: {{10: True}}
&lt;/code>&lt;/pre>&lt;p>With a resulting aligned allocation of:&lt;/p>
&lt;pre>&lt;code>{cpu: {4, 5}, gpu: 1, nic: 1}
&lt;/code>&lt;/pre>&lt;p align="center">
&lt;img height="300" src="https://kubernetes.io/images/blog/2020-03-25-kubernetes-1.18-release-announcement/numa-hint-provider2.png">
&lt;/p>
&lt;p>&lt;strong>NOTE:&lt;/strong> Unlike the pseudocode provided at the beginning of this section, the call to &lt;strong>&lt;code>Allocate()&lt;/code>&lt;/strong> does not actually take a parameter for the merged &amp;quot;best&amp;quot; hint directly. Instead, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> implements the following &lt;strong>&lt;code>Store&lt;/code>&lt;/strong> interface that &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> can query to retrieve the hint generated for a particular container once it has been generated:&lt;/p>
&lt;pre>&lt;code>type Store interface {
GetAffinity(podUID string, containerName string) TopologyHint
}
&lt;/code>&lt;/pre>&lt;p>Separating this out into its own API call allows one to access this hint outside of the pod admission loop. This is useful for debugging as well as for reporting generated hints in tools such as &lt;strong>&lt;code>kubectl&lt;/code>&lt;/strong>(not yet available).&lt;/p>
&lt;h3 id="policy-merge">Policy.Merge&lt;/h3>
&lt;p>The merge strategy defined by a given policy dictates how it combines the set of &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> generated by all &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> into a single &lt;strong>&lt;code>TopologyHint&lt;/code>&lt;/strong> that can be used to inform aligned resource allocations.&lt;/p>
&lt;p>The general merge strategy for all supported policies begins the same:&lt;/p>
&lt;ol>
&lt;li>Take the cross-product of &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> generated for each resource type&lt;/li>
&lt;li>For each entry in the cross-product, &lt;strong>&lt;code>bitwise-and&lt;/code>&lt;/strong> the NUMA affinities of each &lt;strong>&lt;code>TopologyHint&lt;/code>&lt;/strong> together. Set this as the NUMA affinity in a resulting &amp;quot;merged&amp;quot; hint.&lt;/li>
&lt;li>If all of the hints in an entry have &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> set to &lt;strong>&lt;code>True&lt;/code>&lt;/strong> , set &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> to &lt;strong>&lt;code>True&lt;/code>&lt;/strong> in the resulting &amp;quot;merged&amp;quot; hint.&lt;/li>
&lt;li>If even one of the hints in an entry has &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> set to &lt;strong>&lt;code>False&lt;/code>&lt;/strong> , set &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> to &lt;strong>&lt;code>False&lt;/code>&lt;/strong> in the resulting &amp;quot;merged&amp;quot; hint. Also set &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> to &lt;strong>&lt;code>False&lt;/code>&lt;/strong> in the &amp;quot;merged&amp;quot; hint if its NUMA affinity contains all 0s.&lt;/li>
&lt;/ol>
&lt;p>Following the example from the previous section with hints for &lt;strong>&lt;code>Container0&lt;/code>&lt;/strong> generated as:&lt;/p>
&lt;pre>&lt;code> cpu: {{01: True}, {10: True}, {11: False}}
gpu-vendor.com/gpu: {{01: True}, {10: True}}
nic-vendor.com/nic: {{01: True}, {10: True}}
&lt;/code>&lt;/pre>&lt;p>The above algorithm results in the following set of cross-product entries and &amp;quot;merged&amp;quot; hints:&lt;/p>
&lt;table>
&lt;tr>
&lt;td align="center">cross-product entry
&lt;p>
&lt;strong>&lt;code>{cpu, gpu-vendor.com/gpu, nic-vendor.com/nic}&lt;/code>&lt;/strong>
&lt;/p>
&lt;/td>
&lt;td align="center">"merged" hint
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{01: True}, {01: True}, {01: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{01: True}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{01: True}, {01: True}, {10: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{01: True}, {10: True}, {01: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{01: True}, {10: True}, {10: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{10: True}, {01: True}, {01: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{10: True}, {01: True}, {10: True}}&lt;/code>&lt;/strong>
&lt;/td align="center">
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{10: True}, {10: True}, {01: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{10: True}, {10: True}, {10: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{01: True}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{11: False}, {01: True}, {01: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{01: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{11: False}, {01: True}, {10: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{11: False}, {10: True}, {01: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{11: False}, {10: True}, {10: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{10: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>Once this list of &amp;quot;merged&amp;quot; hints has been generated, it is the job of the specific &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> policy in use to decide which one to consider as the &amp;quot;best&amp;quot; hint.&lt;/p>
&lt;p>In general, this involves:&lt;/p>
&lt;ol>
&lt;li>Sorting merged hints by their &amp;quot;narrowness&amp;quot;. Narrowness is defined as the number of bits set in a hint’s NUMA affinity mask. The fewer bits set, the narrower the hint. For hints that have the same number of bits set in their NUMA affinity mask, the hint with the most low order bits set is considered narrower.&lt;/li>
&lt;li>Sorting merged hints by their &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> field. Hints that have &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> set to &lt;strong>&lt;code>True&lt;/code>&lt;/strong> are considered more likely candidates than hints with &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> set to &lt;strong>&lt;code>False&lt;/code>&lt;/strong>.&lt;/li>
&lt;li>Selecting the narrowest hint with the best possible setting for &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong>.&lt;/li>
&lt;/ol>
&lt;p>In the case of the &lt;strong>&lt;code>best-effort&lt;/code>&lt;/strong> policy this algorithm will always result in &lt;em>some&lt;/em> hint being selected as the &amp;quot;best&amp;quot; hint and the pod being admitted. This &amp;quot;best&amp;quot; hint is then made available to &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> so they can make their resource allocations based on it.&lt;/p>
&lt;p>However, in the case of the &lt;strong>&lt;code>restricted&lt;/code>&lt;/strong> and &lt;strong>&lt;code>single-numa-node&lt;/code>&lt;/strong> policies, any selected hint with &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> set to &lt;strong>&lt;code>False&lt;/code>&lt;/strong> will be rejected immediately, causing pod admission to fail and no resources to be allocated. Moreover, the &lt;strong>&lt;code>single-numa-node&lt;/code>&lt;/strong> will also reject a selected hint that has more than one NUMA node set in its affinity mask.&lt;/p>
&lt;p>In the example above, the pod would be admitted by all policies with a hint of &lt;strong>&lt;code>{01: True}&lt;/code>&lt;/strong>.&lt;/p>
&lt;h2 id="upcoming-enhancements">Upcoming enhancements&lt;/h2>
&lt;p>While the 1.18 release and promotion to Beta brings along some great enhancements and fixes, there are still a number of limitations, described &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/topology-manager/#known-limitations">here&lt;/a>. We are already underway working to address these limitations and more.&lt;/p>
&lt;p>This section walks through the set of enhancements we plan to implement for the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> in the near future. This list is not exhaustive, but it gives a good idea of the direction we are moving in. It is ordered by the timeframe in which we expect to see each enhancement completed.&lt;/p>
&lt;p>If you would like to get involved in helping with any of these enhancements, please &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">join the weekly Kubernetes SIG-node meetings&lt;/a> to learn more and become part of the community effort!&lt;/p>
&lt;h3 id="supporting-device-specific-constraints">Supporting device-specific constraints&lt;/h3>
&lt;p>Currently, NUMA affinity is the only constraint considered by the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> for resource alignment. Moreover, the only scalable extensions that can be made to a &lt;strong>&lt;code>TopologyHint&lt;/code>&lt;/strong> involve &lt;em>node-level&lt;/em> constraints, such as PCIe bus alignment across device types. It would be intractable to try and add any &lt;em>device-specific&lt;/em> constraints to this struct (e.g. the internal NVLINK topology among a set of GPU devices).&lt;/p>
&lt;p>As such, we propose an extension to the device plugin interface that will allow a plugin to state its topology-aware allocation preferences, without having to expose any device-specific topology information to the kubelet. In this way, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> can be restricted to only deal with common node-level topology constraints, while still having a way of incorporating device-specific topology constraints into its allocation decisions.&lt;/p>
&lt;p>Details of this proposal can be found &lt;a href="https://github.com/kubernetes/enhancements/pull/1121">here&lt;/a>, and should be available as soon as Kubernetes 1.19.&lt;/p>
&lt;h3 id="numa-alignment-for-hugepages">NUMA alignment for hugepages&lt;/h3>
&lt;p>As stated previously, the only two &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> currently available to the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> are the &lt;strong>&lt;code>CPUManager&lt;/code>&lt;/strong> and the &lt;strong>&lt;code>DeviceManager&lt;/code>&lt;/strong>. However, work is currently underway to add support for hugepages as well. With the completion of this work, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> will finally be able to allocate memory, hugepages, CPUs and PCI devices all on the same NUMA node.&lt;/p>
&lt;p>A &lt;a href="https://github.com/kubernetes/enhancements/blob/253f1e5bdd121872d2d0f7020a5ac0365b229e30/keps/sig-node/20200203-memory-manager.md">KEP&lt;/a> for this work is currently under review, and a prototype is underway to get this feature implemented very soon.&lt;/p>
&lt;h3 id="scheduler-awareness">Scheduler awareness&lt;/h3>
&lt;p>Currently, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> acts as a Pod Admission controller. It is not directly involved in the scheduling decision of where a pod will be placed. Rather, when the kubernetes scheduler (or whatever scheduler is running in the deployment), places a pod on a node to run, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> will decide if the pod should be &amp;quot;admitted&amp;quot; or &amp;quot;rejected&amp;quot;. If the pod is rejected due to lack of available NUMA aligned resources, things can get a little interesting. This kubernetes &lt;a href="https://github.com/kubernetes/kubernetes/issues/84869">issue&lt;/a> highlights and discusses this situation well.&lt;/p>
&lt;p>So how do we go about addressing this limitation? We have the &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/20180409-scheduling-framework.md">Kubernetes Scheduling Framework&lt;/a> to the rescue! This framework provides a new set of plugin APIs that integrate with the existing Kubernetes Scheduler and allow scheduling features, such as NUMA alignment, to be implemented without having to resort to other, perhaps less appealing alternatives, including writing your own scheduler, or even worse, creating a fork to add your own scheduler secret sauce.&lt;/p>
&lt;p>The details of how to implement these extensions for integration with the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> have not yet been worked out. We still need to answer questions like:&lt;/p>
&lt;ul>
&lt;li>Will we require duplicated logic to determine device affinity in the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> and the scheduler?&lt;/li>
&lt;li>Do we need a new API to get &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> from the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> to the scheduler plugin?&lt;/li>
&lt;/ul>
&lt;p>Work on this feature should begin in the next couple of months, so stay tuned!&lt;/p>
&lt;h3 id="per-pod-alignment-policy">Per-pod alignment policy&lt;/h3>
&lt;p>As stated previously, a single policy is applied to &lt;em>all&lt;/em> pods on a node via a global &lt;strong>&lt;code>kubelet&lt;/code>&lt;/strong> flag, rather than allowing users to select different policies on a pod-by-pod basis (or a container-by-container basis).&lt;/p>
&lt;p>While we agree that this would be a great feature to have, there are quite a few hurdles that need to be overcome before it is achievable. The biggest hurdle being that this enhancement will require an API change to be able to express the desired alignment policy in either the Pod spec or its associated &lt;strong>&lt;code>&lt;a href="https://kubernetes.io/docs/concepts/containers/runtime-class/">RuntimeClass&lt;/a>&lt;/code>&lt;/strong>.&lt;/p>
&lt;p>We are only now starting to have serious discussions around this feature, and it is still a few releases away, at the best, from being available.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>With the promotion of the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> to Beta in 1.18, we encourage everyone to give it a try and look forward to any feedback you may have. Many fixes and enhancements have been worked on in the past several releases, greatly improving the functionality and reliability of the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> and its &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong>. While there are still a number of limitations, we have a set of enhancements planned to address them, and look forward to providing you with a number of new features in upcoming releases.&lt;/p>
&lt;p>If you have ideas for additional enhancements or a desire for certain features, don’t hesitate to let us know. The team is always open to suggestions to enhance and improve the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>.&lt;/p>
&lt;p>We hope you have found this blog informative and useful! Let us know if you have any questions or comments. And, happy deploying…..Align Up!&lt;/p>
&lt;!-- Docs to Markdown version 1.0β20 --></description></item><item><title>Blog: Kubernetes 1.18: Fit &amp; Finish</title><link>https://kubernetes.io/blog/2020/03/25/kubernetes-1-18-release-announcement/</link><pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/03/25/kubernetes-1-18-release-announcement/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md">Kubernetes 1.18 Release Team&lt;/a>&lt;/p>
&lt;p>We're pleased to announce the delivery of Kubernetes 1.18, our first release of 2020! Kubernetes 1.18 consists of 38 enhancements: 15 enhancements are moving to stable, 11 enhancements in beta, and 12 enhancements in alpha.&lt;/p>
&lt;p>Kubernetes 1.18 is a &amp;quot;fit and finish&amp;quot; release. Significant work has gone into improving beta and stable features to ensure users have a better experience. An equal effort has gone into adding new developments and exciting new features that promise to enhance the user experience even more.
Having almost as many enhancements in alpha, beta, and stable is a great achievement. It shows the tremendous effort made by the community on improving the reliability of Kubernetes as well as continuing to expand its existing functionality.&lt;/p>
&lt;h2 id="major-themes">Major Themes&lt;/h2>
&lt;h3 id="kubernetes-topology-manager-moves-to-beta-align-up">Kubernetes Topology Manager Moves to Beta - Align Up!&lt;/h3>
&lt;p>A beta feature of Kubernetes in release 1.18, the &lt;a href="https://github.com/nolancon/website/blob/f4200307260ea3234540ef13ed80de325e1a7267/content/en/docs/tasks/administer-cluster/topology-manager.md">Topology Manager feature&lt;/a> enables NUMA alignment of CPU and devices (such as SR-IOV VFs) that will allow your workload to run in an environment optimized for low-latency. Prior to the introduction of the Topology Manager, the CPU and Device Manager would make resource allocation decisions independent of each other. This could result in undesirable allocations on multi-socket systems, causing degraded performance on latency critical applications.&lt;/p>
&lt;h3 id="serverside-apply-introduces-beta-2">Serverside Apply Introduces Beta 2&lt;/h3>
&lt;p>Server-side Apply was promoted to Beta in 1.16, but is now introducing a second Beta in 1.18. This new version will track and manage changes to fields of all new Kubernetes objects, allowing you to know what changed your resources and when.&lt;/p>
&lt;h3 id="extending-ingress-with-and-replacing-a-deprecated-annotation-with-ingressclass">Extending Ingress with and replacing a deprecated annotation with IngressClass&lt;/h3>
&lt;p>In Kubernetes 1.18, there are two significant additions to Ingress: A new &lt;code>pathType&lt;/code> field and a new &lt;code>IngressClass&lt;/code> resource. The &lt;code>pathType&lt;/code> field allows specifying how paths should be matched. In addition to the default &lt;code>ImplementationSpecific&lt;/code> type, there are new &lt;code>Exact&lt;/code> and &lt;code>Prefix&lt;/code> path types.&lt;/p>
&lt;p>The &lt;code>IngressClass&lt;/code> resource is used to describe a type of Ingress within a Kubernetes cluster. Ingresses can specify the class they are associated with by using a new &lt;code>ingressClassName&lt;/code> field on Ingresses. This new resource and field replace the deprecated &lt;code>kubernetes.io/ingress.class&lt;/code> annotation.&lt;/p>
&lt;h3 id="sig-cli-introduces-kubectl-alpha-debug">SIG-CLI introduces kubectl alpha debug&lt;/h3>
&lt;p>SIG-CLI was debating the need for a debug utility for quite some time already. With the development of &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/">ephemeral containers&lt;/a>, it became more obvious how we can support developers with tooling built on top of &lt;code>kubectl exec&lt;/code>. The addition of the &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-cli/20190805-kubectl-debug.md">&lt;code>kubectl alpha debug&lt;/code> command&lt;/a> (it is alpha but your feedback is more than welcome), allows developers to easily debug their Pods inside the cluster. We think this addition is invaluable. This command allows one to create a temporary container which runs next to the Pod one is trying to examine, but also attaches to the console for interactive troubleshooting.&lt;/p>
&lt;h3 id="introducing-windows-csi-support-alpha-for-kubernetes">Introducing Windows CSI support alpha for Kubernetes&lt;/h3>
&lt;p>The alpha version of CSI Proxy for Windows is being released with Kubernetes 1.18. CSI proxy enables CSI Drivers on Windows by allowing containers in Windows to perform privileged storage operations.&lt;/p>
&lt;h2 id="other-updates">Other Updates&lt;/h2>
&lt;h3 id="graduated-to-stable">Graduated to Stable 💯&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/166">Taint Based Eviction&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/491">&lt;code>kubectl diff&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/565">CSI Block storage support&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/576">API Server dry run&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/603">Pass Pod information in CSI calls&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/670">Support Out-of-Tree vSphere Cloud Provider&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/689">Support GMSA for Windows workloads&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/770">Skip attach for non-attachable CSI volumes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/989">PVC cloning&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1020">Moving kubectl package code to staging&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1043">RunAsUserName for Windows&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1507">AppProtocol for Services and Endpoints&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1539">Extending Hugepage Feature&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1601">client-go signature refactor to standardize options and context handling&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1024">Node-local DNS cache&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="major-changes">Major Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/752">EndpointSlice API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1020">Moving kubectl package code to staging&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1513">CertificateSigningRequest API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1539">Extending Hugepage Feature&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1601">client-go signature refactor to standardize options and context handling&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="release-notes">Release Notes&lt;/h3>
&lt;p>Check out the full details of the Kubernetes 1.18 release in our &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md">release notes&lt;/a>.&lt;/p>
&lt;h3 id="availability">Availability&lt;/h3>
&lt;p>Kubernetes 1.18 is available for download on &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.18.0">GitHub&lt;/a>. To get started with Kubernetes, check out these &lt;a href="https://kubernetes.io/docs/tutorials/">interactive tutorials&lt;/a> or run local Kubernetes clusters using Docker container “nodes” with &lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a>. You can also easily install 1.18 using &lt;a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a>.&lt;/p>
&lt;h3 id="release-team">Release Team&lt;/h3>
&lt;p>This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md">release team&lt;/a> led by Jorge Alarcon Ochoa, Site Reliability Engineer at Searchable AI. The 34 release team members coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.&lt;/p>
&lt;p>As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over &lt;a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">40,000 individual contributors&lt;/a> to date and an active community of more than 3,000 people.&lt;/p>
&lt;h3 id="release-logo">Release Logo&lt;/h3>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-03-25-kubernetes-1.18-release-announcement/release-logo.png" alt="Kubernetes 1.18 Release Logo">&lt;/p>
&lt;h4 id="why-the-lhc">Why the LHC?&lt;/h4>
&lt;p>The LHC is the world’s largest and most powerful particle accelerator. It is the result of the collaboration of thousands of scientists from around the world, all for the advancement of science. In a similar manner, Kubernetes has been a project that has united thousands of contributors from hundreds of organizations – all to work towards the same goal of improving cloud computing in all aspects! &amp;quot;A Bit Quarky&amp;quot; as the release name is meant to remind us that unconventional ideas can bring about great change and keeping an open mind to diversity will lead help us innovate.&lt;/p>
&lt;h4 id="about-the-designer">About the designer&lt;/h4>
&lt;p>Maru Lango is a designer currently based in Mexico City. While her area of expertise is Product Design, she also enjoys branding, illustration and visual experiments using CSS + JS and contributing to diversity efforts within the tech and design communities. You may find her in most social media as @marulango or check her website: &lt;a href="https://marulango.com">https://marulango.com&lt;/a>&lt;/p>
&lt;h3 id="user-highlights">User Highlights&lt;/h3>
&lt;ul>
&lt;li>Ericsson is using Kubernetes and other cloud native technology to deliver a &lt;a href="https://www.cncf.io/case-study/ericsson/">highly demanding 5G network&lt;/a> that resulted in up to 90 percent CI/CD savings.&lt;/li>
&lt;li>Zendesk is using Kubernetes to &lt;a href="https://www.cncf.io/case-study/zendesk/">run around 70% of its existing applications&lt;/a>. It’s also building all new applications to also run on Kubernetes, which has brought time savings, greater flexibility, and increased velocity to its application development.&lt;/li>
&lt;li>LifeMiles has &lt;a href="https://www.cncf.io/case-study/lifemiles/">reduced infrastructure spending by 50%&lt;/a> because of its move to Kubernetes. It has also allowed them to double its available resource capacity.&lt;/li>
&lt;/ul>
&lt;h3 id="ecosystem-updates">Ecosystem Updates&lt;/h3>
&lt;ul>
&lt;li>The CNCF published the results of its &lt;a href="https://www.cncf.io/blog/2020/03/04/2019-cncf-survey-results-are-here-deployments-are-growing-in-size-and-speed-as-cloud-native-adoption-becomes-mainstream/">annual survey&lt;/a> showing that Kubernetes usage in production is skyrocketing. The survey found that 78% of respondents are using Kubernetes in production compared to 58% last year.&lt;/li>
&lt;li>The “Introduction to Kubernetes” course hosted by the CNCF &lt;a href="https://www.cncf.io/announcement/2020/01/28/cloud-native-computing-foundation-announces-introduction-to-kubernetes-course-surpasses-100000-registrations/">surpassed 100,000 registrations&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="project-velocity">Project Velocity&lt;/h3>
&lt;p>The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. &lt;a href="https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1">K8s DevStats&lt;/a> illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times.&lt;/p>
&lt;p>This past quarter, 641 different companies and over 6,409 individuals contributed to Kubernetes. &lt;a href="https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;amp;var-period=m&amp;amp;var-repogroup_name=All">Check out DevStats&lt;/a> to learn more about the overall velocity of the Kubernetes project and community.&lt;/p>
&lt;h3 id="event-update">Event Update&lt;/h3>
&lt;p>Kubecon + CloudNativeCon EU 2020 is being pushed back – for the more most up-to-date information, please check the &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/attend/novel-coronavirus-update/">Novel Coronavirus Update page&lt;/a>.&lt;/p>
&lt;h3 id="upcoming-release-webinar">Upcoming Release Webinar&lt;/h3>
&lt;p>Join members of the Kubernetes 1.18 release team on April 23rd, 2020 to learn about the major features in this release including kubectl debug, Topography Manager, Ingress to V1 graduation, and client-go. Register here: &lt;a href="https://www.cncf.io/webinars/kubernetes-1-18/">https://www.cncf.io/webinars/kubernetes-1-18/&lt;/a>.&lt;/p>
&lt;h3 id="get-involved">Get Involved&lt;/h3>
&lt;p>The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Special Interest Groups&lt;/a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href="https://github.com/kubernetes/community/tree/master/communication">community meeting&lt;/a>, and through the channels below. Thank you for your continued feedback and support.&lt;/p>
&lt;ul>
&lt;li>Follow us on Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for latest updates&lt;/li>
&lt;li>Join the community discussion on &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>&lt;/li>
&lt;li>Join the community on &lt;a href="http://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>Post questions (or answer questions) on &lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a>&lt;/li>
&lt;li>Share your Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">story&lt;/a>&lt;/li>
&lt;li>Read more about what’s happening with Kubernetes on the &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>&lt;/li>
&lt;li>Learn more about the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release Team&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Join SIG Scalability and Learn Kubernetes the Hard Way</title><link>https://kubernetes.io/blog/2020/03/19/join-sig-scalability/</link><pubDate>Thu, 19 Mar 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/03/19/join-sig-scalability/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Alex Handy&lt;/p>
&lt;p>Contributing to SIG Scalability is a great way to learn Kubernetes in all its depth and breadth, and the team would love to have you &lt;a href="https://github.com/kubernetes/community/tree/master/sig-scalability#scalability-special-interest-group">join as a contributor&lt;/a>. I took a look at the value of learning the hard way and interviewed the current SIG chairs to give you an idea of what contribution feels like.&lt;/p>
&lt;h2 id="the-value-of-learning-the-hard-way">The value of Learning The Hard Way&lt;/h2>
&lt;p>There is a belief in the software development community that pushes for the most challenging and rigorous possible method of learning a new language or system. These tend to go by the moniker of &amp;quot;Learn __ the Hard Way.&amp;quot; Examples abound: Learn Code the Hard Way, Learn Python the Hard Way, and many others originating with Zed Shaw's courses in the topic.&lt;/p>
&lt;p>While there are folks out there who offer you a &amp;quot;Learn Kubernetes the Hard Way&amp;quot; type experience (most notably &lt;a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kelsey Hightower's&lt;/a>), any &amp;quot;Hard Way&amp;quot; project should attempt to cover every aspect of the core topic's principles.&lt;/p>
&lt;p>Therefore, the real way to &amp;quot;Learn Kubernetes the Hard Way,&amp;quot; is to join the CNCF and get involved in the project itself. And there is only one SIG that could genuinely offer a full-stack learning experience for Kubernetes: SIG Scalability.&lt;/p>
&lt;p>The team behind SIG Scalability is responsible for detecting and dealing with issues that arise when Kubernetes clusters are working with upwards of a thousand nodes. Said &lt;a href="https://github.com/wojtek-t">Wojiciech Tyczynski&lt;/a>, a staff software engineer at Google and a member of SIG Scalability, the standard size for a test cluster for this SIG is over 5,000 nodes.&lt;/p>
&lt;p>And yet, this SIG is not composed of Ph.D.'s in highly scalable systems designs. Many of the folks working with Tyczynski, for example, joined the SIG knowing very little about these types of issues, and often, very little about Kubernetes.&lt;/p>
&lt;p>Working on SIG Scalability is like jumping into the deep end of the pool to learn to swim, and the SIG is inherently concerned with the entire Kubernetes project. SIG Scalability focuses on how Kubernetes functions as a whole and at scale. The SIG Scalability team members have an impetus to learn about every system and to understand how all systems interact with one another.&lt;/p>
&lt;h2 id="a-complex-and-rewarding-contributor-experience">A complex and rewarding contributor experience&lt;/h2>
&lt;p>While that may sound complicated (and it is!), that doesn't mean it's outside the reach of an average developer, tester, or administrator. Google software developer Matt Matejczyk has only been on the team since the beginning of 2019, and he's been a valued member of the team since then, ferreting out bugs.&lt;/p>
&lt;p>&amp;quot;I am new here,&amp;quot; said Matejczyk. &amp;quot;I joined the team in January [2019]. Before that, I worked on AdWords at Google in New York. Why did I join? I knew some people there, so that was one of the decisions for me to move. I thought at that time that Kubernetes is a unique, cutting edge technology. I thought it'd be cool to work on that.&amp;quot;&lt;/p>
&lt;p>Matejczyk was correct about the coolness. &amp;quot;It's cool,&amp;quot; he said. &amp;quot;So actually, ramping up on scalability is not easy. There are many things you need to understand. You need to understand Kubernetes very well. It can use every part of Kubernetes. I am still ramping up after these 8 months. I think it took me maybe 3 months to get up to decent speed.&amp;quot;&lt;/p>
&lt;p>When Matejczyk spoke to what he had worked on during those 8 months, he answered, &amp;quot;An interesting example is a regression I have been working on recently. We noticed the overall slowness of Kubernetes control plane in specific scenarios, and we couldn't attribute it to any particular component. In the end, we realized that everything boiled down to the memory allocation on the golang level. It was very counterintuitive to have two completely separate pieces of code (running as a part of the same binary) affecting the performance of each other only because one of them was allocating memory too fast. But connecting all the dots and getting to the bottom of regression like this gives great satisfaction.&amp;quot;&lt;/p>
&lt;p>Tyczynski said that &amp;quot;It's not only debugging regressions, but it's also debugging and finding bottlenecks. In general, those can be regressions, but those can be things we can improve. The other significant area is extending what we want to guarantee to users. Extending SLA and SLO coverage of the system so users can rely on what they can expect from the system in terms of performance and scalability. Matt is doing much work in extending our tests to be more representative and cover more Kubernetes concepts.&amp;quot;&lt;/p>
&lt;h2 id="give-sig-scalability-a-try">Give SIG Scalability a try&lt;/h2>
&lt;p>The SIG Scalability team is always in need of new members, and if you're the sort of developer or tester who loves taking on new complex challenges, and perhaps loves learning things the hard way, consider joining this SIG. As the team points out, adding Kubernetes expertise to your resume is never a bad idea, and this is the one SIG where you can learn it all from top to bottom.&lt;/p>
&lt;p>See &lt;a href="https://github.com/kubernetes/community/tree/master/sig-scalability#scalability-special-interest-group">the SIG's documentation&lt;/a> to learn about upcoming meetings, its charter, and more. You can also join the &lt;a href="https://kubernetes.slack.com/archives/C09QZTRH7">#sig-scalability Slack channel&lt;/a> to see what it's like. We hope to see you join in to take advantage of this great opportunity to learn Kubernetes and contribute back at the same time.&lt;/p></description></item><item><title>Blog: Kong Ingress Controller and Service Mesh: Setting up Ingress to Istio on Kubernetes</title><link>https://kubernetes.io/blog/2020/03/18/kong-ingress-controller-and-istio-service-mesh/</link><pubDate>Wed, 18 Mar 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/03/18/kong-ingress-controller-and-istio-service-mesh/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Kevin Chen, Kong&lt;/p>
&lt;p>Kubernetes has become the de facto way to orchestrate containers and the services within services. But how do we give services outside our cluster access to what is within? Kubernetes comes with the Ingress API object that manages external access to services within a cluster.&lt;/p>
&lt;p>Ingress is a group of rules that will proxy inbound connections to endpoints defined by a backend. However, Kubernetes does not know what to do with Ingress resources without an Ingress controller, which is where an open source controller can come into play. In this post, we are going to use one option for this: the Kong Ingress Controller. The Kong Ingress Controller was open-sourced a year ago and recently reached one million downloads. In the recent 0.7 release, service mesh support was also added. Other features of this release include:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Built-In Kubernetes Admission Controller&lt;/strong>, which validates Custom Resource Definitions (CRD) as they are created or updated and rejects any invalid configurations.&lt;/li>
&lt;li>&lt;strong>In-memory Mode&lt;/strong> - Each pod’s controller actively configures the Kong container in its pod, which limits the blast radius of failure of a single container of Kong or controller container to that pod only.&lt;/li>
&lt;li>&lt;strong>Native gRPC Routing&lt;/strong> - gRPC traffic can now be routed via Kong Ingress Controller natively with support for method-based routing.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/Kong-Ingress-Controller-and-Service-Mesh/KIC-gRPC.png" alt="K4K-gRPC">&lt;/p>
&lt;p>If you would like a deeper dive into Kong Ingress Controller 0.7, please check out the &lt;a href="https://github.com/Kong/kubernetes-ingress-controller">GitHub repository&lt;/a>.&lt;/p>
&lt;p>But let’s get back to the service mesh support since that will be the main focal point of this blog post. Service mesh allows organizations to address microservices challenges related to security, reliability, and observability by abstracting inter-service communication into a mesh layer. But what if our mesh layer sits within Kubernetes and we still need to expose certain services beyond our cluster? Then you need an Ingress controller such as the Kong Ingress Controller. In this blog post, we’ll cover how to deploy Kong Ingress Controller as your Ingress layer to an Istio mesh. Let’s dive right in:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/Kong-Ingress-Controller-and-Service-Mesh/k4k8s.png" alt="Kong Kubernetes Ingress Controller">&lt;/p>
&lt;h3 id="part-0-set-up-istio-on-kubernetes">Part 0: Set up Istio on Kubernetes&lt;/h3>
&lt;p>This blog will assume you have Istio set up on Kubernetes. If you need to catch up to this point, please check out the &lt;a href="https://istio.io/docs/setup/">Istio documentation&lt;/a>. It will walk you through setting up Istio on Kubernetes.&lt;/p>
&lt;h3 id="1-install-the-bookinfo-application">1. Install the Bookinfo Application&lt;/h3>
&lt;p>First, we need to label the namespaces that will host our application and Kong proxy. To label our default namespace where the bookinfo app sits, run this command:&lt;/p>
&lt;pre>&lt;code>$ kubectl label namespace default istio-injection=enabled
namespace/default labeled
&lt;/code>&lt;/pre>&lt;p>Then create a new namespace that will be hosting our Kong gateway and the Ingress controller:&lt;/p>
&lt;pre>&lt;code>$ kubectl create namespace kong
namespace/kong created
&lt;/code>&lt;/pre>&lt;p>Because Kong will be sitting outside the default namespace, be sure you also label the Kong namespace with istio-injection enabled as well:&lt;/p>
&lt;pre>&lt;code>$ kubectl label namespace kong istio-injection=enabled
namespace/kong labeled
&lt;/code>&lt;/pre>&lt;p>Having both namespaces labeled &lt;code>istio-injection=enabled&lt;/code> is necessary. Or else the default configuration will not inject a sidecar container into the pods of your namespaces.&lt;/p>
&lt;p>Now deploy your BookInfo application with the following command:&lt;/p>
&lt;pre>&lt;code>$ kubectl apply -f http://bit.ly/bookinfoapp
service/details created
serviceaccount/bookinfo-details created
deployment.apps/details-v1 created
service/ratings created
serviceaccount/bookinfo-ratings created
deployment.apps/ratings-v1 created
service/reviews created
serviceaccount/bookinfo-reviews created
deployment.apps/reviews-v1 created
deployment.apps/reviews-v2 created
deployment.apps/reviews-v3 created
service/productpage created
serviceaccount/bookinfo-productpage created
deployment.apps/productpage-v1 created
&lt;/code>&lt;/pre>&lt;p>Let’s double-check our Services and Pods to make sure that we have it all set up correctly:&lt;/p>
&lt;pre>&lt;code>$ kubectl get services
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
details ClusterIP 10.97.125.254 &amp;lt;none&amp;gt; 9080/TCP 29s
kubernetes ClusterIP 10.96.0.1 &amp;lt;none&amp;gt; 443/TCP 29h
productpage ClusterIP 10.97.62.68 &amp;lt;none&amp;gt; 9080/TCP 28s
ratings ClusterIP 10.96.15.180 &amp;lt;none&amp;gt; 9080/TCP 28s
reviews ClusterIP 10.104.207.136 &amp;lt;none&amp;gt; 9080/TCP 28s
&lt;/code>&lt;/pre>&lt;p>You should see four new services: details, productpage, ratings, and reviews. None of them have an external IP so we will use the &lt;a href="https://github.com/Kong/kong">Kong gateway&lt;/a> to expose the necessary services. And to check pods, run the following command:&lt;/p>
&lt;pre>&lt;code>$ kubectl get pods
NAME READY STATUS RESTARTS AGE
details-v1-c5b5f496d-9wm29 2/2 Running 0 101s
productpage-v1-7d6cfb7dfd-5mc96 2/2 Running 0 100s
ratings-v1-f745cf57b-hmkwf 2/2 Running 0 101s
reviews-v1-85c474d9b8-kqcpt 2/2 Running 0 101s
reviews-v2-ccffdd984-9jnsj 2/2 Running 0 101s
reviews-v3-98dc67b68-nzw97 2/2 Running 0 101s
&lt;/code>&lt;/pre>&lt;p>This command outputs useful data, so let’s take a second to understand it. If you examine the READY column, each pod has two containers running: the service and an Envoy sidecar injected alongside it. Another thing to highlight is that there are three review pods but only 1 review service. The Envoy sidecar will load balance the traffic to three different review pods that contain different versions, giving us the ability to A/B test our changes. We have one step before we can access the deployed application. We need to add an additional annotation to the &lt;code>productpage&lt;/code> service. To do so, run:&lt;/p>
&lt;pre>&lt;code>$ kubectl annotate service productpage ingress.kubernetes.io/service-upstream=true
service/productpage annotated
&lt;/code>&lt;/pre>&lt;p>Both the API gateway (Kong) and the service mesh (Istio) can handle the load-balancing. Without the additional &lt;code>ingress.kubernetes.io/service-upstream: &amp;quot;true&amp;quot;&lt;/code> annotation, Kong will try to load-balance by selecting its own endpoint/target from the productpage service. This causes Envoy to receive that pod’s IP as the upstream local address, instead of the service’s cluster IP. But we want the service's cluster IP so that Envoy can properly load balance.&lt;/p>
&lt;p>With that added, you should now be able to access your product page!&lt;/p>
&lt;pre>&lt;code>$ kubectl exec -it $(kubectl get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}') -c ratings -- curl productpage:9080/productpage | grep -o &amp;quot;&amp;lt;title&amp;gt;.*&amp;lt;/title&amp;gt;&amp;quot;
&amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;
&lt;/code>&lt;/pre>&lt;h3 id="2-kong-kubernetes-ingress-controller-without-database">2. Kong Kubernetes Ingress Controller Without Database&lt;/h3>
&lt;p>To expose your services to the world, we will deploy Kong as the north-south traffic gateway. &lt;a href="https://github.com/Kong/kong/releases/tag/1.1.2">Kong 1.1&lt;/a> released with declarative configuration and DB-less mode. Declarative configuration allows you to specify the desired system state through a YAML or JSON file instead of a sequence of API calls. Using declarative config provides several key benefits to reduce complexity, increase automation and enhance system performance. And with the Kong Ingress Controller, any Ingress rules you apply to the cluster will automatically be configured on the Kong proxy. Let’s set up the Kong Ingress Controller and the actual Kong proxy first like this:&lt;/p>
&lt;pre>&lt;code>$ kubectl apply -f https://bit.ly/k4k8s
namespace/kong configured
customresourcedefinition.apiextensions.k8s.io/kongconsumers.configuration.konghq.com created
customresourcedefinition.apiextensions.k8s.io/kongcredentials.configuration.konghq.com created
customresourcedefinition.apiextensions.k8s.io/kongingresses.configuration.konghq.com created
customresourcedefinition.apiextensions.k8s.io/kongplugins.configuration.konghq.com created
serviceaccount/kong-serviceaccount created
clusterrole.rbac.authorization.k8s.io/kong-ingress-clusterrole created
clusterrolebinding.rbac.authorization.k8s.io/kong-ingress-clusterrole-nisa-binding created
configmap/kong-server-blocks created
service/kong-proxy created
service/kong-validation-webhook created
deployment.apps/ingress-kong created
&lt;/code>&lt;/pre>&lt;p>To check if the Kong pod is up and running, run:&lt;/p>
&lt;pre>&lt;code>$ kubectl get pods -n kong
NAME READY STATUS RESTARTS AGE
pod/ingress-kong-8b44c9856-9s42v 3/3 Running 0 2m26s
&lt;/code>&lt;/pre>&lt;p>There will be three containers within this pod. The first container is the Kong Gateway that will be the Ingress point to your cluster. The second container is the Ingress controller. It uses Ingress resources and updates the proxy to follow rules defined in the resource. And lastly, the third container is the Envoy proxy injected by Istio. Kong will route traffic through the Envoy sidecar proxy to the appropriate service. To send requests into the cluster via our newly deployed Kong Gateway, setup an environment variable with the a URL based on the IP address at which Kong is accessible.&lt;/p>
&lt;pre>&lt;code>$ export PROXY_URL=&amp;quot;$(minikube service -n kong kong-proxy --url | head -1)&amp;quot;
$ echo $PROXY_URL
http://192.168.99.100:32728
&lt;/code>&lt;/pre>&lt;p>Next, we need to change some configuration so that the side-car Envoy process can route the request correctly based on the host/authority header of the request. Run the following to stop the route from preserving host:&lt;/p>
&lt;pre>&lt;code>$ echo &amp;quot;
apiVersion: configuration.konghq.com/v1
kind: KongIngress
metadata:
name: do-not-preserve-host
route:
preserve_host: false
upstream:
host_header: productpage.default.svc
&amp;quot; | kubectl apply -f -
kongingress.configuration.konghq.com/do-not-preserve-host created
&lt;/code>&lt;/pre>&lt;p>And annotate the existing productpage service to set service-upstream as true:&lt;/p>
&lt;pre>&lt;code>$ kubectl annotate svc productpage Ingress.kubernetes.io/service-upstream=&amp;quot;true&amp;quot;
service/productpage annotated
&lt;/code>&lt;/pre>&lt;p>Now that we have everything set up, we can look at how to use the Ingress resource to help route external traffic to the services within your Istio mesh. We’ll create an Ingress rule that routes all traffic with the path of &lt;code>/&lt;/code> to our productpage service:&lt;/p>
&lt;pre>&lt;code>$ echo &amp;quot;
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
name: productpage
annotations:
configuration.konghq.com: do-not-preserve-host
spec:
rules:
- http:
paths:
- path: /
backend:
serviceName: productpage
servicePort: 9080
&amp;quot; | kubectl apply -f -
ingress.extensions/productpage created
&lt;/code>&lt;/pre>&lt;p>And just like that, the Kong Ingress Controller is able to understand the rules you defined in the Ingress resource and routes it to the productpage service! To view the product page service’s GUI, go to &lt;code>$PROXY_URL/productpage&lt;/code> in your browser. Or to test it in your command line, try:&lt;/p>
&lt;pre>&lt;code>$ curl $PROXY_URL/productpage
&lt;/code>&lt;/pre>&lt;p>That is all I have for this walk-through. If you enjoyed the technologies used in this post, please check out their repositories since they are all open source and would love to have more contributors! Here are their links for your convenience:&lt;/p>
&lt;ul>
&lt;li>Kong: [&lt;a href="https://github.com/Kong/kubernetes-ingress-controller">GitHub&lt;/a>] [&lt;a href="https://twitter.com/thekonginc">Twitter&lt;/a>]&lt;/li>
&lt;li>Kubernetes: [&lt;a href="https://github.com/kubernetes/kubernetes">GitHub&lt;/a>] [&lt;a href="https://twitter.com/kubernetesio">Twitter&lt;/a>]&lt;/li>
&lt;li>Istio: [&lt;a href="https://github.com/istio/istio">GitHub&lt;/a>] [&lt;a href="https://twitter.com/IstioMesh">Twitter&lt;/a>]&lt;/li>
&lt;li>Envoy: [&lt;a href="https://github.com/envoyproxy/envoy">GitHub&lt;/a>] [&lt;a href="https://twitter.com/EnvoyProxy">Twitter&lt;/a>]&lt;/li>
&lt;/ul>
&lt;p>Thank you for following along!&lt;/p></description></item><item><title>Blog: Contributor Summit Amsterdam Postponed</title><link>https://kubernetes.io/blog/2020/03/04/contributor-summit-delayed/</link><pubDate>Wed, 04 Mar 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/03/04/contributor-summit-delayed/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Dawn Foster (VMware), Jorge Castro (VMware)&lt;/p>
&lt;p>The CNCF has announced that &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/attend/novel-coronavirus-update/">KubeCon + CloudNativeCon EU has been delayed&lt;/a> until July/August of 2020. As a result the Contributor Summit planning team is weighing options for how to proceed. Here’s the current plan:&lt;/p>
&lt;ul>
&lt;li>There will be an in-person Contributor Summit as planned when KubeCon + CloudNativeCon is rescheduled.&lt;/li>
&lt;li>We are looking at options for having additional virtual contributor activities in the meantime.&lt;/li>
&lt;/ul>
&lt;p>We will communicate via this blog and the usual communications channels on the final plan. Please bear with us as we adapt when we get more information. Thank you for being patient as the team pivots to bring you a great Contributor Summit!&lt;/p></description></item><item><title>Blog: Bring your ideas to the world with kubectl plugins</title><link>https://kubernetes.io/blog/2020/02/28/bring-your-ideas-to-the-world-with-kubectl-plugins/</link><pubDate>Fri, 28 Feb 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/02/28/bring-your-ideas-to-the-world-with-kubectl-plugins/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Cornelius Weig (TNG Technology Consulting GmbH)&lt;/p>
&lt;p>&lt;code>kubectl&lt;/code> is the most critical tool to interact with Kubernetes and has to address multiple user personas, each with their own needs and opinions.
One way to make &lt;code>kubectl&lt;/code> do what you need is to build new functionality into &lt;code>kubectl&lt;/code>.&lt;/p>
&lt;h2 id="challenges-with-building-commands-into-kubectl">Challenges with building commands into &lt;code>kubectl&lt;/code>&lt;/h2>
&lt;p>However, that's easier said than done. Being such an important cornerstone of
Kubernetes, any meaningful change to &lt;code>kubectl&lt;/code> needs to undergo a Kubernetes
Enhancement Proposal (KEP) where the intended change is discussed beforehand.&lt;/p>
&lt;p>When it comes to implementation, you'll find that &lt;code>kubectl&lt;/code> is an ingenious and
complex piece of engineering. It might take a long time to get used to
the processes and style of the codebase to get done what you want to achieve. Next
comes the review process which may go through several rounds until it meets all
the requirements of the Kubernetes maintainers -- after all, they need to take
over ownership of this feature and maintain it from the day it's merged.&lt;/p>
&lt;p>When everything goes well, you can finally rejoice. Your code will be shipped
with the next Kubernetes release. Well, that could mean you need to wait
another 3 months to ship your idea in &lt;code>kubectl&lt;/code> if you are unlucky.&lt;/p>
&lt;p>So this was the happy path where everything goes well. But there are good
reasons why your new functionality may never make it into &lt;code>kubectl&lt;/code>. For one,
&lt;code>kubectl&lt;/code> has a particular look and feel and violating that style will not be
acceptable by the maintainers. For example, an interactive command that
produces output with colors would be inconsistent with the rest of &lt;code>kubectl&lt;/code>.
Also, when it comes to tools or commands useful only to a minuscule proportion
of users, the maintainers may simply reject your proposal as &lt;code>kubectl&lt;/code> needs to
address common needs.&lt;/p>
&lt;p>But this doesn’t mean you can’t ship your ideas to &lt;code>kubectl&lt;/code> users.&lt;/p>
&lt;h2 id="what-if-you-didn-t-have-to-change-kubectl-to-add-functionality">What if you didn’t have to change &lt;code>kubectl&lt;/code> to add functionality?&lt;/h2>
&lt;p>This is where &lt;code>kubectl&lt;/code> &lt;a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/">plugins&lt;/a> shine.
Since &lt;code>kubectl&lt;/code> v1.12, you can simply
drop executables into your &lt;code>PATH&lt;/code>, which follows the naming pattern
&lt;code>kubectl-myplugin&lt;/code>. Then you can execute this plugin as &lt;code>kubectl myplugin&lt;/code>, and
it will just feel like a normal sub-command of &lt;code>kubectl&lt;/code>.&lt;/p>
&lt;p>Plugins give you the opportunity to try out new experiences like terminal UIs,
colorful output, specialized functionality, or other innovative ideas. You can
go creative, as you’re the owner of your own plugin.&lt;/p>
&lt;p>Further, plugins offer safe experimentation space for commands you’d like to
propose to &lt;code>kubectl&lt;/code>. By pre-releasing as a plugin, you can push your
functionality faster to the end-users and quickly gather feedback. For example,
the &lt;a href="https://github.com/verb/kubectl-debug">kubectl-debug&lt;/a> plugin is proposed
to become a built-in command in &lt;code>kubectl&lt;/code> in a
&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-cli/20190805-kubectl-debug.md">KEP&lt;/a>).
In the meanwhile, the plugin author can ship the functionality and collect
feedback using the plugin mechanism.&lt;/p>
&lt;h2 id="how-to-get-started-with-developing-plugins">How to get started with developing plugins&lt;/h2>
&lt;p>If you already have an idea for a plugin, how do you best make it happen?
First you have to ask yourself if you can implement it as a wrapper around
existing &lt;code>kubectl&lt;/code> functionality. If so, writing the plugin as a shell script
is often the best way forward, because the resulting plugin will be small,
works cross-platform, and has a high level of trust because it is not
compiled.&lt;/p>
&lt;p>On the other hand, if the plugin logic is complex, a general-purpose language
is usually better. The canonical choice here is Go, because you can use the
excellent &lt;code>client-go&lt;/code> library to interact with the Kubernetes API. The Kubernetes
maintained &lt;a href="https://github.com/kubernetes/sample-cli-plugin">sample-cli-plugin&lt;/a>
demonstrates some best practices and can be used as a template for new plugin
projects.&lt;/p>
&lt;p>When the development is done, you just need to ship your plugin to the
Kubernetes users. For the best plugin installation experience and discoverability,
you should consider doing so via the
&lt;a href="https://github.com/kubernetes-sigs/krew">krew&lt;/a> plugin manager. For an in-depth
discussion about the technical details around &lt;code>kubectl&lt;/code> plugins, refer to the
documentation on &lt;a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/">kubernetes.io&lt;/a>.&lt;/p></description></item><item><title>Blog: Contributor Summit Amsterdam Schedule Announced</title><link>https://kubernetes.io/blog/2020/02/18/contributor-summit-amsterdam-schedule-announced/</link><pubDate>Tue, 18 Feb 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/02/18/contributor-summit-amsterdam-schedule-announced/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Jeffrey Sica (Red Hat), Amanda Katona (VMware)&lt;/p>
&lt;p>tl;dr &lt;a href="https://events.linuxfoundation.org/kubernetes-contributor-summit-europe/">Registration is open&lt;/a> and the &lt;a href="https://kcseu2020.sched.com/">schedule is live&lt;/a> so register now and we’ll see you in Amsterdam!&lt;/p>
&lt;h2 id="kubernetes-contributor-summit">Kubernetes Contributor Summit&lt;/h2>
&lt;p>&lt;strong>Sunday, March 29, 2020&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Evening Contributor Celebration:
&lt;a href="https://www.zuid-pool.nl/en/">ZuidPool&lt;/a>&lt;/li>
&lt;li>Address: &lt;a href="https://www.google.com/search?q=KubeCon+Amsterdam+2020&amp;amp;ie=UTF-8&amp;amp;ibp=htl;events&amp;amp;rciv=evn&amp;amp;sa=X&amp;amp;ved=2ahUKEwiZoLvQ0dvnAhVST6wKHScBBZ8Q5bwDMAB6BAgSEAE#">Europaplein 22, 1078 GZ Amsterdam, Netherlands&lt;/a>&lt;/li>
&lt;li>Time: 18:00 - 21:00&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Monday, March 30, 2020&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>All Day Contributor Summit:&lt;/li>
&lt;li>&lt;a href="https://www.rai.nl/en/">Amsterdam RAI&lt;/a>&lt;/li>
&lt;li>Address: &lt;a href="https://www.google.com/search?q=kubecon+amsterdam+2020&amp;amp;oq=kubecon+amste&amp;amp;aqs=chrome.0.35i39j69i57j0l4j69i61l2.3957j1j4&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&amp;amp;ibp=htl;events&amp;amp;rciv=evn&amp;amp;sa=X&amp;amp;ved=2ahUKEwiZoLvQ0dvnAhVST6wKHScBBZ8Q5bwDMAB6BAgSEAE#">Europaplein 24, 1078 GZ Amsterdam, Netherlands&lt;/a>&lt;/li>
&lt;li>Time: 09:00 - 17:00 (Breakfast at 08:00)&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-02-18-Contributor-Summit-Amsterdam-Schedule-Announced/contribsummit.jpg" alt="Contributor Summit">&lt;/p>
&lt;p>Hello everyone and Happy 2020! It’s hard to believe that KubeCon EU 2020 is less than six weeks away, and with that another contributor summit! This year we have the pleasure of being in Amsterdam in early spring, so be sure to pack some warmer clothing. This summit looks to be exciting with a lot of fantastic community-driven content. We received &lt;strong>26&lt;/strong> submissions from the CFP. From that, the events team selected &lt;strong>12&lt;/strong> sessions. Each of the sessions falls into one of four categories:&lt;/p>
&lt;ul>
&lt;li>Community&lt;/li>
&lt;li>Contributor Improvement&lt;/li>
&lt;li>Sustainability&lt;/li>
&lt;li>In-depth Technical&lt;/li>
&lt;/ul>
&lt;p>On top of the presentations, there will be a dedicated Docs Sprint as well as the New Contributor Workshop 101 and 201 Sessions. All told, we will have five separate rooms of content throughout the day on Monday. Please &lt;strong>&lt;a href="https://kcseu2020.sched.com/">see the full schedule&lt;/a>&lt;/strong> to see what sessions you’d be interested in. We hope between the content provided and the inevitable hallway track, everyone has a fun and enriching experience.&lt;/p>
&lt;p>Speaking of fun, the social Sunday night should be a blast! We’re hosting this summit’s social close to the conference center, at &lt;a href="https://www.zuid-pool.nl/en/">ZuidPool&lt;/a>. There will be games, bingo, and unconference sign-up throughout the evening. It should be a relaxed way to kick off the week.&lt;/p>
&lt;p>&lt;a href="https://events.linuxfoundation.org/kubernetes-contributor-summit-europe/">Registration is open&lt;/a>! Space is limited so it’s always a good idea to register early.&lt;/p>
&lt;p>If you have any questions, reach out to the &lt;a href="https://github.com/kubernetes/community/tree/master/events/2020/03-contributor-summit#team">Amsterdam Team&lt;/a> on Slack in the &lt;a href="https://kubernetes.slack.com/archives/C7J893413">#contributor-summit&lt;/a> channel.&lt;/p>
&lt;p>Hope to see you there!&lt;/p></description></item><item><title>Blog: Deploying External OpenStack Cloud Provider with Kubeadm</title><link>https://kubernetes.io/blog/2020/02/07/deploying-external-openstack-cloud-provider-with-kubeadm/</link><pubDate>Fri, 07 Feb 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/02/07/deploying-external-openstack-cloud-provider-with-kubeadm/</guid><description>
&lt;p>This document describes how to install a single control-plane Kubernetes cluster v1.15 with kubeadm on CentOS, and then deploy an external OpenStack cloud provider and Cinder CSI plugin to use Cinder volumes as persistent volumes in Kubernetes.&lt;/p>
&lt;h3 id="preparation-in-openstack">Preparation in OpenStack&lt;/h3>
&lt;p>This cluster runs on OpenStack VMs, so let's create a few things in OpenStack first.&lt;/p>
&lt;ul>
&lt;li>A project/tenant for this Kubernetes cluster&lt;/li>
&lt;li>A user in this project for Kubernetes, to query node information and attach volumes etc&lt;/li>
&lt;li>A private network and subnet&lt;/li>
&lt;li>A router for this private network and connect it to a public network for floating IPs&lt;/li>
&lt;li>A security group for all Kubernetes VMs&lt;/li>
&lt;li>A VM as a control-plane node and a few VMs as worker nodes&lt;/li>
&lt;/ul>
&lt;p>The security group will have the following rules to open ports for Kubernetes.&lt;/p>
&lt;p>&lt;strong>Control-Plane Node&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Protocol&lt;/th>
&lt;th>Port Number&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>6443&lt;/td>
&lt;td>Kubernetes API Server&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>2379-2380&lt;/td>
&lt;td>etcd server client API&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>10250&lt;/td>
&lt;td>Kubelet API&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>10251&lt;/td>
&lt;td>kube-scheduler&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>10252&lt;/td>
&lt;td>kube-controller-manager&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>10255&lt;/td>
&lt;td>Read-only Kubelet API&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Worker Nodes&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Protocol&lt;/th>
&lt;th>Port Number&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>10250&lt;/td>
&lt;td>Kubelet API&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>10255&lt;/td>
&lt;td>Read-only Kubelet API&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>30000-32767&lt;/td>
&lt;td>NodePort Services&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>CNI ports on both control-plane and worker nodes&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Protocol&lt;/th>
&lt;th>Port Number&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>179&lt;/td>
&lt;td>Calico BGP network&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>9099&lt;/td>
&lt;td>Calico felix (health check)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UDP&lt;/td>
&lt;td>8285&lt;/td>
&lt;td>Flannel&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UDP&lt;/td>
&lt;td>8472&lt;/td>
&lt;td>Flannel&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>6781-6784&lt;/td>
&lt;td>Weave Net&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UDP&lt;/td>
&lt;td>6783-6784&lt;/td>
&lt;td>Weave Net&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>CNI specific ports are only required to be opened when that particular CNI plugin is used. In this guide, we will use Weave Net. Only the Weave Net ports (TCP 6781-6784 and UDP 6783-6784), will need to be opened in the security group.&lt;/p>
&lt;p>The control-plane node needs at least 2 cores and 4GB RAM. After the VM is launched, verify its hostname and make sure it is the same as the node name in Nova.
If the hostname is not resolvable, add it to &lt;code>/etc/hosts&lt;/code>.&lt;/p>
&lt;p>For example, if the VM is called master1, and it has an internal IP 192.168.1.4. Add that to &lt;code>/etc/hosts&lt;/code> and set hostname to master1.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#a2f">echo&lt;/span> &lt;span style="color:#b44">&amp;#34;192.168.1.4 master1&amp;#34;&lt;/span> &amp;gt;&amp;gt; /etc/hosts
hostnamectl set-hostname master1
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="install-docker-and-kubernetes">Install Docker and Kubernetes&lt;/h3>
&lt;p>Next, we'll follow the official documents to install docker and Kubernetes using kubeadm.&lt;/p>
&lt;p>Install Docker following the steps from the &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">container runtime&lt;/a> documentation.&lt;/p>
&lt;p>Note that it is a &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cgroup-drivers">best practice to use systemd as the cgroup driver&lt;/a> for Kubernetes.
If you use an internal container registry, add them to the docker config.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#080;font-style:italic"># Install Docker CE&lt;/span>
&lt;span style="color:#080;font-style:italic">## Set up the repository&lt;/span>
&lt;span style="color:#080;font-style:italic">### Install required packages.&lt;/span>
yum install yum-utils device-mapper-persistent-data lvm2
&lt;span style="color:#080;font-style:italic">### Add Docker repository.&lt;/span>
yum-config-manager &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> --add-repo &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> https://download.docker.com/linux/centos/docker-ce.repo
&lt;span style="color:#080;font-style:italic">## Install Docker CE.&lt;/span>
yum update &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> yum install docker-ce-18.06.2.ce
&lt;span style="color:#080;font-style:italic">## Create /etc/docker directory.&lt;/span>
mkdir /etc/docker
&lt;span style="color:#080;font-style:italic"># Configure the Docker daemon&lt;/span>
cat &amp;gt; /etc/docker/daemon.json &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;span style="color:#b44">{
&lt;/span>&lt;span style="color:#b44"> &amp;#34;exec-opts&amp;#34;: [&amp;#34;native.cgroupdriver=systemd&amp;#34;],
&lt;/span>&lt;span style="color:#b44"> &amp;#34;log-driver&amp;#34;: &amp;#34;json-file&amp;#34;,
&lt;/span>&lt;span style="color:#b44"> &amp;#34;log-opts&amp;#34;: {
&lt;/span>&lt;span style="color:#b44"> &amp;#34;max-size&amp;#34;: &amp;#34;100m&amp;#34;
&lt;/span>&lt;span style="color:#b44"> },
&lt;/span>&lt;span style="color:#b44"> &amp;#34;storage-driver&amp;#34;: &amp;#34;overlay2&amp;#34;,
&lt;/span>&lt;span style="color:#b44"> &amp;#34;storage-opts&amp;#34;: [
&lt;/span>&lt;span style="color:#b44"> &amp;#34;overlay2.override_kernel_check=true&amp;#34;
&lt;/span>&lt;span style="color:#b44"> ]
&lt;/span>&lt;span style="color:#b44">}
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
mkdir -p /etc/systemd/system/docker.service.d
&lt;span style="color:#080;font-style:italic"># Restart Docker&lt;/span>
systemctl daemon-reload
systemctl restart docker
systemctl &lt;span style="color:#a2f">enable&lt;/span> docker
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Install kubeadm following the steps from the &lt;a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">Installing Kubeadm&lt;/a> documentation.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">cat &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo
&lt;/span>&lt;span style="color:#b44">[kubernetes]
&lt;/span>&lt;span style="color:#b44">name=Kubernetes
&lt;/span>&lt;span style="color:#b44">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
&lt;/span>&lt;span style="color:#b44">enabled=1
&lt;/span>&lt;span style="color:#b44">gpgcheck=1
&lt;/span>&lt;span style="color:#b44">repo_gpgcheck=1
&lt;/span>&lt;span style="color:#b44">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;span style="color:#080;font-style:italic"># Set SELinux in permissive mode (effectively disabling it)&lt;/span>
&lt;span style="color:#080;font-style:italic"># Caveat: In a production environment you may not want to disable SELinux, please refer to Kubernetes documents about SELinux&lt;/span>
setenforce &lt;span style="color:#666">0&lt;/span>
sed -i &lt;span style="color:#b44">&amp;#39;s/^SELINUX=enforcing$/SELINUX=permissive/&amp;#39;&lt;/span> /etc/selinux/config
yum install -y kubelet kubeadm kubectl --disableexcludes&lt;span style="color:#666">=&lt;/span>kubernetes
systemctl &lt;span style="color:#a2f">enable&lt;/span> --now kubelet
cat &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf
&lt;/span>&lt;span style="color:#b44">net.bridge.bridge-nf-call-ip6tables = 1
&lt;/span>&lt;span style="color:#b44">net.bridge.bridge-nf-call-iptables = 1
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
sysctl --system
&lt;span style="color:#080;font-style:italic"># check if br_netfilter module is loaded&lt;/span>
lsmod | grep br_netfilter
&lt;span style="color:#080;font-style:italic"># if not, load it explicitly with&lt;/span>
modprobe br_netfilter
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The official document about how to create a single control-plane cluster can be found from the &lt;a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">Creating a single control-plane cluster with kubeadm&lt;/a> documentation.&lt;/p>
&lt;p>We'll largely follow that document but also add additional things for the cloud provider.
To make things more clear, we'll use a &lt;code>kubeadm-config.yml&lt;/code> for the control-plane node.
In this config we specify to use an external OpenStack cloud provider, and where to find its config.
We also enable storage API in API server's runtime config so we can use OpenStack volumes as persistent volumes in Kubernetes.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kubeadm.k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>InitConfiguration&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">nodeRegistration&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kubeletExtraArgs&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">cloud-provider&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;external&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>---&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kubeadm.k8s.io/v1beta2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ClusterConfiguration&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kubernetesVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;v1.15.1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiServer&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">extraArgs&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">enable-admission-plugins&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NodeRestriction&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">runtime-config&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;storage.k8s.io/v1=true&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">controllerManager&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">extraArgs&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">external-cloud-volume-plugin&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>openstack&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">extraVolumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;cloud-config&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;/etc/kubernetes/cloud-config&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;/etc/kubernetes/cloud-config&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">readOnly&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">pathType&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>File&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">networking&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">serviceSubnet&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;10.96.0.0/12&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">podSubnet&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;10.224.0.0/16&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">dnsDomain&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;cluster.local&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now we'll create the cloud config, &lt;code>/etc/kubernetes/cloud-config&lt;/code>, for OpenStack.
Note that the tenant here is the one we created for all Kubernetes VMs in the beginning.
All VMs should be launched in this project/tenant.
In addition you need to create a user in this tenant for Kubernetes to do queries.
The ca-file is the CA root certificate for OpenStack's API endpoint, for example &lt;code>https://openstack.cloud:5000/v3&lt;/code>
At the time of writing the cloud provider doesn't allow insecure connections (skip CA check).&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-ini" data-lang="ini">&lt;span style="color:#a2f;font-weight:bold">[Global]&lt;/span>
&lt;span style="color:#b44">region&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">RegionOne&lt;/span>
&lt;span style="color:#b44">username&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">username&lt;/span>
&lt;span style="color:#b44">password&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">password&lt;/span>
&lt;span style="color:#b44">auth-url&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">https://openstack.cloud:5000/v3&lt;/span>
&lt;span style="color:#b44">tenant-id&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">14ba698c0aec4fd6b7dc8c310f664009&lt;/span>
&lt;span style="color:#b44">domain-id&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">default&lt;/span>
&lt;span style="color:#b44">ca-file&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">/etc/kubernetes/ca.pem&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">[LoadBalancer]&lt;/span>
&lt;span style="color:#b44">subnet-id&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">b4a9a292-ea48-4125-9fb2-8be2628cb7a1&lt;/span>
&lt;span style="color:#b44">floating-network-id&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">bc8a590a-5d65-4525-98f3-f7ef29c727d5&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">[BlockStorage]&lt;/span>
&lt;span style="color:#b44">bs-version&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">v2&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">[Networking]&lt;/span>
&lt;span style="color:#b44">public-network-name&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">public&lt;/span>
&lt;span style="color:#b44">ipv6-support-disabled&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">false&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next run kubeadm to initiate the control-plane node&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubeadm init --config&lt;span style="color:#666">=&lt;/span>kubeadm-config.yml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>With the initialization completed, copy admin config to .kube&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell"> mkdir -p &lt;span style="color:#b8860b">$HOME&lt;/span>/.kube
sudo cp -i /etc/kubernetes/admin.conf &lt;span style="color:#b8860b">$HOME&lt;/span>/.kube/config
sudo chown &lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>id -u&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>:&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>id -g&lt;span style="color:#a2f;font-weight:bold">)&lt;/span> &lt;span style="color:#b8860b">$HOME&lt;/span>/.kube/config
&lt;/code>&lt;/pre>&lt;/div>&lt;p>At this stage, the control-plane node is created but not ready. All the nodes have the taint &lt;code>node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule&lt;/code> and are waiting to be initialized by the cloud-controller-manager.&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console"># kubectl describe no master1
Name: master1
Roles: master
......
Taints: node-role.kubernetes.io/master:NoSchedule
node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
node.kubernetes.io/not-ready:NoSchedule
......
&lt;/code>&lt;/pre>&lt;p>Now deploy the OpenStack cloud controller manager into the cluster, following &lt;a href="https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-controller-manager-with-kubeadm.md">using controller manager with kubeadm&lt;/a>.&lt;/p>
&lt;p>Create a secret with the cloud-config for the openstack cloud provider.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl create secret -n kube-system generic cloud-config --from-literal&lt;span style="color:#666">=&lt;/span>cloud.conf&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>cat /etc/kubernetes/cloud-config&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> --dry-run -o yaml &amp;gt; cloud-config-secret.yaml
kubectl apply -f cloud-config-secret.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Get the CA certificate for OpenStack API endpoints and put that into &lt;code>/etc/kubernetes/ca.pem&lt;/code>.&lt;/p>
&lt;p>Create RBAC resources.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl apply -f https://github.com/kubernetes/cloud-provider-openstack/raw/release-1.15/cluster/addons/rbac/cloud-controller-manager-roles.yaml
kubectl apply -f https://github.com/kubernetes/cloud-provider-openstack/raw/release-1.15/cluster/addons/rbac/cloud-controller-manager-role-bindings.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>We'll run the OpenStack cloud controller manager as a DaemonSet rather than a pod.
The manager will only run on the control-plane node, so if there are multiple control-plane nodes, multiple pods will be run for high availability.
Create &lt;code>openstack-cloud-controller-manager-ds.yaml&lt;/code> containing the following manifests, then apply it.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">---&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ServiceAccount&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cloud-controller-manager&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>---&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>apps/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DaemonSet&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>openstack-cloud-controller-manager&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">k8s-app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>openstack-cloud-controller-manager&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">k8s-app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>openstack-cloud-controller-manager&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">updateStrategy&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>RollingUpdate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">k8s-app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>openstack-cloud-controller-manager&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">nodeSelector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">node-role.kubernetes.io/master&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">securityContext&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">runAsUser&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1001&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">tolerations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">key&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>node.cloudprovider.kubernetes.io/uninitialized&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;true&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">effect&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NoSchedule&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">key&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>node-role.kubernetes.io/master&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">effect&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NoSchedule&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">effect&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NoSchedule&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">key&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>node.kubernetes.io/not-ready&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">serviceAccountName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cloud-controller-manager&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>openstack-cloud-controller-manager&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>docker.io/k8scloudprovider/openstack-cloud-controller-manager:v1&lt;span style="color:#666">.15.0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">args&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- /bin/openstack-cloud-controller-manager&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- --v=&lt;span style="color:#666">1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- --cloud-config=$(CLOUD_CONFIG)&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- --cloud-provider=openstack&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- --use-service-account-credentials=&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- --address=&lt;span style="color:#666">127.0.0.1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/kubernetes/pki&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>k8s-certs&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">readOnly&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/ssl/certs&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ca-certs&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">readOnly&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cloud-config-volume&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">readOnly&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/usr/libexec/kubernetes/kubelet-plugins/volume/exec&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>flexvolume-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/kubernetes&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ca-cert&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">readOnly&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>200m&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">env&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>CLOUD_CONFIG&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/config/cloud.conf&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostNetwork&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/usr/libexec/kubernetes/kubelet-plugins/volume/exec&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DirectoryOrCreate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>flexvolume-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/kubernetes/pki&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DirectoryOrCreate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>k8s-certs&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/ssl/certs&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DirectoryOrCreate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ca-certs&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cloud-config-volume&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secret&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secretName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cloud-config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ca-cert&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secret&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secretName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>openstack-ca-cert&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>When the controller manager is running, it will query OpenStack to get information about the nodes and remove the taint. In the node info you'll see the VM's UUID in OpenStack.&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console"># kubectl describe no master1
Name: master1
Roles: master
......
Taints: node-role.kubernetes.io/master:NoSchedule
node.kubernetes.io/not-ready:NoSchedule
......
sage:docker: network plugin is not ready: cni config uninitialized
......
PodCIDR: 10.224.0.0/24
ProviderID: openstack:///548e3c46-2477-4ce2-968b-3de1314560a5
&lt;/code>&lt;/pre>&lt;p>Now install your favourite CNI and the control-plane node will become ready.&lt;/p>
&lt;p>For example, to install Weave Net, run this command:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl apply -f &lt;span style="color:#b44">&amp;#34;https://cloud.weave.works/k8s/net?k8s-version=&lt;/span>&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>kubectl version | base64 | tr -d &lt;span style="color:#b44">&amp;#39;\n&amp;#39;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next we'll set up worker nodes.&lt;/p>
&lt;p>Firstly, install docker and kubeadm in the same way as how they were installed in the control-plane node.
To join them to the cluster we need a token and ca cert hash from the output of control-plane node installation.
If it is expired or lost we can recreate it using these commands.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#080;font-style:italic"># check if token is expired&lt;/span>
kubeadm token list
&lt;span style="color:#080;font-style:italic"># re-create token and show join command&lt;/span>
kubeadm token create --print-join-command
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Create &lt;code>kubeadm-config.yml&lt;/code> for worker nodes with the above token and ca cert hash.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kubeadm.k8s.io/v1beta2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">discovery&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">bootstrapToken&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiServerEndpoint&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">192.168.1.7&lt;/span>:&lt;span style="color:#666">6443&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">token&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>0c0z4p.dnafh6vnmouus569&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">caCertHashes&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;sha256:fcb3e956a6880c05fc9d09714424b827f57a6fdc8afc44497180905946527adf&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>JoinConfiguration&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">nodeRegistration&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kubeletExtraArgs&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">cloud-provider&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;external&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>apiServerEndpoint is the control-plane node, token and caCertHashes can be taken from the join command printed in the output of 'kubeadm token create' command.&lt;/p>
&lt;p>Run kubeadm and the worker nodes will be joined to the cluster.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubeadm join --config kubeadm-config.yml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>At this stage we'll have a working Kubernetes cluster with an external OpenStack cloud provider.
The provider tells Kubernetes about the mapping between Kubernetes nodes and OpenStack VMs.
If Kubernetes wants to attach a persistent volume to a pod, it can find out which OpenStack VM the pod is running on from the mapping, and attach the underlying OpenStack volume to the VM accordingly.&lt;/p>
&lt;h3 id="deploy-cinder-csi">Deploy Cinder CSI&lt;/h3>
&lt;p>The integration with Cinder is provided by an external Cinder CSI plugin, as described in the &lt;a href="https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-cinder-csi-plugin.md">Cinder CSI&lt;/a> documentation.&lt;/p>
&lt;p>We'll perform the following steps to install the Cinder CSI plugin.
Firstly, create a secret with CA certs for OpenStack's API endpoints. It is the same cert file as what we use in cloud provider above.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl create secret -n kube-system generic openstack-ca-cert --from-literal&lt;span style="color:#666">=&lt;/span>ca.pem&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>cat /etc/kubernetes/ca.pem&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> --dry-run -o yaml &amp;gt; openstack-ca-cert.yaml
kubectl apply -f openstack-ca-cert.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then create RBAC resources.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-openstack/release-1.15/manifests/cinder-csi-plugin/cinder-csi-controllerplugin-rbac.yaml
kubectl apply -f https://github.com/kubernetes/cloud-provider-openstack/raw/release-1.15/manifests/cinder-csi-plugin/cinder-csi-nodeplugin-rbac.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The Cinder CSI plugin includes a controller plugin and a node plugin.
The controller communicates with Kubernetes APIs and Cinder APIs to create/attach/detach/delete Cinder volumes. The node plugin in-turn runs on each worker node to bind a storage device (attached volume) to a pod, and unbind it during deletion.
Create &lt;code>cinder-csi-controllerplugin.yaml&lt;/code> and apply it to create csi controller.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Service&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-controller-service&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-controllerplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-controllerplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>dummy&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">12345&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>---&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>StatefulSet&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>apps/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-controllerplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">serviceName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;csi-cinder-controller-service&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">replicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-controllerplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-controllerplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">serviceAccount&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-controller-sa&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-attacher&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>quay.io/k8scsi/csi-attacher:v1&lt;span style="color:#666">.0.1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">args&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--v=5&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--csi-address=$(ADDRESS)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">env&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ADDRESS&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/csi/sockets/pluginproxy/csi.sock&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">imagePullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;IfNotPresent&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>socket-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/csi/sockets/pluginproxy/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-provisioner&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>quay.io/k8scsi/csi-provisioner:v1&lt;span style="color:#666">.0.1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">args&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--provisioner=csi-cinderplugin&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--csi-address=$(ADDRESS)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">env&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ADDRESS&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/csi/sockets/pluginproxy/csi.sock&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">imagePullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;IfNotPresent&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>socket-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/csi/sockets/pluginproxy/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-snapshotter&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>quay.io/k8scsi/csi-snapshotter:v1&lt;span style="color:#666">.0.1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">args&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--connection-timeout=15s&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--csi-address=$(ADDRESS)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">env&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ADDRESS&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/csi/sockets/pluginproxy/csi.sock&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">imagePullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Always&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/csi/sockets/pluginproxy/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>socket-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cinder-csi-plugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>docker.io/k8scloudprovider/cinder-csi-plugin:v1&lt;span style="color:#666">.15.0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">args &lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- /bin/cinder-csi-plugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--v=5&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--nodeid=$(NODE_ID)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--endpoint=$(CSI_ENDPOINT)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--cloud-config=$(CLOUD_CONFIG)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--cluster=$(CLUSTER_NAME)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">env&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NODE_ID&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">valueFrom&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">fieldRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">fieldPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>spec.nodeName&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>CSI_ENDPOINT&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>unix://csi/csi.sock&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>CLOUD_CONFIG&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/config/cloud.conf&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>CLUSTER_NAME&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kubernetes&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">imagePullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;IfNotPresent&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>socket-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/csi&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>secret-cinderplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">readOnly&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/kubernetes&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ca-cert&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">readOnly&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>socket-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/csi/sockets/pluginproxy/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DirectoryOrCreate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>secret-cinderplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secret&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secretName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cloud-config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ca-cert&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secret&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secretName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>openstack-ca-cert&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Create &lt;code>cinder-csi-nodeplugin.yaml&lt;/code> and apply it to create csi node.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DaemonSet&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>apps/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-nodeplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-nodeplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-nodeplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">serviceAccount&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-node-sa&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostNetwork&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>node-driver-registrar&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>quay.io/k8scsi/csi-node-driver-registrar:v1&lt;span style="color:#666">.1.0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">args&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--v=5&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--csi-address=$(ADDRESS)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">lifecycle&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">preStop&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">exec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">command&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;/bin/sh&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;-c&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;rm -rf /registration/cinder.csi.openstack.org /registration/cinder.csi.openstack.org-reg.sock&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">env&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ADDRESS&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/csi/csi.sock&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DRIVER_REG_SOCK_PATH&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>KUBE_NODE_NAME&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">valueFrom&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">fieldRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">fieldPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>spec.nodeName&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">imagePullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;IfNotPresent&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>socket-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/csi&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>registration-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/registration&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cinder-csi-plugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">securityContext&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">privileged&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">capabilities&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">add&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;SYS_ADMIN&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">allowPrivilegeEscalation&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>docker.io/k8scloudprovider/cinder-csi-plugin:v1&lt;span style="color:#666">.15.0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">args &lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- /bin/cinder-csi-plugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--nodeid=$(NODE_ID)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--endpoint=$(CSI_ENDPOINT)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--cloud-config=$(CLOUD_CONFIG)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">env&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NODE_ID&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">valueFrom&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">fieldRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">fieldPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>spec.nodeName&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>CSI_ENDPOINT&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>unix://csi/csi.sock&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>CLOUD_CONFIG&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/config/cloud.conf&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">imagePullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;IfNotPresent&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>socket-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/csi&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pods-mount-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/kubelet/pods&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPropagation&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Bidirectional&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kubelet-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/kubelet&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPropagation&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Bidirectional&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pods-cloud-data&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/cloud/data&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">readOnly&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pods-probe-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/dev&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPropagation&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;HostToContainer&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>secret-cinderplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">readOnly&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/kubernetes&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ca-cert&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">readOnly&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>socket-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/kubelet/plugins/cinder.csi.openstack.org&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DirectoryOrCreate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>registration-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/kubelet/plugins_registry/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Directory&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kubelet-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/kubelet&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Directory&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pods-mount-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/kubelet/pods&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Directory&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pods-cloud-data&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/cloud/data&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Directory&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pods-probe-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/dev&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Directory&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>secret-cinderplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secret&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secretName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cloud-config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ca-cert&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secret&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secretName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>openstack-ca-cert&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>When they are both running, create a storage class for Cinder.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>StorageClass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-sc-cinderplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">provisioner&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinderplugin&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then we can create a PVC with this class.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>myvol&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>1Gi&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">storageClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-sc-cinderplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>When the PVC is created, a Cinder volume is created correspondingly.&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console"># kubectl get pvc
NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE
myvol Bound pvc-14b8bc68-6c4c-4dc6-ad79-4cb29a81faad 1Gi RWO csi-sc-cinderplugin 3s
&lt;/code>&lt;/pre>&lt;p>In OpenStack the volume name will match the Kubernetes persistent volume generated name. In this example it would be: &lt;em>pvc-14b8bc68-6c4c-4dc6-ad79-4cb29a81faad&lt;/em>&lt;/p>
&lt;p>Now we can create a pod with the PVC.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Pod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>web&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>web&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nginx&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>web&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">containerPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">80&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8081&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">protocol&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>TCP&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;/usr/share/nginx/html&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>mypd&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>mypd&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">persistentVolumeClaim&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">claimName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>myvol&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>When the pod is running, the volume will be attached to the pod.
If we go back to OpenStack, we can see the Cinder volume is mounted to the worker node where the pod is running on.&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console"># openstack volume show 6b5f3296-b0eb-40cd-bd4f-2067a0d6287f
+--------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Field | Value |
+--------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| attachments | [{u'server_id': u'1c5e1439-edfa-40ed-91fe-2a0e12bc7eb4', u'attachment_id': u'11a15b30-5c24-41d4-86d9-d92823983a32', u'attached_at': u'2019-07-24T05:02:34.000000', u'host_name': u'compute-6', u'volume_id': u'6b5f3296-b0eb-40cd-bd4f-2067a0d6287f', u'device': u'/dev/vdb', u'id': u'6b5f3296-b0eb-40cd-bd4f-2067a0d6287f'}] |
| availability_zone | nova |
| bootable | false |
| consistencygroup_id | None |
| created_at | 2019-07-24T05:02:18.000000 |
| description | Created by OpenStack Cinder CSI driver |
| encrypted | False |
| id | 6b5f3296-b0eb-40cd-bd4f-2067a0d6287f |
| migration_status | None |
| multiattach | False |
| name | pvc-14b8bc68-6c4c-4dc6-ad79-4cb29a81faad |
| os-vol-host-attr:host | rbd:volumes@rbd#rbd |
| os-vol-mig-status-attr:migstat | None |
| os-vol-mig-status-attr:name_id | None |
| os-vol-tenant-attr:tenant_id | 14ba698c0aec4fd6b7dc8c310f664009 |
| properties | attached_mode='rw', cinder.csi.openstack.org/cluster='kubernetes' |
| replication_status | None |
| size | 1 |
| snapshot_id | None |
| source_volid | None |
| status | in-use |
| type | rbd |
| updated_at | 2019-07-24T05:02:35.000000 |
| user_id | 5f6a7a06f4e3456c890130d56babf591 |
+--------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
&lt;/code>&lt;/pre>&lt;h3 id="summary">Summary&lt;/h3>
&lt;p>In this walk-through, we deployed a Kubernetes cluster on OpenStack VMs and integrated it with OpenStack using an external OpenStack cloud provider. Then on this Kubernetes cluster we deployed Cinder CSI plugin which can create Cinder volumes and expose them in Kubernetes as persistent volumes.&lt;/p></description></item><item><title>Blog: KubeInvaders - Gamified Chaos Engineering Tool for Kubernetes</title><link>https://kubernetes.io/blog/2020/01/22/kubeinvaders-gamified-chaos-engineering-tool-for-kubernetes/</link><pubDate>Wed, 22 Jan 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/01/22/kubeinvaders-gamified-chaos-engineering-tool-for-kubernetes/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong> Eugenio Marzo, Sourcesense&lt;/p>
&lt;p>Some months ago, I released my latest project called KubeInvaders. The
first time I shared it with the community was during an Openshift
Commons Briefing session. Kubenvaders is a Gamified Chaos Engineering
tool for Kubernetes and Openshift and helps test how resilient your
Kubernetes cluster is, in a fun way.&lt;/p>
&lt;p>It is like Space Invaders, but the aliens are pods.&lt;/p>
&lt;p>&lt;img src="https://github.com/lucky-sideburn/KubeInvaders-kubernetes-post/raw/master/img1.png" alt="">&lt;/p>
&lt;p>During my presentation at Codemotion Milan 2019, I started saying &amp;quot;of
course you can do it with few lines of Bash, but it is boring.&amp;quot;&lt;/p>
&lt;p>&lt;img src="https://github.com/lucky-sideburn/KubeInvaders-kubernetes-post/raw/master/img2.png" alt="">&lt;/p>
&lt;p>Using the code above you can kill random pods across a Kubernetes cluster, but I
think it is much more fun with the spaceship of KubeInvaders.&lt;/p>
&lt;p>I published the code at
&lt;a href="https://github.com/lucky-sideburn/KubeInvaders">https://github.com/lucky-sideburn/KubeInvaders&lt;/a>
and there is a little community that is growing gradually. Some people
love to use it for demo sessions killing pods on a big screen.&lt;/p>
&lt;p>&lt;img src="https://github.com/lucky-sideburn/KubeInvaders-kubernetes-post/raw/master/img3.png" alt="">&lt;/p>
&lt;h2 id="how-to-install-kubeinvaders">How to install KubeInvaders&lt;/h2>
&lt;p>I defined multiples modes to install it:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Helm Chart
&lt;a href="https://github.com/lucky-sideburn/KubeInvaders/tree/master/helm-charts/kubeinvaders">https://github.com/lucky-sideburn/KubeInvaders/tree/master/helm-charts/kubeinvaders&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Manual Installation for Openshift using a template
&lt;a href="https://github.com/lucky-sideburn/KubeInvaders#install-kubeinvaders-on-openshift">https://github.com/lucky-sideburn/KubeInvaders#install-kubeinvaders-on-openshift&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Manual Installation for Kubernetes
&lt;a href="https://github.com/lucky-sideburn/KubeInvaders#install-kubeinvaders-on-kubernetes">https://github.com/lucky-sideburn/KubeInvaders#install-kubeinvaders-on-kubernetes&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>The preferred way, of course, is with a Helm chart:&lt;/p>
&lt;pre>&lt;code># Please set target_namespace to set your target namespace!
helm install --set-string target_namespace=&amp;quot;namespace1,namespace2&amp;quot; \
--name kubeinvaders --namespace kubeinvaders ./helm-charts/kubeinvaders
&lt;/code>&lt;/pre>&lt;h2 id="how-to-use-kubeinvaders">How to use KubeInvaders&lt;/h2>
&lt;p>Once it is installed on your cluster you can use the following
functionalities:&lt;/p>
&lt;ul>
&lt;li>Key 'a' — Switch to automatic pilot&lt;/li>
&lt;li>Key 'm' — Switch to manual pilot&lt;/li>
&lt;li>Key 'i' — Show pod's name. Move the ship towards an alien&lt;/li>
&lt;li>Key 'h' — Print help&lt;/li>
&lt;li>Key 'n' — Jump between different namespaces (my favorite feature!)&lt;/li>
&lt;/ul>
&lt;h2 id="tuning-kubeinvaders">Tuning KubeInvaders&lt;/h2>
&lt;p>At Codemotion Milan 2019, my colleagues and I organized a desk with a
game station for playing KubeInvaders. People had to fight with Kubernetes to
win a t-shirt.&lt;/p>
&lt;p>If you have pods that require a few seconds to start, you may lose. It
is possible to set the complexity of the game with these parameters as
environmment variables in the Kubernetes deployment:&lt;/p>
&lt;ul>
&lt;li>ALIENPROXIMITY — Reduce this value to increase the distance between aliens;&lt;/li>
&lt;li>HITSLIMIT — Seconds of CPU time to wait before shooting;&lt;/li>
&lt;li>UPDATETIME — Seconds to wait before updating pod status (you can set also 0.x Es: 0.5);&lt;/li>
&lt;/ul>
&lt;p>The result is a harder game experience against the machine.&lt;/p>
&lt;h2 id="use-cases">Use cases&lt;/h2>
&lt;p>Adopting chaos engineering strategies for your production environment is
really useful, because it is the only way to test if a system supports
unexpected destructive events.&lt;/p>
&lt;p>KubeInvaders is a game — so please do not take it too seriously! — but it demonstrates
some important use cases:&lt;/p>
&lt;ul>
&lt;li>Test how resilient Kubernetes clusters are on unexpected pod deletion&lt;/li>
&lt;li>Collect metrics like pod restart time&lt;/li>
&lt;li>Tune readiness probes&lt;/li>
&lt;/ul>
&lt;h2 id="next-steps">Next steps&lt;/h2>
&lt;p>I want to continue to add some cool features and integrate it into a
Kubernetes dashboard because I am planning to transform it into a
&amp;quot;Gamified Chaos Engineering and Development Tool for Kubernetes&amp;quot;, to help
developer to interact with deployments in a Kubernetes environment. For
example:&lt;/p>
&lt;ul>
&lt;li>Point to the aliens to get pod logs&lt;/li>
&lt;li>Deploy Helm charts by shooting some particular objects&lt;/li>
&lt;li>Read messages stored in a specific label present in a deployment&lt;/li>
&lt;/ul>
&lt;p>Please feel free to contribute to
&lt;a href="https://github.com/lucky-sideburn/KubeInvaders">https://github.com/lucky-sideburn/KubeInvaders&lt;/a>
and stay updated following #kubeinvaders news &lt;a href="https://twitter.com/luckysideburn">on Twitter&lt;/a>.&lt;/p></description></item><item><title>Blog: CSI Ephemeral Inline Volumes</title><link>https://kubernetes.io/blog/2020/01/21/csi-ephemeral-inline-volumes/</link><pubDate>Tue, 21 Jan 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/01/21/csi-ephemeral-inline-volumes/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Patrick Ohly (Intel)&lt;/p>
&lt;p>Typically, volumes provided by an external storage driver in
Kubernetes are &lt;em>persistent&lt;/em>, with a lifecycle that is completely
independent of pods or (as a special case) loosely coupled to the
first pod which uses a volume (&lt;a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode">late binding
mode&lt;/a>).
The mechanism for requesting and defining such volumes in Kubernetes
are &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volume Claim (PVC) and Persistent Volume
(PV)&lt;/a>
objects. Originally, volumes that are backed by a Container Storage Interface
(CSI) driver could only be used via this PVC/PV mechanism.&lt;/p>
&lt;p>But there are also use cases for data volumes whose content and
lifecycle is tied to a pod. For example, a driver might populate a
volume with dynamically created secrets that are specific to the
application running in the pod. Such volumes need to be created
together with a pod and can be deleted as part of pod termination
(&lt;em>ephemeral&lt;/em>). They get defined as part of the pod spec (&lt;em>inline&lt;/em>).&lt;/p>
&lt;p>Since Kubernetes 1.15, CSI drivers can also be used for such
&lt;em>ephemeral inline&lt;/em> volumes. The &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">CSIInlineVolume feature
gate&lt;/a>
had to be set to enable it in 1.15 because support was still in alpha
state. In 1.16, the feature reached beta state, which typically means
that it is enabled in clusters by default.&lt;/p>
&lt;p>CSI drivers have to be adapted to support this because although two
existing CSI gRPC calls are used (&lt;code>NodePublishVolume&lt;/code> and &lt;code>NodeUnpublishVolume&lt;/code>),
the way how they are
used is different and not covered by the CSI spec: for ephemeral
volumes, only &lt;code>NodePublishVolume&lt;/code> is invoked by &lt;code>kubelet&lt;/code> when asking
the CSI driver for a volume. All other calls
(like &lt;code>CreateVolume&lt;/code>, &lt;code>NodeStageVolume&lt;/code>, etc.) are skipped. The volume
parameters are provided in the pod spec and from there copied into the
&lt;code>NodePublishVolumeRequest.volume_context&lt;/code> field. There are currently
no standardized parameters; even common ones like size must be
provided in a format that is defined by the CSI driver. Likewise, only
&lt;code>NodeUnpublishVolume&lt;/code> gets called after the pod has terminated and the
volume needs to be removed.&lt;/p>
&lt;p>Initially, the assumption was that CSI drivers would be specifically
written to provide either persistent or ephemeral volumes. But there
are also drivers which provide storage that is useful in both modes:
for example, &lt;a href="https://github.com/intel/pmem-csi">PMEM-CSI&lt;/a> manages
persistent memory (PMEM), a new kind of local storage that is provided
by &lt;a href="https://www.intel.com/content/www/us/en/architecture-and-technology/optane-dc-persistent-memory.html">Intel® Optane™ DC Persistent
Memory&lt;/a>. Such
memory is useful both as persistent data storage (faster than normal SSDs)
and as ephemeral scratch space (higher capacity than DRAM).&lt;/p>
&lt;p>Therefore the support in Kubernetes 1.16 was extended:&lt;/p>
&lt;ul>
&lt;li>Kubernetes and users can determine which kind of volumes a driver
supports via the &lt;code>volumeLifecycleModes&lt;/code> field in the &lt;a href="https://kubernetes-csi.github.io/docs/csi-driver-object.html#what-fields-does-the-csidriver-object-have">&lt;code>CSIDriver&lt;/code>
object&lt;/a>.&lt;/li>
&lt;li>Drivers can get information about the volume mode by enabling the
&lt;a href="https://kubernetes-csi.github.io/docs/pod-info.html">&amp;quot;pod info on
mount&amp;quot;&lt;/a> feature
which then will add the new &lt;code>csi.storage.k8s.io/ephemeral&lt;/code> entry to
the &lt;code>NodePublishRequest.volume_context&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>For more information about implementing support of ephemeral inline
volumes in a CSI driver, see the &lt;a href="https://kubernetes-csi.github.io/docs/ephemeral-local-volumes.html">Kubernetes-CSI
documentation&lt;/a>
and the &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/20190122-csi-inline-volumes.md">original design
document&lt;/a>.&lt;/p>
&lt;p>What follows in this blog post are usage examples based on real drivers
and a summary at the end.&lt;/p>
&lt;h1 id="examples">Examples&lt;/h1>
&lt;h2 id="pmem-csi-https-github-com-intel-pmem-csi">&lt;a href="https://github.com/intel/pmem-csi">PMEM-CSI&lt;/a>&lt;/h2>
&lt;p>Support for ephemeral inline volumes was added in &lt;a href="https://github.com/intel/pmem-csi/releases/tag/v0.6.0">release
v0.6.0&lt;/a>. The
driver can be used on hosts with real Intel® Optane™ DC Persistent
Memory, on &lt;a href="https://github.com/intel/pmem-csi/blob/v0.6.0/examples/gce.md">special machines in
GCE&lt;/a> or
with hardware emulated by QEMU. The latter is fully &lt;a href="https://github.com/intel/pmem-csi/tree/v0.6.0#qemu-and-kubernetes">integrated into
the
makefile&lt;/a>
and only needs Go, Docker and KVM, so that approach was used for this
example:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">git clone --branch release-0.6 https://github.com/intel/pmem-csi
&lt;span style="color:#a2f">cd&lt;/span> pmem-csi
&lt;span style="color:#b8860b">TEST_DISTRO&lt;/span>&lt;span style="color:#666">=&lt;/span>clear &lt;span style="color:#b8860b">TEST_DISTRO_VERSION&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">32080&lt;/span> &lt;span style="color:#b8860b">TEST_PMEM_REGISTRY&lt;/span>&lt;span style="color:#666">=&lt;/span>intel make start
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Bringing up the four-node cluster can take a while but eventually should end with:&lt;/p>
&lt;pre>&lt;code>The test cluster is ready. Log in with /work/pmem-csi/_work/pmem-govm/ssh-pmem-govm, run kubectl once logged in.
Alternatively, KUBECONFIG=/work/pmem-csi/_work/pmem-govm/kube.config can also be used directly.
To try out the pmem-csi driver persistent volumes:
...
To try out the pmem-csi driver ephemeral volumes:
cat deploy/kubernetes-1.17/pmem-app-ephemeral.yaml | /work/pmem-csi/_work/pmem-govm/ssh-pmem-govm kubectl create -f -
&lt;/code>&lt;/pre>&lt;p>&lt;code>deploy/kubernetes-1.17/pmem-app-ephemeral.yaml&lt;/code> specifies one volume:&lt;/p>
&lt;pre>&lt;code>kind: Pod
apiVersion: v1
metadata:
name: my-csi-app-inline-volume
spec:
containers:
- name: my-frontend
image: busybox
command: [ &amp;quot;sleep&amp;quot;, &amp;quot;100000&amp;quot; ]
volumeMounts:
- mountPath: &amp;quot;/data&amp;quot;
name: my-csi-volume
volumes:
- name: my-csi-volume
csi:
driver: pmem-csi.intel.com
fsType: &amp;quot;xfs&amp;quot;
volumeAttributes:
size: &amp;quot;2Gi&amp;quot;
nsmode: &amp;quot;fsdax&amp;quot;
&lt;/code>&lt;/pre>&lt;p>Once we have created that pod, we can inspect the result:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl describe pods/my-csi-app-inline-volume
&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>Name: my-csi-app-inline-volume
...
Volumes:
my-csi-volume:
Type: CSI (a Container Storage Interface (CSI) volume source)
Driver: pmem-csi.intel.com
FSType: xfs
ReadOnly: false
VolumeAttributes: nsmode=fsdax
size=2Gi
&lt;/code>&lt;/pre>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl &lt;span style="color:#a2f">exec&lt;/span> my-csi-app-inline-volume -- df -h /data
&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>Filesystem Size Used Available Use% Mounted on
/dev/ndbus0region0fsdax/d7eb073f2ab1937b88531fce28e19aa385e93696
1.9G 34.2M 1.8G 2% /data
&lt;/code>&lt;/pre>&lt;h2 id="image-populator-https-github-com-kubernetes-csi-csi-driver-image-populator">&lt;a href="https://github.com/kubernetes-csi/csi-driver-image-populator">Image Populator&lt;/a>&lt;/h2>
&lt;p>The image populator automatically unpacks a container image and makes
its content available as an ephemeral volume. It's still in
development, but canary images are already available which can be
installed with:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl create -f https://github.com/kubernetes-csi/csi-driver-image-populator/raw/master/deploy/kubernetes-1.16/csi-image-csidriverinfo.yaml
kubectl create -f https://github.com/kubernetes-csi/csi-driver-image-populator/raw/master/deploy/kubernetes-1.16/csi-image-daemonset.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This example pod will run nginx and have it serve data that
comes from the &lt;code>kfox1111/misc:test&lt;/code> image:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl create -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;span style="color:#b44">apiVersion: v1
&lt;/span>&lt;span style="color:#b44">kind: Pod
&lt;/span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;span style="color:#b44"> name: nginx
&lt;/span>&lt;span style="color:#b44">spec:
&lt;/span>&lt;span style="color:#b44"> containers:
&lt;/span>&lt;span style="color:#b44"> - name: nginx
&lt;/span>&lt;span style="color:#b44"> image: nginx:1.16-alpine
&lt;/span>&lt;span style="color:#b44"> ports:
&lt;/span>&lt;span style="color:#b44"> - containerPort: 80
&lt;/span>&lt;span style="color:#b44"> volumeMounts:
&lt;/span>&lt;span style="color:#b44"> - name: data
&lt;/span>&lt;span style="color:#b44"> mountPath: /usr/share/nginx/html
&lt;/span>&lt;span style="color:#b44"> volumes:
&lt;/span>&lt;span style="color:#b44"> - name: data
&lt;/span>&lt;span style="color:#b44"> csi:
&lt;/span>&lt;span style="color:#b44"> driver: image.csi.k8s.io
&lt;/span>&lt;span style="color:#b44"> volumeAttributes:
&lt;/span>&lt;span style="color:#b44"> image: kfox1111/misc:test
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl &lt;span style="color:#a2f">exec&lt;/span> nginx -- cat /usr/share/nginx/html/test
&lt;/code>&lt;/pre>&lt;/div>&lt;p>That &lt;code>test&lt;/code> file just contains a single word:&lt;/p>
&lt;pre>&lt;code>testing
&lt;/code>&lt;/pre>&lt;p>Such data containers can be built with Dockerfiles such as:&lt;/p>
&lt;pre>&lt;code>FROM scratch
COPY index.html /index.html
&lt;/code>&lt;/pre>&lt;h2 id="cert-manager-csi-https-github-com-jetstack-cert-manager-csi">&lt;a href="https://github.com/jetstack/cert-manager-csi">cert-manager-csi&lt;/a>&lt;/h2>
&lt;p>cert-manager-csi works together with
&lt;a href="https://github.com/jetstack/cert-manager">cert-manager&lt;/a>. The goal for
this driver is to facilitate requesting and mounting certificate key
pairs to pods seamlessly. This is useful for facilitating mTLS, or
otherwise securing connections of pods with guaranteed present
certificates whilst having all of the features that cert-manager
provides. This project is experimental.&lt;/p>
&lt;h1 id="next-steps">Next steps&lt;/h1>
&lt;p>One of the issues with ephemeral inline volumes is that pods get
scheduled by Kubernetes onto nodes without knowing anything about the
currently available storage on that node. Once the pod has been
scheduled, the CSI driver must make the volume available one that
node. If that is currently not possible, the pod cannot start. This
will be retried until eventually the volume becomes ready. The
&lt;a href="https://github.com/kubernetes/enhancements/pull/1353">storage capacity tracking
KEP&lt;/a> is an
attempt to address this problem.&lt;/p>
&lt;p>A related KEP introduces a &lt;a href="https://github.com/kubernetes/enhancements/pull/1409">standardized size
parameter&lt;/a>.&lt;/p>
&lt;p>Currently, CSI ephemeral inline volumes stay in beta while issues like
these are getting discussed. Your feedback is needed to decide how to
proceed with this feature. For the KEPs, the two PRs linked to above
is a good place to comment. The SIG Storage also &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#meetings">meets
regularly&lt;/a>
and can be reached via &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#contact">Slack and a mailing
list&lt;/a>.&lt;/p></description></item><item><title>Blog: Reviewing 2019 in Docs</title><link>https://kubernetes.io/blog/2020/01/21/reviewing-2019-in-docs/</link><pubDate>Tue, 21 Jan 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/01/21/reviewing-2019-in-docs/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Zach Corleissen (Cloud Native Computing Foundation)&lt;/p>
&lt;p>Hi, folks! I'm one of the co-chairs for the Kubernetes documentation special interest group (SIG Docs). This blog post is a review of SIG Docs in 2019. Our contributors did amazing work last year, and I want to highlight their successes.&lt;/p>
&lt;p>Although I review 2019 in this post, my goal is to point forward to 2020. I observe some trends in SIG Docs–some good, others troubling. I want to raise visibility before those challenges increase in severity.&lt;/p>
&lt;h2 id="the-good">The good&lt;/h2>
&lt;p>There was much to celebrate in SIG Docs in 2019.&lt;/p>
&lt;p>Kubernetes docs started the year with three localizations in progress. By the end of the year, we ended with ten localizations available, four of which (Chinese, French, Japanese, Korean) are reasonably complete. The Korean and French teams deserve special mentions for their contributions to git best practices across all localizations (Korean team) and help bootstrapping other localizations (French team).&lt;/p>
&lt;p>Despite significant transition over the year, SIG Docs &lt;a href="https://k8s.devstats.cncf.io/d/44/pr-time-to-approve-and-merge?orgId=1&amp;amp;var-period=w&amp;amp;var-repogroup_name=SIG%20Docs&amp;amp;var-apichange=All&amp;amp;var-size_name=All&amp;amp;var-kind_name=All">improved its review velocity&lt;/a>, with a median review time from PR open to merge of just over 24 hours.&lt;/p>
&lt;p>Issue triage improved significantly in both volume and speed, largely due to the efforts of GitHub users @sftim, @tengqm, and @kbhawkey.&lt;/p>
&lt;p>Doc sprints remain valuable at KubeCon contributor days, introducing new contributors to Kubernetes documentation.&lt;/p>
&lt;p>The docs component of Kubernetes quarterly releases improved over 2019, thanks to iterative playbook improvements from release leads and their teams.&lt;/p>
&lt;p>Site traffic increased over the year. The website ended the year with ~6 million page views per month in December, up from ~5M page views in January. The kubernetes.io website had 851k site visitors in October, a new all-time high. Reader satisfaction &lt;a href="https://kubernetes.io/blog/2019/10/29/kubernetes-documentation-end-user-survey/">remains general&lt;/a>.&lt;/p>
&lt;p>We onboarded a new SIG chair: @jimangel, a Cloud Architect at General Motors. Jim was a docs contributor for a year, during which he led the 1.14 docs release, before stepping up as chair.&lt;/p>
&lt;h2 id="the-not-so-good">The not so good&lt;/h2>
&lt;p>While reader satisfaction is decent, &lt;strong>most respondents indicated dissatisfaction with stale content&lt;/strong> in every area: concepts, tasks, tutorials, and reference. Additionally, readers requested more diagrams, advanced conceptual content, and code samples—things that technical writers excel at providing.&lt;/p>
&lt;p>SIG Docs continues to solve how best to handle &lt;a href="https://github.com/kubernetes/enhancements/pull/1327">third-party content&lt;/a>. &lt;strong>There's too much vendor content on kubernetes.io&lt;/strong>, and guidelines for adding or rejecting third-party content remain unclear. The discussion so far has been powerful, including pushback demanding greater collaborative input—a powerful reminder that Kubernetes is in all ways a communal effort.&lt;/p>
&lt;p>We're in the middle of our third chair transition in 18 months. Each chair transition has been healthy and collegial, but it's still a lot of turnover in a short time. Chairing any open source project is difficult, but especially so with SIG Docs. Chairship of SIG Docs requires a steep learning curve across multiple domains: docs (both written and generated from spec), information architecture, specialized contribution paths (for example, localization), how to run a release cycle, website development, CI/CD, community management, on and on. It's a role that requires multiple people to function successfully without burning people out. Training replacements is time-intensive.&lt;/p>
&lt;p>Perhaps most pressing in the Not So Good category is that SIG Docs currently has only one technical writer dedicated full-time to Kubernetes docs. This has impacts on Kubernetes docs: some obvious, some less so.&lt;/p>
&lt;h2 id="impacts-of-understaffing-on-kubernetes-docs">Impacts of understaffing on Kubernetes docs&lt;/h2>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">Me today: &lt;a href="https://t.co/cDpHOWEsjf">pic.twitter.com/cDpHOWEsjf&lt;/a>&lt;/p>&amp;mdash; Benjamin Elder (@BenTheElder) &lt;a href="https://twitter.com/BenTheElder/status/1215453579651104768?ref_src=twsrc%5Etfw">January 10, 2020&lt;/a>&lt;/blockquote> &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>If Kubernetes continues through 2020 without more technical writers dedicated to the docs, here's what I see as the most likely possibilities.&lt;/p>
&lt;h3 id="but-first-a-disclaimer">But first, a disclaimer&lt;/h3>
&lt;blockquote class="caution">
&lt;div>&lt;strong>Caution:&lt;/strong> It is very hard to predict, especially the future.
-Niels Bohr&lt;/div>
&lt;/blockquote>
&lt;p>Some of my predictions are almost certainly wrong. Any errors are mine alone.&lt;/p>
&lt;p>That said...&lt;/p>
&lt;h3 id="effects-in-2020">Effects in 2020&lt;/h3>
&lt;p>Current levels of function aren't self-sustaining. Even with a strong playbook, the release cycle still requires expert support from at least one (and usually two) chairs during every cycle. Without fail, each release breaks in new and unexpected ways, and it requires familiarity and expertise to diagnose and resolve. As chairs continue to cycle—and to be clear, regular transitions are part of a healthy project—we accrue the risks associated with a pool lacking sufficient professional depth and employer support.&lt;/p>
&lt;p>Oddly enough, one of the challenges to staffing is that the docs appear good enough. Based on site analytics and survey responses, readers are pleased with the quality of the docs. When folks visit the site, they generally find what they need and behave like satisfied visitors.&lt;/p>
&lt;p>The danger is that this will change over time: slowly with occasional losses of function, annoying at first, then increasingly critical. The more time passes without adequate staffing, the more difficult and costly fixes will become.&lt;/p>
&lt;p>I suspect this is true because the challenges we face now at decent levels of reader satisfaction are already difficult to fix. API reference generation is complex and brittle; the site's UI is outdated; and our most consistent requests are for more tutorials, advanced concepts, diagrams, and code samples, all of which require ongoing, dedicated time to create.&lt;/p>
&lt;p>&lt;strong>Release support remains strong.&lt;/strong>&lt;/p>
&lt;p>The release team continues a solid habit of leaving each successive team with better support than the previous release. This mostly takes the form of iterative improvements to the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-release#docs-lead">docs release playbook&lt;/a>, producing better documentation and reducing siloed knowledge.&lt;/p>
&lt;p>&lt;strong>Staleness accelerates.&lt;/strong>&lt;/p>
&lt;p>Conceptual content becomes less accurate or relevant as features change or deprecate. Tutorial content degrades for the same reason.&lt;/p>
&lt;p>The content structure will also degrade: the categories of concepts, tasks, and tutorials are legacy categories that may not best fit the needs of current readers, let alone future ones.&lt;/p>
&lt;p>Cruft accumulates for both readers and contributors. Reference docs become increasingly brittle without intervention.&lt;/p>
&lt;p>&lt;strong>Critical knowledge vanishes.&lt;/strong>&lt;/p>
&lt;p>As I mentioned previously, SIG Docs has a wide range of functions, some with a steep learning curve. As contributors change roles or jobs, their expertise and availability will diminish or reduce to zero. Contributors with specific knowledge may not be available for consultation, exposing critical vulnerabilities in docs function. Specific examples include reference generation and chair leadership.&lt;/p>
&lt;h3 id="that-s-a-lot-to-take-in">That's a lot to take in&lt;/h3>
&lt;p>It's difficult to strike a balance between the importance of SIG Docs' work to the community and our users, the joy it brings me personally, and the fact that things can't remain as they are without significant negative impacts (eventually). SIG Docs is by no means dying; it's a vibrant community with active contributors doing cool things. It's also a community with some critical knowledge and capacity shortages that can only be remedied with trained, paid staff dedicated to documentation.&lt;/p>
&lt;h2 id="what-the-community-can-do-for-healthy-docs">What the community can do for healthy docs&lt;/h2>
&lt;p>Hire technical writers dedicated to Kubernetes docs. Support advanced content creation, not just release docs and incremental feature updates.&lt;/p>
&lt;p>Thanks, and Happy 2020.&lt;/p></description></item><item><title>Blog: Kubernetes on MIPS</title><link>https://kubernetes.io/blog/2020/01/15/kubernetes-on-mips/</link><pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/01/15/kubernetes-on-mips/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> TimYin Shi, Dominic Yin, Wang Zhan, Jessica Jiang, Will Cai, Jeffrey Gao, Simon Sun (Inspur)&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/MIPS_architecture">MIPS&lt;/a> (Microprocessor without Interlocked Pipelined Stages) is a reduced instruction set computer (RISC) instruction set architecture (ISA), appeared in 1981 and developed by MIPS Technologies. Now MIPS architecture is widely used in many electronic products.&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io">Kubernetes&lt;/a> has officially supported a variety of CPU architectures such as x86, arm/arm64, ppc64le, s390x. However, it's a pity that Kubernetes doesn't support MIPS. With the widespread use of cloud native technology, users under MIPS architecture also have an urgent demand for Kubernetes on MIPS.&lt;/p>
&lt;h2 id="achievements">Achievements&lt;/h2>
&lt;p>For many years, to enrich the ecology of the open-source community, we have been working on adjusting MIPS architecture for Kubernetes use cases. With the continuous iterative optimization and the performance improvement of the MIPS CPU, we have made some breakthrough progresses on the mips64el platform.&lt;/p>
&lt;p>Over the years, we have been actively participating in the Kubernetes community and have rich experience in the using and optimization of Kubernetes technology. Recently, we tried to adapt the MIPS architecture platform for Kubernetes and achieved a new a stage on that journey. The team has completed migration and adaptation of Kubernetes and related components, built not only a stable and highly available MIPS cluster but also completed the conformance test for Kubernetes v1.16.2.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/kubernetes-on-mips.png" alt="Kubernetes on MIPS">&lt;/p>
&lt;p>&lt;em>Figure 1 Kubernetes on MIPS&lt;/em>&lt;/p>
&lt;h2 id="k8s-mips-component-build">K8S-MIPS component build&lt;/h2>
&lt;p>Almost all native cloud components related to Kubernetes do not provide a MIPS version installation package or image. The prerequisite of deploying Kubernetes on the MIPS platform is to compile and build all required components on the mips64el platform. These components include:&lt;/p>
&lt;ul>
&lt;li>golang&lt;/li>
&lt;li>docker-ce&lt;/li>
&lt;li>hyperkube&lt;/li>
&lt;li>pause&lt;/li>
&lt;li>etcd&lt;/li>
&lt;li>calico&lt;/li>
&lt;li>coredns&lt;/li>
&lt;li>metrics-server&lt;/li>
&lt;/ul>
&lt;p>Thanks to the excellent design of Golang and its good support for the MIPS platform, the compilation processes of the above cloud native components are greatly simplified. First of all, we compiled Golang on the latest stable version for the mips64el platform, and then we compiled most of the above components with source code.&lt;/p>
&lt;p>During the compilation processes, we inevitably encountered many platform compatibility problems, such as a Golang system call compatibility problem (syscall), typecasting of syscall. Stat_t from uint32 to uint64, patching for EpollEvent, and so on.&lt;/p>
&lt;p>To build K8S-MIPS components, we used cross-compilation technology. Our process involved integrating a QEMU tool to translate MIPS CPU instructions and modifying the build script of Kubernetes and E2E image script of Kubernetes, Hyperkube, and E2E test images on MIPS architecture.&lt;/p>
&lt;p>After successfully building the above components, we use tools such as kubespray and kubeadm to complete kubernetes cluster construction.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Version&lt;/th>
&lt;th>MIPS Repository&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>golang on MIPS&lt;/td>
&lt;td>1.12.5&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>docker-ce on MIPS&lt;/td>
&lt;td>18.09.8&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>metrics-server for CKE on MIPS&lt;/td>
&lt;td>0.3.2&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/metrics-server-mips64el:v0.3.2&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>etcd for CKE on MIPS&lt;/td>
&lt;td>3.2.26&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/etcd/etcd-mips64el:v3.2.26&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>pause for CKE on MIPS&lt;/td>
&lt;td>3.1&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/pause-mips64el:3.1&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>hyperkube for CKE on MIPS&lt;/td>
&lt;td>1.14.3&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/hyperkube-mips64el:v1.14.3&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>coredns for CKE on MIPS&lt;/td>
&lt;td>1.6.5&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/coredns-mips64el:v1.6.5&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>calico for CKE on MIPS&lt;/td>
&lt;td>3.8.0&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/calico/cni-mips64el:v3.8.0&lt;/code> &lt;code>registry.inspurcloud.cn/library/cke/calico/ctl-mips64el:v3.8.0&lt;/code> &lt;code>registry.inspurcloud.cn/library/cke/calico/node-mips64el:v3.8.0&lt;/code> &lt;code>registry.inspurcloud.cn/library/cke/calico/kube-controllers-mips64el:v3.8.0&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Note&lt;/strong>: CKE is a Kubernetes-based cloud container engine launched by Inspur&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/k8s-mips-cluster-components.png" alt="K8S-MIPS Cluster Components">&lt;/p>
&lt;p>&lt;em>Figure 2 K8S-MIPS Cluster Components&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/cpu-architecture.png" alt="CPU Architecture">&lt;/p>
&lt;p>&lt;em>Figure 3 CPU Architecture&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/cluster-node-information.png" alt="Cluster Node Information">&lt;/p>
&lt;p>&lt;em>Figure 4 Cluster Node Information&lt;/em>&lt;/p>
&lt;h2 id="run-k8s-conformance-test">Run K8S Conformance Test&lt;/h2>
&lt;p>The most straightforward way to verify the stability and availability of the K8S-MIPS cluster is to run a Kubernetes &lt;a href="https://github.com/kubernetes/kubernetes/blob/v1.16.2/cluster/images/conformance/README.md">conformance test&lt;/a>.&lt;/p>
&lt;p>Conformance is a standalone container to launch Kubernetes end-to-end tests for conformance testing.&lt;/p>
&lt;p>Once the test has started, it launches several pods for various end-to-end tests. The source code of those images used by these pods is mostly from &lt;code>kubernetes/test/images&lt;/code>, and the built images are at &lt;code>gcr.io/kubernetes-e2e-test-images&lt;/code>. Since there are no MIPS images in the repository, we must first build all needed images to run the test.&lt;/p>
&lt;h3 id="build-needed-images-for-test">Build needed images for test&lt;/h3>
&lt;p>The first step is to find all needed images for the test. We can run &lt;code>sonobuoy images-p e2e&lt;/code> command to list all images, or we can find those images in &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/test/utils/image/manifest.go">/test/utils/image/manifest.go&lt;/a>. Although Kubernetes officially has a complete Makefile and shell-script that provides commands for building test images, there are still a number of architecture-related issues that have not been resolved, such as the incompatibilities of base images and dependencies. So we cannot directly build mips64el architecture images by executing these commands.&lt;/p>
&lt;p>Most test images are in golang, then compiled into binaries and built as Docker image based on the corresponding Dockerfile. These images are easy to build. But note that most images are using alpine as their base image, which does not officially support mips64el architecture for now. For this moment, we are unable to make mips64el version of &lt;a href="https://www.alpinelinux.org/">alpine&lt;/a>, so we have to replace the alpine to existing MIPS images, such as Debian-stretch, fedora, ubuntu. Replacing the base image also requires replacing the command to install the dependencies, even the version of these dependencies.&lt;/p>
&lt;p>Some images are not in &lt;code>kubernetes/test/images&lt;/code>, such as &lt;code>gcr.io/google-samples/gb-frontend:v6&lt;/code>. There is no clear documentation explaining where these images are locaated, though we found the source code in repository &lt;a href="https://github.com/GoogleCloudPlatform/kubernetes-engine-samples">github.com/GoogleCloudPlatform/kubernetes-engine-samples&lt;/a>. We soon ran into new problems: to build these google sample images, we have to build the base image it uses, even the base image of the base images, such as &lt;code>php:5-apache&lt;/code>, &lt;code>redis&lt;/code>, and &lt;code>perl&lt;/code>.&lt;/p>
&lt;p>After a long process of building an image, we finished with about four dozen images, including the images used by the test pod, and the base images. The last step before we run the tests is to place all those images into every node in the cluster and make sure the Pod image pull policy is &lt;code>imagePullPolicy: ifNotPresent&lt;/code>.&lt;/p>
&lt;p>Here are some of the images we built：&lt;/p>
&lt;ul>
&lt;li>&lt;code>docker.io/library/busybox:1.29&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/nginx:1.14-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/nginx:1.15-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/perl:5.26&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/httpd:2.4.38-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/redis:5.0.5-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/google-containers/conformance:v1.16.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/google-containers/hyperkube:v1.16.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/google-samples/gb-frontend:v6&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/agnhost:2.6&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/apparmor-loader:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/dnsutils:1.1&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/echoserver:2.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/ipc-utils:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/kitten:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/metadata-concealment:1.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/mounttest:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/nautilus:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/nonroot:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/resource-consumer-controller:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.10&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/test-webserver:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/gluster:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/iscsi:2.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/nfs:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/rbd:1.0.1&lt;/code>&lt;/li>
&lt;li>&lt;code>k8s.gcr.io/etcd:3.3.15&lt;/code>&lt;/li>
&lt;li>&lt;code>k8s.gcr.io/pause:3.1&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Finally, we ran the tests and got the test result, include &lt;code>e2e.log&lt;/code>, which showed that all test cases passed. Additionally, we submitted our test result to &lt;a href="https://github.com/cncf/k8s-conformance">k8s-conformance&lt;/a> as a &lt;a href="https://github.com/cncf/k8s-conformance/pull/779">pull request&lt;/a>.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/pull-request-for-conformance-test-results.png" alt="Pull request for conformance test results">&lt;/p>
&lt;p>&lt;em>Figure 5 Pull request for conformance test results&lt;/em>&lt;/p>
&lt;h2 id="what-s-next">What's next&lt;/h2>
&lt;p>We built the kubernetes-MIPS component manually and finished the conformance test, which verified the feasibility of Kubernetes On the MIPS platform and greatly enhanced our confidence in promoting the support of the MIPS architecture by Kubernetes.&lt;/p>
&lt;p>In the future, we plan to actively contribute our experience and achievements to the community, submit PR, and patch for MIPS. We hope that more developers and companies in the community join us and promote Kubernetes on MIPS.&lt;/p>
&lt;p>Contribution plan：&lt;/p>
&lt;ul>
&lt;li>contribute the source of e2e test images for MIPS&lt;/li>
&lt;li>contribute the source of hyperkube for MIPS&lt;/li>
&lt;li>contribute the source of deploy tools like kubeadm for MIPS&lt;/li>
&lt;/ul>
&lt;hr></description></item><item><title>Blog: Announcing the Kubernetes bug bounty program</title><link>https://kubernetes.io/blog/2020/01/14/kubernetes-bug-bounty-announcement/</link><pubDate>Tue, 14 Jan 2020 09:00:00 -0800</pubDate><guid>https://kubernetes.io/blog/2020/01/14/kubernetes-bug-bounty-announcement/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Maya Kaczorowski and Tim Allclair, Google, on behalf of the &lt;a href="https://github.com/kubernetes/community/tree/master/committee-product-security">Kubernetes Product Security Committee&lt;/a>&lt;/p>
&lt;p>Today, the &lt;a href="https://github.com/kubernetes/community/tree/master/committee-product-security">Kubernetes Product Security Committee&lt;/a> is launching a &lt;a href="https://hackerone.com/kubernetes">new bug bounty program&lt;/a>, funded by the &lt;a href="https://www.cncf.io/">CNCF&lt;/a>, to reward researchers finding security vulnerabilities in Kubernetes.&lt;/p>
&lt;h2 id="setting-up-a-new-bug-bounty-program">Setting up a new bug bounty program&lt;/h2>
&lt;p>We aimed to set up this bug bounty program as transparently as possible, with &lt;a href="https://docs.google.com/document/d/1dvlQsOGODhY3blKpjTg6UXzRdPzv5y8V55RD_Pbo7ag/edit#heading=h.7t1efwpev42p">an initial proposal&lt;/a>, &lt;a href="https://github.com/kubernetes/kubernetes/issues/73079">evaluation of vendors&lt;/a>, and &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/guide/bug-bounty.md">working draft of the components in scope&lt;/a>. Once we onboarded the selected bug bounty program vendor, &lt;a href="https://www.hackerone.com/">HackerOne&lt;/a>, these documents were further refined based on the feedback from HackerOne, as well as what was learned in the recent &lt;a href="https://github.com/kubernetes/community/blob/master/wg-security-audit/findings/Kubernetes%20Final%20Report.pdf">Kubernetes security audit&lt;/a>. The bug bounty program has been in a private release for several months now, with invited researchers able to submit bugs and help us test the triage process. After almost two years since the initial proposal, the program is now ready for all security researchers to contribute!&lt;/p>
&lt;p>What’s exciting is that this is rare: a bug bounty for an open-source infrastructure tool. Some open-source bug bounty programs exist, such as the &lt;a href="https://internetbugbounty.org/">Internet Bug Bounty&lt;/a>, this mostly covers core components that are consistently deployed across environments; but most bug bounties are still for hosted web apps. In fact, with more than&lt;a href="https://www.cncf.io/certification/kcsp/"> 100 certified distributions of Kubernetes&lt;/a>, the bug bounty program needs to apply to the Kubernetes code that powers all of them. By far, the most time-consuming challenge here has been ensuring that the program provider (HackerOne) and their researchers who do the first line triage have the awareness of Kubernetes and the ability to easily test the validity of a reported bug. As part of the bootstrapping process, HackerOne had their team pass the &lt;a href="https://www.cncf.io/certification/cka/">Certified Kubernetes Administrator&lt;/a> (CKA) exam.&lt;/p>
&lt;h2 id="what-s-in-scope">What’s in scope&lt;/h2>
&lt;p>The bug bounty scope covers code from the main Kubernetes organizations on GitHub, as well as continuous integration, release, and documentation artifacts. Basically, most content you’d think of as ‘core’ Kubernetes, included at &lt;a href="https://github.com/kubernetes">https://github.com/kubernetes&lt;/a>, is in scope. We’re particularly interested in cluster attacks, such as privilege escalations, authentication bugs, and remote code execution in the kubelet or API server. Any information leak about a workload, or unexpected permission changes is also of interest. Stepping back from the cluster admin’s view of the world, you’re also encouraged to look at the Kubernetes supply chain, including the build and release processes, which would allow any unauthorized access to commits, or the ability to publish unauthorized artifacts.&lt;/p>
&lt;p>Notably out of scope is the community management tooling, e.g., the Kubernetes mailing lists or Slack channel. Container escapes, attacks on the Linux kernel, or other dependencies, such as etcd, are also out of scope and should be reported to the appropriate party. We would still appreciate that any Kubernetes vulnerability, even if not in scope for the bug bounty, be &lt;a href="https://kubernetes.io/docs/reference/issues-security/security/#report-a-vulnerability">disclosed privately&lt;/a> to the Kubernetes Product Security Committee. See the full scope on the &lt;a href="https://hackerone.com/kubernetes">program reporting page&lt;/a>.&lt;/p>
&lt;h2 id="how-kubernetes-handles-vulnerabilities-and-disclosures">How Kubernetes handles vulnerabilities and disclosures&lt;/h2>
&lt;p>Kubernetes’ &lt;a href="https://github.com/kubernetes/community/tree/master/committee-product-security">Product Security Committee&lt;/a> is a group of security-focused maintainers who are responsible for receiving and responding to reports of security issues in Kubernetes. This follows the documented &lt;a href="https://kubernetes.io/docs/reference/issues-security/security/">security vulnerability response process&lt;/a>, which includes initial triage, assessing impact, generating and rolling out a fix.&lt;/p>
&lt;p>With our bug bounty program, initial triage and initial assessment are handled by the bug bounty provider, in this case, HackerOne, enabling us better scale our limited Kubernetes security experts to handle only valid reports. Nothing else in this process is changing - the Product Security Committee will continue to develop fixes, build private patches, and coordinate special security releases. New releases with security patches will be announced at &lt;a href="https://groups.google.com/forum/#!forum/kubernetes-security-announce">kubernetes-security-announce@googlegroups.com&lt;/a>.&lt;/p>
&lt;p>If you want to report a bug, you don’t need to use the bug bounty - you can still follow the &lt;a href="https://kubernetes.io/docs/reference/issues-security/security/#report-a-vulnerability">existing process&lt;/a> and report what you’ve found at &lt;a href="mailto:security@kubernetes.io">security@kubernetes.io&lt;/a>.&lt;/p>
&lt;h2 id="get-started">Get started&lt;/h2>
&lt;p>Just as many organizations support open source by hiring developers, paying bug bounties directly supports security researchers. This bug bounty is a critical step for Kubernetes to build up its community of security researchers and reward their hard work.&lt;/p>
&lt;p>If you’re a security researcher, and new to Kubernetes, check out these resources to learn more and get started bug hunting:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Hardening guides&lt;/strong>
&lt;ul>
&lt;li>Kubernetes.io: &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/">https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Frameworks&lt;/strong>
&lt;ul>
&lt;li>CIS benchmarks: &lt;a href="https://www.cisecurity.org/benchmark/kubernetes/">https://www.cisecurity.org/benchmark/kubernetes/&lt;/a>&lt;/li>
&lt;li>NIST 800-190: &lt;a href="https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf">https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Talks&lt;/strong>
&lt;ul>
&lt;li>The Devil in the Details: Kubernetes’ First Security Assessment (KubeCon NA 2019): &lt;a href="https://www.youtube.com/watch?v=vknE5XEa_Do">https://www.youtube.com/watch?v=vknE5XEa_Do&lt;/a>&lt;/li>
&lt;li>Crafty Requests: Deep Dive into Kubernetes CVE-2018-1002105 (KubeCon EU 2019): &lt;a href="https://www.youtube.com/watch?v=VjSJqc13PNk">https://www.youtube.com/watch?v=VjSJqc13PNk&lt;/a>&lt;/li>
&lt;li>A Hacker’s Guide to Kubernetes and the Cloud (KubeCon EU 2018): &lt;a href="https://www.youtube.com/watch?v=dxKpCO2dAy8">https://www.youtube.com/watch?v=dxKpCO2dAy8&lt;/a>&lt;/li>
&lt;li>Shipping in pirate-infested waters (KubeCon NA 2017): &lt;a href="https://www.youtube.com/watch?v=ohTq0no0ZVU">https://www.youtube.com/watch?v=ohTq0no0ZVU&lt;/a>&lt;/li>
&lt;li>Hacking and Hardening Kubernetes clusters by example (KubeCon NA 2017): &lt;a href="https://www.youtube.com/watch?v=vTgQLzeBfRU">https://www.youtube.com/watch?v=vTgQLzeBfRU&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>If you find something, please report a security bug to the Kubernetes bug bounty at &lt;a href="https://hackerone.com/kubernetes">https://hackerone.com/kubernetes&lt;/a>.&lt;/p>
&lt;!-- Docs to Markdown version 1.0β17 --></description></item><item><title>Blog: Remembering Brad Childs</title><link>https://kubernetes.io/blog/2020/01/10/remembering-brad-childs/</link><pubDate>Fri, 10 Jan 2020 10:00:00 -0800</pubDate><guid>https://kubernetes.io/blog/2020/01/10/remembering-brad-childs/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Paul Morie, Red Hat&lt;/p>
&lt;p>Last year, the Kubernetes family lost one of its own. Brad Childs was a
SIG Storage chair and long time contributor to the project. Brad worked on a
number of features in storage and was known as much for his friendliness and
sense of humor as for his technical contributions and leadership.&lt;/p>
&lt;p>We recently spent time remembering Brad at Kubecon NA:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://youtu.be/4eI2PTAJ-sE">A Tribute to Bradley Childs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/cncf/memorials/blob/master/bradley-childs.md">CNCF Memorial&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Our hearts go out to Brad’s friends and family and others whose lives he touched
inside and outside the Kubernetes community.&lt;/p>
&lt;p>Thank you for everything, Brad. We’ll miss you.&lt;/p></description></item><item><title>Blog: Testing of CSI drivers</title><link>https://kubernetes.io/blog/2020/01/08/testing-of-csi-drivers/</link><pubDate>Wed, 08 Jan 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/01/08/testing-of-csi-drivers/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Patrick Ohly (Intel)&lt;/p>
&lt;p>When developing a &lt;a href="https://kubernetes-csi.github.io/docs/">Container Storage Interface (CSI)
driver&lt;/a>, it is useful to leverage
as much prior work as possible. This includes source code (like the
&lt;a href="https://github.com/kubernetes-csi/csi-driver-host-path/">sample CSI hostpath
driver&lt;/a>) but
also existing tests. Besides saving time, using tests written by
someone else has the advantage that it can point out aspects of the
specification that might have been overlooked otherwise.&lt;/p>
&lt;p>An earlier blog post about &lt;a href="https://kubernetes.io/blog/2019/03/22/kubernetes-end-to-end-testing-for-everyone/">end-to-end
testing&lt;/a>
already showed how to use the &lt;a href="https://github.com/kubernetes/kubernetes/tree/master/test/e2e/storage/testsuites">Kubernetes storage
tests&lt;/a>
for testing of a third-party CSI driver. That
approach makes sense when the goal to also add custom E2E tests, but
depends on quite a bit of effort for setting up and maintaining a test
suite.&lt;/p>
&lt;p>When the goal is to merely run the existing tests, then there are
simpler approaches. This blog post introduces those.&lt;/p>
&lt;h2 id="sanity-testing">Sanity testing&lt;/h2>
&lt;p>&lt;a href="https://github.com/kubernetes-csi/csi-test/tree/master/pkg/sanity">csi-test
sanity&lt;/a>
ensures that a CSI driver conforms to the CSI specification by calling
the gRPC methods in various ways and checking that the outcome is as
required. Despite
its current hosting under the Kubernetes-CSI organization, it is
completely independent of Kubernetes. Tests connect to a running CSI
driver through its Unix domain socket, so although the tests are
written in Go, the driver itself can be implemented in any language.&lt;/p>
&lt;p>The main
&lt;a href="https://github.com/kubernetes-csi/csi-test/blob/master/pkg/sanity/README.md">README&lt;/a>
explains how to include those tests into an existing Go test
suite. The simpler alternative is to just invoke the &lt;a href="https://github.com/kubernetes-csi/csi-test/tree/master/cmd/csi-sanity">csi-sanity&lt;/a>
command.&lt;/p>
&lt;h3 id="installation">Installation&lt;/h3>
&lt;p>Starting with csi-test v3.0.0, you can build the &lt;code>csi-sanity&lt;/code> command
with &lt;code>go get github.com/kubernetes-csi/csi-test/cmd/csi-sanity&lt;/code> and
you'll find the compiled binary in &lt;code>$GOPATH/bin/csi-sanity&lt;/code>.&lt;/p>
&lt;p>&lt;code>go get&lt;/code> always builds the latest revision from the master branch. To
build a certain release, &lt;a href="https://github.com/kubernetes-csi/csi-test/releases">get the source
code&lt;/a> and run
&lt;code>make -C cmd/csi-sanity&lt;/code>. This produces &lt;code>cmd/csi-sanity/csi-sanity&lt;/code>.&lt;/p>
&lt;h3 id="usage">Usage&lt;/h3>
&lt;p>The &lt;code>csi-sanity&lt;/code> binary is a full &lt;a href="http://onsi.github.io/ginkgo/">Ginkgo test
suite&lt;/a> and thus has the usual &lt;code>-gingko&lt;/code>
command line flags. In particular, &lt;code>-ginkgo.focus&lt;/code> and
&lt;code>-ginkgo.skip&lt;/code> can be used to select which tests are run resp. not
run.&lt;/p>
&lt;p>During a test run, &lt;code>csi-sanity&lt;/code> simulates the behavior of a container
orchestrator (CO) by creating staging and target directories as required by the CSI spec
and calling a CSI driver via gRPC. The driver must be started before
invoking &lt;code>csi-sanity&lt;/code>. Although the tests currently only check the gRPC
return codes, that might change and so the driver really should make
the changes requested by a call, like mounting a filesystem. That may
mean that it has to run as root.&lt;/p>
&lt;p>At least one &lt;a href="https://github.com/grpc/grpc/blob/master/doc/naming.md">gRPC
endpoint&lt;/a> must
be specified via the &lt;code>-csi.endpoint&lt;/code> parameter when invoking
&lt;code>csi-sanity&lt;/code>, either as absolute path (&lt;code>unix:/tmp/csi.sock&lt;/code>) for a Unix
domain socket or as host name plus port (&lt;code>dns:///my-machine:9000&lt;/code>) for
TCP. &lt;code>csi-sanity&lt;/code> then uses that endpoint for both node and controller
operations. A separate endpoint for controller operations can be
specified with &lt;code>-csi.controllerendpoint&lt;/code>. Directories are created in
&lt;code>/tmp&lt;/code> by default. This can be changed via &lt;code>-csi.mountdir&lt;/code> and
&lt;code>-csi.stagingdir&lt;/code>.&lt;/p>
&lt;p>Some drivers cannot be deployed such that everything is guaranteed to
run on the same host. In such a case, custom scripts have to be used
to handle directories: they log into the host where the CSI node
controller runs and create or remove the directories there.&lt;/p>
&lt;p>For example, during CI testing the &lt;a href="https://github.com/kubernetes-csi/csi-driver-host-path">CSI hostpath example
driver&lt;/a> gets
deployed on a real Kubernetes cluster before invoking &lt;code>csi-sanity&lt;/code> and then
&lt;code>csi-sanity&lt;/code> connects to it through port forwarding provided by
&lt;a href="https://github.com/kubernetes-csi/csi-driver-host-path/blob/v1.2.0/deploy/kubernetes-1.16/hostpath/csi-hostpath-testing.yaml">&lt;code>socat&lt;/code>&lt;/a>.
&lt;a href="https://github.com/kubernetes-csi/csi-driver-host-path/blob/v1.2.0/release-tools/prow.sh#L808-L859">Scripts&lt;/a>
are used to create and remove the directories.&lt;/p>
&lt;p>Here's how one can replicate that, using the v1.2.0 release of the CSI hostpath driver:&lt;/p>
&lt;pre>&lt;code>$ cd csi-driver-host-path
$ git describe --tags HEAD
v1.2.0
$ kubectl get nodes
NAME STATUS ROLES AGE VERSION
127.0.0.1 Ready &amp;lt;none&amp;gt; 42m v1.16.0
$ deploy/kubernetes-1.16/deploy-hostpath.sh
applying RBAC rules
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-provisioner/v1.4.0/deploy/kubernetes/rbac.yaml
...
deploying hostpath components
deploy/kubernetes-1.16/hostpath/csi-hostpath-attacher.yaml
using image: quay.io/k8scsi/csi-attacher:v2.0.0
service/csi-hostpath-attacher created
statefulset.apps/csi-hostpath-attacher created
deploy/kubernetes-1.16/hostpath/csi-hostpath-driverinfo.yaml
csidriver.storage.k8s.io/hostpath.csi.k8s.io created
deploy/kubernetes-1.16/hostpath/csi-hostpath-plugin.yaml
using image: quay.io/k8scsi/csi-node-driver-registrar:v1.2.0
using image: quay.io/k8scsi/hostpathplugin:v1.2.0
using image: quay.io/k8scsi/livenessprobe:v1.1.0
...
service/hostpath-service created
statefulset.apps/csi-hostpath-socat created
07:38:46 waiting for hostpath deployment to complete, attempt #0
deploying snapshotclass
volumesnapshotclass.snapshot.storage.k8s.io/csi-hostpath-snapclass created
$ cat &amp;gt;mkdir_in_pod.sh &amp;lt;&amp;lt;EOF
#!/bin/sh
kubectl exec csi-hostpathplugin-0 -c hostpath -- mktemp -d /tmp/csi-sanity.XXXXXX
EOF
$ cat &amp;gt;rmdir_in_pod.sh &amp;lt;&amp;lt;EOF
#!/bin/sh
kubectl exec csi-hostpathplugin-0 -c hostpath -- rmdir &amp;quot;\$@&amp;quot;
EOF
$ chmod u+x *_in_pod.sh
$ csi-sanity -ginkgo.v \
-csi.endpoint dns:///127.0.0.1:$(kubectl get &amp;quot;services/hostpath-service&amp;quot; -o &amp;quot;jsonpath={..nodePort}&amp;quot;) \
-csi.createstagingpathcmd ./mkdir_in_pod.sh \
-csi.createmountpathcmd ./mkdir_in_pod.sh \
-csi.removestagingpathcmd ./rmdir_in_pod.sh \
-csi.removemountpathcmd ./rmdir_in_pod.sh
Running Suite: CSI Driver Test Suite
====================================
Random Seed: 1570540138
Will run 72 of 72 specs
...
Controller Service [Controller Server] ControllerGetCapabilities
should return appropriate capabilities
/nvme/gopath/src/github.com/kubernetes-csi/csi-test/pkg/sanity/controller.go:111
STEP: connecting to CSI driver
STEP: creating mount and staging directories
STEP: checking successful response
•
------------------------------
Controller Service [Controller Server] GetCapacity
should return capacity (no optional values added)
/nvme/gopath/src/github.com/kubernetes-csi/csi-test/pkg/sanity/controller.go:149
STEP: reusing connection to CSI driver at dns:///127.0.0.1:30056
STEP: creating mount and staging directories
...
Ran 53 of 72 Specs in 148.206 seconds
SUCCESS! -- 53 Passed | 0 Failed | 0 Pending | 19 Skipped
PASS
&lt;/code>&lt;/pre>&lt;p>Some comments:&lt;/p>
&lt;ul>
&lt;li>The source code of these tests is in the &lt;a href="https://github.com/kubernetes-csi/csi-test/tree/master/pkg/sanity">&lt;code>pkg/sanity&lt;/code>
package&lt;/a>.&lt;/li>
&lt;li>How to determine the external IP address of the node depends on the
cluster. In this example, the cluster was brought up with
&lt;code>hack/local-up-cluster.sh&lt;/code> and thus runs on the local host (&lt;code>127.0.0.1&lt;/code>).
It uses a port allocated by Kubernetes, obtained above with &lt;code>kubectl get &amp;quot;services/hostpath-service&amp;quot;&lt;/code>.
The Kubernetes-CSI CI uses
&lt;a href="https://kind.sigs.k8s.io/docs/user/quick-start/">kind&lt;/a> and there &lt;a href="https://github.com/kubernetes-csi/csi-driver-host-path/blob/3488dc7f994e33485629b86b69a6f34ebb7ef2d9/release-tools/prow.sh#L850">a
Docker
command&lt;/a>
can be used.&lt;/li>
&lt;li>The create script must print the final directory. Using a
unique directory for each test case has the advantage that if
something goes wrong in one test case, others still start with a
clean slate.&lt;/li>
&lt;li>The &amp;quot;staging directory&amp;quot;, aka &lt;code>NodePublishVolumeRequest.target_path&lt;/code>
in the CSI spec, must be created and deleted by the CSI driver while
the CO is responsible for the parent directory. &lt;code>csi-sanity&lt;/code> handles
that by creating a directory and then giving the CSI driver that
directory path with &lt;code>/target&lt;/code> appended at the end. Kubernetes &lt;a href="https://github.com/kubernetes/kubernetes/issues/75535">got
this wrong&lt;/a>
and creates the actual &lt;code>target_path&lt;/code> directory, so CSI drivers which
want to work with Kubernetes currently have to be lenient and must
not fail when that directory already exists.&lt;/li>
&lt;li>The &amp;quot;mount directory&amp;quot; corresponds to
&lt;code>NodeStageVolumeRequest.staging_target_path&lt;/code> and really gets created
by the CO, i.e. &lt;code>csi-sanity&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="end-to-end-testing">End-to-end testing&lt;/h2>
&lt;p>In contrast to &lt;code>csi-sanity&lt;/code>, end-to-end testing interacts with the CSI
driver through the Kubernetes API, i.e. it simulates operations from a
normal user, like creating a PersistentVolumeClaim. Support for testing external CSI
drivers was
&lt;a href="https://github.com/kubernetes/kubernetes/commit/6644db9914379a4a7b3d3487b41b2010f226e4dc#diff-5b2d9461c960bc9b146c4ab3d77bcaa5">added&lt;/a>
in Kubernetes 1.14.0.&lt;/p>
&lt;h3 id="installation-1">Installation&lt;/h3>
&lt;p>For each Kubernetes release, a test tar archive is published. It's not
listed in the release notes (for example, the ones for
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.16.md#downloads-for-v1161">1.16&lt;/a>),
so one has to know that the full URL is
&lt;code>https://dl.k8s.io/&amp;lt;version&amp;gt;/kubernetes-test-linux-amd64.tar.gz&lt;/code> (like
for
&lt;a href="https://dl.k8s.io/v1.16.0/kubernetes-test-linux-amd64.tar.gz">v1.16.0&lt;/a>).&lt;/p>
&lt;p>These include a &lt;code>e2e.test&lt;/code> binary for Linux on x86-64. Archives for
other platforms are also available, see &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-testing/20190118-breaking-apart-the-kubernetes-test-tarball.md#proposal">this
KEP&lt;/a>. The
&lt;code>e2e.test&lt;/code> binary is completely self-contained, so one can &amp;quot;install&amp;quot;
it and the &lt;a href="https://onsi.github.io/ginkgo/">&lt;code>ginkgo&lt;/code> test runner&lt;/a> with:&lt;/p>
&lt;pre>&lt;code>curl --location https://dl.k8s.io/v1.16.0/kubernetes-test-linux-amd64.tar.gz | \
tar --strip-components=3 -zxf - kubernetes/test/bin/e2e.test kubernetes/test/bin/ginkgo
&lt;/code>&lt;/pre>&lt;p>Each &lt;code>e2e.test&lt;/code> binary contains tests that match the features
available in the corresponding release. In particular, the &lt;code>[Feature: xyz]&lt;/code> tags change between releases: they separate tests of alpha
features from tests of non-alpha features. Also, the tests from an
older release might rely on APIs that were removed in more recent
Kubernetes releases. To avoid problems, it's best to simply use the
&lt;code>e2e.test&lt;/code> binary that matches the Kubernetes release that is used for
testing.&lt;/p>
&lt;h3 id="usage-1">Usage&lt;/h3>
&lt;p>Not all features of a CSI driver can be discovered through the
Kubernetes API. Therefore a configuration file in YAML or JSON format
is needed which describes the driver that is to be tested. That file
is used to populate &lt;a href="https://github.com/kubernetes/kubernetes/blob/v1.16.0/test/e2e/storage/external/external.go#L142-L211">the driverDefinition
struct&lt;/a>
and &lt;a href="https://github.com/kubernetes/kubernetes/blob/v1.16.0/test/e2e/storage/testsuites/testdriver.go#L139-L185">the DriverInfo
struct&lt;/a>
that is embedded inside it. For detailed usage instructions of
individual fields refer to these structs.&lt;/p>
&lt;p>A word of warning: tests are often only run when setting some fields and the
file parser does not warn about unknown fields, so always check that
the file really matches those structs.&lt;/p>
&lt;p>Here is an example that tests the
&lt;a href="https://github.com/kubernetes-csi/csi-driver-host-path">&lt;code>csi-driver-host-path&lt;/code>&lt;/a>:&lt;/p>
&lt;pre>&lt;code>$ cat &amp;gt;test-driver.yaml &amp;lt;&amp;lt;EOF
StorageClass:
FromName: true
SnapshotClass:
FromName: true
DriverInfo:
Name: hostpath.csi.k8s.io
Capabilities:
block: true
controllerExpansion: true
exec: true
multipods: true
persistence: true
pvcDataSource: true
snapshotDataSource: true
InlineVolumes:
- Attributes: {}
EOF
&lt;/code>&lt;/pre>&lt;p>At a minimum, you need to define the storage class you want to use in
the test, the name of your driver, and what capabilities you want to
test.
As with &lt;code>csi-sanity&lt;/code>, the driver has to be running in the cluster
before testing it.
The actual &lt;code>e2e.test&lt;/code> invocation then enables tests for this driver
with &lt;code>-storage.testdriver&lt;/code> and selects the storage tests for it with
&lt;code>-ginkgo.focus&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ ./e2e.test -ginkgo.v \
-ginkgo.focus='External.Storage' \
-storage.testdriver=test-driver.yaml
Oct 8 17:17:42.230: INFO: The --provider flag is not set. Continuing as if --provider=skeleton had been used.
I1008 17:17:42.230210 648569 e2e.go:92] Starting e2e run &amp;quot;90b9adb0-a3a2-435f-80e0-640742d56104&amp;quot; on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1570547861 - Will randomize all specs
Will run 163 of 5060 specs
Oct 8 17:17:42.237: INFO: &amp;gt;&amp;gt;&amp;gt; kubeConfig: /var/run/kubernetes/admin.kubeconfig
Oct 8 17:17:42.241: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
...
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
External Storage [Driver: hostpath.csi.k8s.io] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow]
should access to two volumes with different volume mode and retain data across pod recreation on the same node
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/testsuites/multivolume.go:191
[BeforeEach] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow]
...
&lt;/code>&lt;/pre>&lt;p>You can use &lt;code>ginkgo&lt;/code> to run some kinds of test in parallel.
Alpha feature tests or those that by design have to be run
sequentially then need to be run separately:&lt;/p>
&lt;pre>&lt;code>$ ./ginkgo -p -v \
-focus='External.Storage' \
-skip='\[Feature:|\[Disruptive\]|\[Serial\]' \
./e2e.test \
-- \
-storage.testdriver=test-driver.yaml
$ ./ginkgo -v \
-focus='External.Storage.*(\[Feature:|\[Disruptive\]|\[Serial\])' \
./e2e.test \
-- \
-storage.testdriver=test-driver.yaml
&lt;/code>&lt;/pre>&lt;h2 id="getting-involved">Getting involved&lt;/h2>
&lt;p>Both the Kubernetes storage tests and the sanity tests are meant to be
applicable to arbitrary CSI drivers. But perhaps tests are based on
additional assumptions and your driver does not pass the testing
although it complies with the CSI specification. If that happens then
please file issues (links below).&lt;/p>
&lt;p>These are open source projects which depend on the help of those
using them, so once a problem has been acknowledged, a pull request
addressing it will be highly welcome.&lt;/p>
&lt;p>The same applies to writing new tests. The following searches in the
issue trackers select issues that have been marked specifically as
something that needs someone's help:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/csi-test/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22">csi-test&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues?utf8=%E2%9C%93&amp;amp;q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22+label%3Asig%2Fstorage+">Kubernetes&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Happy testing! May the issues it finds be few and easy to fix.&lt;/p></description></item><item><title>Blog: Kubernetes 1.17: Stability</title><link>https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-release-announcement/</link><pubDate>Mon, 09 Dec 2019 13:00:00 -0800</pubDate><guid>https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-release-announcement/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md">Kubernetes 1.17 Release Team&lt;/a>&lt;/p>
&lt;p>We’re pleased to announce the delivery of Kubernetes 1.17, our fourth and final release of 2019! Kubernetes v1.17 consists of 22 enhancements: 14 enhancements have graduated to stable, 4 enhancements are moving to beta, and 4 enhancements are entering alpha.&lt;/p>
&lt;h2 id="major-themes">Major Themes&lt;/h2>
&lt;h3 id="cloud-provider-labels-reach-general-availability">Cloud Provider Labels reach General Availability&lt;/h3>
&lt;p>Added as a beta feature way back in v1.2, v1.17 sees the general availability of cloud provider labels.&lt;/p>
&lt;h3 id="volume-snapshot-moves-to-beta">Volume Snapshot Moves to Beta&lt;/h3>
&lt;p>The Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17. It was introduced as alpha in Kubernetes v1.12, with a second alpha with breaking changes in Kubernetes v1.13.&lt;/p>
&lt;h3 id="csi-migration-beta">CSI Migration Beta&lt;/h3>
&lt;p>The Kubernetes in-tree storage plugin to Container Storage Interface (CSI) migration infrastructure is now beta in Kubernetes v1.17. CSI migration was introduced as alpha in Kubernetes v1.14.&lt;/p>
&lt;h2 id="cloud-provider-labels-reach-general-availability-1">Cloud Provider Labels reach General Availability&lt;/h2>
&lt;p>When nodes and volumes are created, a set of standard labels are applied based on the underlying cloud provider of the Kubernetes cluster. Nodes get a label for the instance type. Both nodes and volumes get two labels describing the location of the resource in the cloud provider topology, usually organized in zones and regions.&lt;/p>
&lt;p>Standard labels are used by Kubernetes components to support some features. For example, the scheduler would ensure that pods are placed on the same zone as the volumes they claim; and when scheduling pods belonging to a deployment, the scheduler would prioritize spreading them across zones. You can also use the labels in your pod specs to configure things as such node affinity. Standard labels allow you to write pod specs that are portable among different cloud providers.&lt;/p>
&lt;p>The labels are reaching general availability in this release. Kubernetes components have been updated to populate the GA and beta labels and to react to both. However, if you are using the beta labels in your pod specs for features such as node affinity, or in your custom controllers, we recommend that you start migrating them to the new GA labels. You can find the documentation for the new labels here:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#nodekubernetesioinstance-type">node.kubernetes.io/instance-type&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesioregion">topology.kubernetes.io/region&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone">topology.kubernetes.io/zone&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="volume-snapshot-moves-to-beta-1">Volume Snapshot Moves to Beta&lt;/h2>
&lt;p>The Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17. It was introduced as alpha in Kubernetes v1.12, with a second alpha with breaking changes in Kubernetes v1.13. This post summarizes the changes in the beta release.&lt;/p>
&lt;h3 id="what-is-a-volume-snapshot">What is a Volume Snapshot?&lt;/h3>
&lt;p>Many storage systems (like Google Cloud Persistent Disks, Amazon Elastic Block Storage, and many on-premise storage systems) provide the ability to create a “snapshot” of a persistent volume. A snapshot represents a point-in-time copy of a volume. A snapshot can be used either to provision a new volume (pre-populated with the snapshot data) or to restore an existing volume to a previous state (represented by the snapshot).&lt;/p>
&lt;h3 id="why-add-volume-snapshots-to-kubernetes">Why add Volume Snapshots to Kubernetes?&lt;/h3>
&lt;p>The Kubernetes volume plugin system already provides a powerful abstraction that automates the provisioning, attaching, and mounting of block and file storage.&lt;/p>
&lt;p>Underpinning all these features is the Kubernetes goal of workload portability: Kubernetes aims to create an abstraction layer between distributed systems applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no “cluster specific” knowledge.&lt;/p>
&lt;p>The Kubernetes Storage SIG identified snapshot operations as critical functionality for many stateful workloads. For example, a database administrator may want to snapshot a database volume before starting a database operation.&lt;/p>
&lt;p>By providing a standard way to trigger snapshot operations in the Kubernetes API, Kubernetes users can now handle use cases like this without having to go around the Kubernetes API (and manually executing storage system specific operations).&lt;/p>
&lt;p>Instead, Kubernetes users are now empowered to incorporate snapshot operations in a cluster agnostic way into their tooling and policy with the comfort of knowing that it will work against arbitrary Kubernetes clusters regardless of the underlying storage.&lt;/p>
&lt;p>Additionally these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise grade, storage administration features for Kubernetes: including application or cluster level backup solutions.&lt;/p>
&lt;p>You can read more in the blog entry about &lt;a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/">releasing CSI volume snapshots to beta&lt;/a>.&lt;/p>
&lt;h2 id="csi-migration-beta-1">CSI Migration Beta&lt;/h2>
&lt;h3 id="why-are-we-migrating-in-tree-plugins-to-csi">Why are we migrating in-tree plugins to CSI?&lt;/h3>
&lt;p>Prior to CSI, Kubernetes provided a powerful volume plugin system. These volume plugins were “in-tree” meaning their code was part of the core Kubernetes code and shipped with the core Kubernetes binaries. However, adding support for new volume plugins to Kubernetes was challenging. Vendors that wanted to add support for their storage system to Kubernetes (or even fix a bug in an existing volume plugin) were forced to align with the Kubernetes release process. In addition, third-party storage code caused reliability and security issues in core Kubernetes binaries and the code was often difficult (and in some cases impossible) for Kubernetes maintainers to test and maintain. Using the Container Storage Interface in Kubernetes resolves these major issues.&lt;/p>
&lt;p>As more CSI Drivers were created and became production ready, we wanted all Kubernetes users to reap the benefits of the CSI model. However, we did not want to force users into making workload/configuration changes by breaking the existing generally available storage APIs. The way forward was clear - we would have to replace the backend of the “in-tree plugin” APIs with CSI.
What is CSI migration?&lt;/p>
&lt;p>The CSI migration effort enables the replacement of existing in-tree storage plugins such as &lt;code>kubernetes.io/gce-pd&lt;/code> or &lt;code>kubernetes.io/aws-ebs&lt;/code> with a corresponding CSI driver. If CSI Migration is working properly, Kubernetes end users shouldn’t notice a difference. After migration, Kubernetes users may continue to rely on all the functionality of in-tree storage plugins using the existing interface.&lt;/p>
&lt;p>When a Kubernetes cluster administrator updates a cluster to enable CSI migration, existing stateful deployments and workloads continue to function as they always have; however, behind the scenes Kubernetes hands control of all storage management operations (previously targeting in-tree drivers) to CSI drivers.&lt;/p>
&lt;p>The Kubernetes team has worked hard to ensure the stability of storage APIs and for the promise of a smooth upgrade experience. This involves meticulous accounting of all existing features and behaviors to ensure backwards compatibility and API stability. You can think of it like changing the wheels on a racecar while it’s speeding down the straightaway.&lt;/p>
&lt;p>You can read more in the blog entry about &lt;a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/">CSI migration going to beta&lt;/a>.&lt;/p>
&lt;h2 id="other-updates">Other Updates&lt;/h2>
&lt;h3 id="graduated-to-stable">Graduated to Stable 💯&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/382">Taint Node by Condition&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/495">Configurable Pod Process Namespace Sharing&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/548">Schedule DaemonSet Pods by kube-scheduler&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/554">Dynamic Maximum Volume Count&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/557">Kubernetes CSI Topology Support&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/559">Provide Environment Variables Expansion in SubPath Mount&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/575">Defaulting of Custom Resources&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/589">Move Frequent Kubelet Heartbeats To Lease Api&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/714">Break Apart The Kubernetes Test Tarball&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/956">Add Watch Bookmarks Support&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/960">Behavior-Driven Conformance Testing&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/980">Finalizer Protection For Service Loadbalancers&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1152">Avoid Serializing The Same Object Independently For Every Watcher&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="major-changes">Major Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/563">Add IPv4/IPv6 Dual Stack Support&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="other-notable-features">Other Notable Features&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/536">Topology Aware Routing of Services (Alpha)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1043">RunAsUserName for Windows&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="availability">Availability&lt;/h3>
&lt;p>Kubernetes 1.17 is available for &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.17.0">download on GitHub&lt;/a>. To get started with Kubernetes, check out these &lt;a href="https://kubernetes.io/docs/tutorials/">interactive tutorials&lt;/a>. You can also easily install 1.17 using &lt;a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a>.&lt;/p>
&lt;h3 id="release-team">Release Team&lt;/h3>
&lt;p>This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md">release team&lt;/a> led by Guinevere Saenger. The 35 individuals on the release team coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.&lt;/p>
&lt;p>As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over &lt;a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">39,000 individual contributors&lt;/a> to date and an active community of more than 66,000 people.&lt;/p>
&lt;h3 id="webinar">Webinar&lt;/h3>
&lt;p>Join members of the Kubernetes 1.17 release team on Jan 7th, 2020 to learn about the major features in this release. Register &lt;a href="https://zoom.us/webinar/register/9315759188139/WN_kPOZA_6RTjeGdXTG7YFO3A">here&lt;/a>.&lt;/p>
&lt;h3 id="get-involved">Get Involved&lt;/h3>
&lt;p>The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Special Interest Groups&lt;/a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href="https://github.com/kubernetes/community/tree/master/communication">community meeting&lt;/a>, and through the channels below. Thank you for your continued feedback and support.&lt;/p>
&lt;ul>
&lt;li>Follow us on Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for latest updates&lt;/li>
&lt;li>Join the community discussion on &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>&lt;/li>
&lt;li>Join the community on &lt;a href="http://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>Post questions (or answer questions) on &lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a>&lt;/li>
&lt;li>Share your Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">story&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 1.17 Feature: Kubernetes Volume Snapshot Moves to Beta</title><link>https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/</link><pubDate>Mon, 09 Dec 2019 10:00:00 -0800</pubDate><guid>https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Xing Yang, VMware &amp;amp; Xiangqian Yu, Google&lt;/p>
&lt;p>The Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17. It was introduced &lt;a href="https://kubernetes.io/blog/2018/10/09/introducing-volume-snapshot-alpha-for-kubernetes/">as alpha&lt;/a> in Kubernetes v1.12, with a &lt;a href="https://kubernetes.io/blog/2019/01/17/update-on-volume-snapshot-alpha-for-kubernetes/">second alpha&lt;/a> with breaking changes in Kubernetes v1.13. This post summarizes the changes in the beta release.&lt;/p>
&lt;h2 id="what-is-a-volume-snapshot">What is a Volume Snapshot?&lt;/h2>
&lt;p>Many storage systems (like Google Cloud Persistent Disks, Amazon Elastic Block Storage, and many on-premise storage systems) provide the ability to create a “snapshot” of a persistent volume. A snapshot represents a point-in-time copy of a volume. A snapshot can be used either to provision a new volume (pre-populated with the snapshot data) or to restore an existing volume to a previous state (represented by the snapshot).&lt;/p>
&lt;h2 id="why-add-volume-snapshots-to-kubernetes">Why add Volume Snapshots to Kubernetes?&lt;/h2>
&lt;p>The Kubernetes volume plugin system already provides a powerful abstraction that automates the provisioning, attaching, and mounting of block and file storage.&lt;/p>
&lt;p>Underpinning all these features is the Kubernetes goal of workload portability: Kubernetes aims to create an abstraction layer between distributed applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no “cluster specific” knowledge.&lt;/p>
&lt;p>The Kubernetes Storage SIG identified snapshot operations as critical functionality for many stateful workloads. For example, a database administrator may want to snapshot a database volume before starting a database operation.&lt;/p>
&lt;p>By providing a standard way to trigger snapshot operations in the Kubernetes API, Kubernetes users can now handle use cases like this without having to go around the Kubernetes API (and manually executing storage system specific operations).&lt;/p>
&lt;p>Instead, Kubernetes users are now empowered to incorporate snapshot operations in a cluster agnostic way into their tooling and policy with the comfort of knowing that it will work against arbitrary Kubernetes clusters regardless of the underlying storage.&lt;/p>
&lt;p>Additionally these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise grade, storage administration features for Kubernetes: including application or cluster level backup solutions.&lt;/p>
&lt;h2 id="what-s-new-in-beta">What’s new in Beta?&lt;/h2>
&lt;p>With the promotion of Volume Snapshot to beta, the feature is now enabled by default on standard Kubernetes deployments instead of being opt-in.&lt;/p>
&lt;p>The move of the Kubernetes Volume Snapshot feature to beta also means:&lt;/p>
&lt;ul>
&lt;li>A revamp of volume snapshot APIs.&lt;/li>
&lt;li>The CSI external-snapshotter sidecar is split into two controllers, a common snapshot controller and a CSI external-snapshotter sidecar.&lt;/li>
&lt;li>Deletion secret is added as an annotation to the volume snapshot content.&lt;/li>
&lt;li>A new finalizer is added to the volume snapshot API object to prevent it from being deleted when it is bound to a volume snapshot content API object.&lt;/li>
&lt;/ul>
&lt;h2 id="kubernetes-volume-snapshots-requirements">Kubernetes Volume Snapshots Requirements&lt;/h2>
&lt;p>As mentioned above, with the promotion of Volume Snapshot to beta, the feature is now enabled by default on standard Kubernetes deployments instead of being opt-in.&lt;/p>
&lt;p>In order to use the Kubernetes Volume Snapshot feature, you must ensure the following components have been deployed on your Kubernetes cluster:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-snapshotter/tree/master/config/crd">Kubernetes Volume Snapshot CRDs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-snapshotter/tree/master/pkg/common-controller">Volume snapshot controller&lt;/a>&lt;/li>
&lt;li>CSI Driver supporting Kubernetes volume snapshot beta&lt;/li>
&lt;/ul>
&lt;p>See the deployment section below for details.&lt;/p>
&lt;h2 id="which-drivers-support-kubernetes-volume-snapshots">Which drivers support Kubernetes Volume Snapshots?&lt;/h2>
&lt;p>Kubernetes supports three types of volume plugins: in-tree, Flex, and CSI. See &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md">Kubernetes Volume Plugin FAQ&lt;/a> for details.&lt;/p>
&lt;p>Snapshots are only supported for CSI drivers (not for in-tree or Flex). To use the Kubernetes snapshots feature, ensure that a CSI Driver that implements snapshots is deployed on your cluster.&lt;/p>
&lt;p>Read the “&lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">Container Storage Interface (CSI) for Kubernetes GA&lt;/a>” blog post to learn more about CSI and how to deploy CSI drivers.&lt;/p>
&lt;p>As of the publishing of this blog, the following CSI drivers have been updated to support volume snapshots beta:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver">GCE Persistent Disk CSI Driver&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/libopenstorage/openstorage/tree/master/csi">Portworx CSI Driver&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/NetApp/trident">NetApp Trident CSI Driver&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Beta level Volume Snapshot support for other &lt;a href="https://kubernetes-csi.github.io/docs/drivers.html">CSI drivers&lt;/a> is pending, and should be available soon.&lt;/p>
&lt;h2 id="kubernetes-volume-snapshot-beta-api">Kubernetes Volume Snapshot Beta API&lt;/h2>
&lt;p>A number of changes were made to the Kubernetes volume snapshot API between alpha to beta. These changes are not backward compatible. The purpose of these changes was to make API definitions clear and easier to use.&lt;/p>
&lt;p>The following changes are made:&lt;/p>
&lt;ul>
&lt;li>&lt;code>DeletionPolicy&lt;/code> is now a required field rather than optional in both &lt;code>VolumeSnapshotClass&lt;/code> and &lt;code>VolumeSnapshotContent&lt;/code>. This way the user has to explicitly specify it, leaving no room for confusion.&lt;/li>
&lt;li>&lt;code>VolumeSnapshotSpec&lt;/code> has a new required &lt;code>Source&lt;/code> field. &lt;code>Source&lt;/code> may be either a &lt;code>PersistentVolumeClaimName&lt;/code> (if dynamically provisioning a snapshot) or &lt;code>VolumeSnapshotContentName&lt;/code> (if pre-provisioning a snapshot).&lt;/li>
&lt;li>&lt;code>VolumeSnapshotContentSpec&lt;/code> also has a new required &lt;code>Source&lt;/code> field. This &lt;code>Source&lt;/code> may be either a &lt;code>VolumeHandle&lt;/code> (if dynamically provisioning a snapshot) or a &lt;code>SnapshotHandle&lt;/code> (if pre-provisioning volume snapshots).&lt;/li>
&lt;li>&lt;code>VolumeSnapshotStatus&lt;/code> now contains a &lt;code>BoundVolumeSnapshotContentName&lt;/code> to indicate the &lt;code>VolumeSnapshot&lt;/code> object is bound to a &lt;code>VolumeSnapshotContent&lt;/code>.&lt;/li>
&lt;li>&lt;code>VolumeSnapshotContent&lt;/code>now contains a &lt;code>Status&lt;/code> to indicate the current state of the content. It has a field &lt;code>SnapshotHandle&lt;/code> to indicate that the &lt;code>VolumeSnapshotContent&lt;/code> represents a snapshot on the storage system.&lt;/li>
&lt;/ul>
&lt;p>The beta Kubernetes VolumeSnapshot API object:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> VolumeSnapshot &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {
metav1.TypeMeta
metav1.ObjectMeta
Spec VolumeSnapshotSpec
Status &lt;span style="color:#666">*&lt;/span>VolumeSnapshotStatus
}
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> VolumeSnapshotSpec &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {
Source VolumeSnapshotSource
VolumeSnapshotClassName &lt;span style="color:#666">*&lt;/span>&lt;span style="color:#0b0;font-weight:bold">string&lt;/span>
}
&lt;span style="color:#080;font-style:italic">// Exactly one of its members MUST be specified
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> VolumeSnapshotSource &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {
&lt;span style="color:#080;font-style:italic">// +optional
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> PersistentVolumeClaimName &lt;span style="color:#666">*&lt;/span>&lt;span style="color:#0b0;font-weight:bold">string&lt;/span>
&lt;span style="color:#080;font-style:italic">// +optional
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> VolumeSnapshotContentName &lt;span style="color:#666">*&lt;/span>&lt;span style="color:#0b0;font-weight:bold">string&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> VolumeSnapshotStatus &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {
BoundVolumeSnapshotContentName &lt;span style="color:#666">*&lt;/span>&lt;span style="color:#0b0;font-weight:bold">string&lt;/span>
CreationTime &lt;span style="color:#666">*&lt;/span>metav1.Time
ReadyToUse &lt;span style="color:#666">*&lt;/span>&lt;span style="color:#0b0;font-weight:bold">bool&lt;/span>
RestoreSize &lt;span style="color:#666">*&lt;/span>resource.Quantity
Error &lt;span style="color:#666">*&lt;/span>VolumeSnapshotError
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The beta Kubernetes VolumeSnapshotContent API object:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> VolumeSnapshotContent &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {
metav1.TypeMeta
metav1.ObjectMeta
Spec VolumeSnapshotContentSpec
Status &lt;span style="color:#666">*&lt;/span>VolumeSnapshotContentStatus
}
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> VolumeSnapshotContentSpec &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {
VolumeSnapshotRef core_v1.ObjectReference
Source VolumeSnapshotContentSource
DeletionPolicy DeletionPolicy
Driver &lt;span style="color:#0b0;font-weight:bold">string&lt;/span>
VolumeSnapshotClassName &lt;span style="color:#666">*&lt;/span>&lt;span style="color:#0b0;font-weight:bold">string&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> VolumeSnapshotContentSource &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {
&lt;span style="color:#080;font-style:italic">// +optional
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> VolumeHandle &lt;span style="color:#666">*&lt;/span>&lt;span style="color:#0b0;font-weight:bold">string&lt;/span>
&lt;span style="color:#080;font-style:italic">// +optional
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> SnapshotHandle &lt;span style="color:#666">*&lt;/span>&lt;span style="color:#0b0;font-weight:bold">string&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> VolumeSnapshotContentStatus &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {
CreationTime &lt;span style="color:#666">*&lt;/span>&lt;span style="color:#0b0;font-weight:bold">int64&lt;/span>
ReadyToUse &lt;span style="color:#666">*&lt;/span>&lt;span style="color:#0b0;font-weight:bold">bool&lt;/span>
RestoreSize &lt;span style="color:#666">*&lt;/span>&lt;span style="color:#0b0;font-weight:bold">int64&lt;/span>
Error &lt;span style="color:#666">*&lt;/span>VolumeSnapshotError
SnapshotHandle &lt;span style="color:#666">*&lt;/span>&lt;span style="color:#0b0;font-weight:bold">string&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The beta Kubernetes VolumeSnapshotClass API object:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> VolumeSnapshotClass &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {
metav1.TypeMeta
metav1.ObjectMeta
Driver &lt;span style="color:#0b0;font-weight:bold">string&lt;/span>
Parameters &lt;span style="color:#a2f;font-weight:bold">map&lt;/span>[&lt;span style="color:#0b0;font-weight:bold">string&lt;/span>]&lt;span style="color:#0b0;font-weight:bold">string&lt;/span>
DeletionPolicy DeletionPolicy
}
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="how-do-i-deploy-support-for-volume-snapshots-on-my-kubernetes-cluster">How do I deploy support for Volume Snapshots on my Kubernetes Cluster?&lt;/h3>
&lt;p>Please note that the Volume Snapshot feature now depends on a new, common &lt;a href="https://github.com/kubernetes-csi/external-snapshotter/tree/master/pkg/common-controller">volume snapshot controller&lt;/a> in addition to the volume snapshot CRDs. Both the volume snapshot controller and the CRDs are independent of any CSI driver. Regardless of the number of CSI drivers deployed on the cluster, there must be only one instance of the volume snapshot controller running and one set of volume snapshot CRDs installed per cluster.&lt;/p>
&lt;p>Therefore, it is strongly recommended that Kubernetes distributors bundle and deploy the controller and CRDs as part of their Kubernetes cluster management process (independent of any CSI Driver).&lt;/p>
&lt;p>If your cluster does not come pre-installed with the correct components, you may manually install these components by executing the following steps.&lt;/p>
&lt;h4 id="install-snapshot-beta-crds">Install Snapshot Beta CRDs&lt;/h4>
&lt;ul>
&lt;li>&lt;code>kubectl create -f config/crd&lt;/code>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-snapshotter/tree/master/config/crd">https://github.com/kubernetes-csi/external-snapshotter/tree/master/config/crd&lt;/a>&lt;/li>
&lt;li>Do this once per cluster&lt;/li>
&lt;/ul>
&lt;h4 id="install-common-snapshot-controller">Install Common Snapshot Controller&lt;/h4>
&lt;ul>
&lt;li>&lt;code>kubectl create -f deploy/kubernetes/snapshot-controller&lt;/code>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-snapshotter/tree/master/deploy/kubernetes/snapshot-controller">https://github.com/kubernetes-csi/external-snapshotter/tree/master/deploy/kubernetes/snapshot-controller&lt;/a>&lt;/li>
&lt;li>Do this once per cluster&lt;/li>
&lt;/ul>
&lt;h4 id="install-csi-driver">Install CSI Driver&lt;/h4>
&lt;p>Follow instructions provided by your CSI Driver vendor.&lt;/p>
&lt;h3 id="how-do-i-use-kubernetes-volume-snapshots">How do I use Kubernetes Volume Snapshots?&lt;/h3>
&lt;p>Assuming all the required components (including CSI driver) are already deployed and running on your cluster, you can create volume snapshots using the VolumeSnapshot API object, and restore them by specifying a VolumeSnapshot data source on a PVC.&lt;/p>
&lt;h4 id="creating-a-new-volume-snapshot-with-kubernetes">Creating a New Volume Snapshot with Kubernetes&lt;/h4>
&lt;p>You can enable creation/deletion of volume snapshots in a Kubernetes cluster, by creating a VolumeSnapshotClass API object pointing to a CSI Driver that support volume snapshots.&lt;/p>
&lt;p>The following VolumeSnapshotClass, for example, tells the Kubernetes cluster that a CSI driver, &lt;code>testdriver.csi.k8s.io&lt;/code>, can handle volume snapshots, and that when these snapshots are created, their deletion policy should be to delete.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshotClass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapclass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">driver&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>testdriver.csi.k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">deletionPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Delete&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">csi.storage.k8s.io/snapshotter-secret-name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>mysecret&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">csi.storage.k8s.io/snapshotter-secret-namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>mysecretnamespace&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The common snapshot controller reserves the parameter keys &lt;code>csi.storage.k8s.io/snapshotter-secret-name&lt;/code> and &lt;code>csi.storage.k8s.io/snapshotter-secret-namespace&lt;/code>. If specified, it fetches the referenced Kubernetes secret and sets it as an annotation on the volume snapshot content object. The CSI external-snapshotter sidecar retrieves it from the content annotation and passes it to the CSI driver during snapshot creation.&lt;/p>
&lt;p>Creation of a volume snapshot is triggered by the creation of a VolumeSnapshot API object.&lt;/p>
&lt;p>The VolumeSnapshot object must specify the following source type:
&lt;code>persistentVolumeClaimName&lt;/code> - The name of the PVC to snapshot. Please note that the source PVC, PV, and VolumeSnapshotClass for a VolumeSnapshot object must point to the same CSI driver.&lt;/p>
&lt;p>The following VolumeSnapshot, for example, triggers the creation of a snapshot for a PVC called &lt;code>test-pvc&lt;/code> using the VolumeSnapshotClass above.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeSnapshotClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapclass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">persistentVolumeClaimName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>When volume snapshot creation is invoked, the common snapshot controller first creates a VolumeSnapshotContent object with the &lt;code>volumeSnapshotRef&lt;/code>, source &lt;code>volumeHandle&lt;/code>, &lt;code>volumeSnapshotClassName&lt;/code> if specified, &lt;code>driver&lt;/code>, and &lt;code>deletionPolicy&lt;/code>.&lt;/p>
&lt;p>The CSI external-snapshotter sidecar then passes the VolumeSnapshotClass parameters, the source volume ID, and any referenced secret(s) to the CSI driver (in this case &lt;code>testdriver.csi.k8s.io&lt;/code>) via a CSI &lt;code>CreateSnapshot&lt;/code> call. In response, the CSI driver creates a new snapshot for the specified volume, and returns the ID for that snapshot. The CSI external-snapshotter sidecar then updates the &lt;code>snapshotHandle&lt;/code>, &lt;code>creationTime&lt;/code>, &lt;code>restoreSize&lt;/code>, and &lt;code>readyToUse&lt;/code> in the status field of the VolumeSnapshotContent object that represents the new snapshot. For a storage system that needs to upload the snapshot after it has been cut, the CSI external-snapshotter sidecar will keep calling the CSI &lt;code>CreateSnapshot&lt;/code> to check the status until upload is complete and set &lt;code>readyToUse&lt;/code> to true.&lt;/p>
&lt;p>The common snapshot controller binds the VolumeSnapshotContent object to the VolumeSnapshot (sets &lt;code>BoundVolumeSnapshotContentName&lt;/code>), and updates the &lt;code>creationTime&lt;/code>, &lt;code>restoreSize&lt;/code>, and &lt;code>readyToUse&lt;/code> in the status field of the VolumeSnapshot object based on the status field of the VolumeSnapshotContent object.&lt;/p>
&lt;p>If no &lt;code>volumeSnapshotClassName&lt;/code> is specified, one is automatically selected as follows:&lt;/p>
&lt;p>The &lt;code>StorageClass&lt;/code> from PVC or PV of the source volume is fetched. The default VolumeSnapshotClass is fetched, if available. A default VolumeSnapshotClass is a snapshot class created by the admin with the &lt;code>snapshot.storage.kubernetes.io/is-default-class&lt;/code> annotation. If the &lt;code>Driver&lt;/code> field of the default VolumeSnapshotClass is the same as the &lt;code>Provisioner&lt;/code> field in the StorageClass, the default VolumeSnapshotClass is used. If there is no default VolumeSnapshotClass or more than one default VolumeSnapshotClass for a snapshot, an error will be returned.&lt;/p>
&lt;p>Please note that the Kubernetes Snapshot API does not provide any consistency guarantees. You have to prepare your application (pause application, freeze filesystem etc.) before taking the snapshot for data consistency either manually or using some other higher level APIs/controllers.&lt;/p>
&lt;p>You can verify that the VolumeSnapshot object is created and bound with VolumeSnapshotContent by running &lt;code>kubectl describe volumesnapshot&lt;/code>:&lt;/p>
&lt;p>&lt;code>Bound Volume Snapshot Content Name&lt;/code> - field in the &lt;code>Status&lt;/code> indicates the volume is bound to the specified VolumeSnapshotContent.
&lt;code>Ready To Use&lt;/code> - field in the &lt;code>Status&lt;/code> indicates this volume snapshot is ready for use.
&lt;code>Creation Time&lt;/code> - field in the &lt;code>Status&lt;/code> indicates when the snapshot was actually created (cut).
&lt;code>Restore Size&lt;/code> - field in the &lt;code>Status&lt;/code> indicates the minimum volume size required when restoring a volume from this snapshot.&lt;/p>
&lt;pre>&lt;code>Name: test-snapshot
Namespace: default
Labels: &amp;lt;none&amp;gt;
Annotations: &amp;lt;none&amp;gt;
API Version: snapshot.storage.k8s.io/v1beta1
Kind: VolumeSnapshot
Metadata:
Creation Timestamp: 2019-11-16T00:36:04Z
Finalizers:
snapshot.storage.kubernetes.io/volumesnapshot-as-source-protection
snapshot.storage.kubernetes.io/volumesnapshot-bound-protection
Generation: 1
Resource Version: 1294
Self Link: /apis/snapshot.storage.k8s.io/v1beta1/namespaces/default/volumesnapshots/new-snapshot-demo
UID: 32ceaa2a-3802-4edd-a808-58c4f1bd7869
Spec:
Source:
Persistent Volume Claim Name: test-pvc
Volume Snapshot Class Name: test-snapclass
Status:
Bound Volume Snapshot Content Name: snapcontent-32ceaa2a-3802-4edd-a808-58c4f1bd7869
Creation Time: 2019-11-16T00:36:04Z
Ready To Use: true
Restore Size: 1Gi
&lt;/code>&lt;/pre>&lt;p>As a reminder to any developers building controllers using volume snapshot APIs: before using a VolumeSnapshot API object, validate the bi-directional binding between the VolumeSnpashot and the VolumeSnapshotContent it is bound to, to ensure the binding is complete and correct (not doing so may result in security issues).&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl describe volumesnapshotcontent
&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>Name: snapcontent-32ceaa2a-3802-4edd-a808-58c4f1bd7869
Namespace:
Labels: &amp;lt;none&amp;gt;
Annotations: &amp;lt;none&amp;gt;
API Version: snapshot.storage.k8s.io/v1beta1
Kind: VolumeSnapshotContent
Metadata:
Creation Timestamp: 2019-11-16T00:36:04Z
Finalizers:
snapshot.storage.kubernetes.io/volumesnapshotcontent-bound-protection
Generation: 1
Resource Version: 1292
Self Link: /apis/snapshot.storage.k8s.io/v1beta1/volumesnapshotcontents/snapcontent-32ceaa2a-3802-4edd-a808-58c4f1bd7869
UID: 7dfdf22e-0b0c-4b71-9ddf-2f1612ca2aed
Spec:
Deletion Policy: Delete
Driver: testdriver.csi.k8s.io
Source:
Volume Handle: d1b34a5f-0808-11ea-808a-0242ac110003
Volume Snapshot Class Name: test-snapclass
Volume Snapshot Ref:
API Version: snapshot.storage.k8s.io/v1beta1
Kind: VolumeSnapshot
Name: test-snapshot
Namespace: default
Resource Version: 1286
UID: 32ceaa2a-3802-4edd-a808-58c4f1bd7869
Status:
Creation Time: 1573864564608810101
Ready To Use: true
Restore Size: 1073741824
Snapshot Handle: 127c5798-0809-11ea-808a-0242ac110003
Events: &amp;lt;none&amp;gt;
&lt;/code>&lt;/pre>&lt;h4 id="importing-an-existing-volume-snapshot-with-kubernetes">Importing an existing volume snapshot with Kubernetes&lt;/h4>
&lt;p>You can always expose a pre-existing volume snapshot in Kubernetes by manually creating a VolumeSnapshotContent object to represent the existing volume snapshot. Because VolumeSnapshotContent is a non-namespace API object, only a cluster admin may have the permission to create it. By specifying the &lt;code>volumeSnapshotRef&lt;/code> the cluster admin specifies exactly which user can use the snapshot.&lt;/p>
&lt;p>The following VolumeSnapshotContent, for example exposes a volume snapshot with the name &lt;code>7bdd0de3-aaeb-11e8-9aae-0242ac110002&lt;/code> belonging to a CSI driver called &lt;code>testdriver.csi.k8s.io&lt;/code>.&lt;/p>
&lt;p>A VolumeSnapshotContent object should be created by a cluster admin with the following fields to represent an existing snapshot:&lt;/p>
&lt;ul>
&lt;li>&lt;code>driver&lt;/code> - CSI driver used to handle this volume. This field is required.&lt;/li>
&lt;li>&lt;code>source&lt;/code> - Snapshot identifying information&lt;/li>
&lt;li>&lt;code>snapshotHandle&lt;/code> - name/identifier of the snapshot. This field is required.&lt;/li>
&lt;li>&lt;code>volumeSnapshotRef&lt;/code> - Pointer to the VolumeSnapshot object this content should bind to.&lt;/li>
&lt;li>&lt;code>name&lt;/code> and &lt;code>namespace&lt;/code> - Specifies the name and namespace of the VolumeSnapshot object which the content is bound to.&lt;/li>
&lt;li>&lt;code>deletionPolicy&lt;/code> - Valid values are &lt;code>Delete&lt;/code> and &lt;code>Retain&lt;/code>. If the &lt;code>deletionPolicy&lt;/code> is &lt;code>Delete&lt;/code>, then the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object. If the - &lt;code>deletionPolicy&lt;/code> is &lt;code>Retain&lt;/code>, then both the underlying snapshot and VolumeSnapshotContent remain.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshotContent&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>manually-created-snapshot-content&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">deletionPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Delete&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">driver&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>testdriver.csi.k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">snapshotHandle&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>7bdd0de3-aaeb&lt;span style="color:#666">-11e8&lt;/span>-9aae-0242ac110002&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeSnapshotRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once a VolumeSnapshotContent object is created, a user can create a VolumeSnapshot object pointing to the VolumeSnapshotContent object. The name and namespace of the VolumeSnapshot object must match the name/namespace specified in the volumeSnapshotRef of the VolumeSnapshotContent. It specifies the following fields:
&lt;code>volumeSnapshotContentName&lt;/code> - name of the volume snapshot content specified above. This field is required.
&lt;code>volumeSnapshotClassName&lt;/code> - name of the volume snapshot class. This field is optional.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>manually-created-snapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeSnapshotContentName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-content&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once both objects are created, the common snapshot controller verifies the binding between VolumeSnapshot and VolumeSnapshotContent objects is correct and marks the VolumeSnapshot as ready (if the CSI driver supports the &lt;code>ListSnapshots&lt;/code> call, the controller also validates that the referenced snapshot exists). The CSI external-snapshotter sidecar checks if the snapshot exists if ListSnapshots CSI method is implemented, otherwise it assumes the snapshot exists. The external-snapshotter sidecar sets &lt;code>readyToUse&lt;/code> to true in the status field of VolumeSnapshotContent. The common snapshot controller marks the snapshot as ready accordingly.&lt;/p>
&lt;h2 id="create-volume-from-snapshot">Create Volume From Snapshot&lt;/h2>
&lt;p>Once you have a bound and ready VolumeSnapshot object, you can use that object to provision a new volume that is pre-populated with data from the snapshot.&lt;/p>
&lt;p>To provision a new volume pre-populated with data from a snapshot, use the &lt;code>dataSource&lt;/code> field in the &lt;code>PersistentVolumeClaim&lt;/code>. It has three parameters:
&lt;code>name&lt;/code> - name of the VolumeSnapshot object representing the snapshot to use as source
&lt;code>kind&lt;/code> - must be VolumeSnapshot
&lt;code>apiGroup&lt;/code> - must be snapshot.storage.k8s.io&lt;/p>
&lt;p>The namespace of the source VolumeSnapshot object is assumed to be the same as the namespace of the &lt;code>PersistentVolumeClaim&lt;/code> object.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pvc-restore&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>demo-namespace&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">storageClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>testdriver.csi.k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">dataSource&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>manually-created-snapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>1Gi&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>When the &lt;code>PersistentVolumeClaim&lt;/code> object is created, it will trigger provisioning of a new volume that is pre-populated with data from the specified snapshot.
As a storage vendor, how do I add support for snapshots to my CSI driver?
To implement the snapshot feature, a CSI driver MUST add support for additional controller capabilities &lt;code>CREATE_DELETE_SNAPSHOT&lt;/code> and &lt;code>LIST_SNAPSHOTS&lt;/code>, and implement additional controller RPCs: &lt;code>CreateSnapshot&lt;/code>, &lt;code>DeleteSnapshot&lt;/code>, and &lt;code>ListSnapshots&lt;/code>. For details, see the &lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">CSI spec&lt;/a> and the &lt;a href="https://kubernetes-csi.github.io/docs/snapshot-restore-feature.html">Kubernetes-CSI Driver Developer Guide&lt;/a>.&lt;/p>
&lt;p>Although Kubernetes is as minimally prescriptive on the packaging and deployment of a CSI Volume Driver as possible, it provides a suggested mechanism for deploying an arbitrary containerized CSI driver on Kubernetes to simplify deployment of containerized CSI compatible volume drivers.&lt;/p>
&lt;p>As part of this recommended deployment process, the Kubernetes team provides a number of sidecar (helper) containers, including the &lt;a href="https://kubernetes-csi.github.io/docs/external-snapshotter.html">external-snapshotter sidecar&lt;/a> container.&lt;/p>
&lt;p>The external-snapshotter watches the Kubernetes API server for VolumeSnapshotContent object and triggers &lt;code>CreateSnapshot&lt;/code> and &lt;code>DeleteSnapshot&lt;/code> operations against a CSI endpoint. The CSI &lt;a href="https://kubernetes-csi.github.io/docs/external-provisioner.html">external-provisioner sidecar container&lt;/a> has also been updated to support restoring volume from snapshot using the dataSource PVC field.&lt;/p>
&lt;p>In order to support snapshot feature, it is recommended that storage vendors deploy the external-snapshotter sidecar containers in addition to the external provisioner, along with their CSI driver.&lt;/p>
&lt;h2 id="what-are-the-limitations-of-beta">What are the limitations of beta?&lt;/h2>
&lt;p>The beta implementation of volume snapshots for Kubernetes has the following limitations:&lt;/p>
&lt;ul>
&lt;li>Does not support reverting an existing volume to an earlier state represented by a snapshot (beta only supports provisioning a new volume from a snapshot).&lt;/li>
&lt;li>No snapshot consistency guarantees beyond any guarantees provided by storage system (e.g. crash consistency). These are the responsibility of higher level APIs/controllers&lt;/li>
&lt;/ul>
&lt;h2 id="what-s-next">What’s next?&lt;/h2>
&lt;p>Depending on feedback and adoption, the Kubernetes team plans to push the CSI Snapshot implementation to GA in either 1.18 or 1.19. Some of the features we are interested in supporting include consistency groups, application consistent snapshots, workload quiescing, in-place restores, volume backups, and more.&lt;/p>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;p>You can also have a look at the &lt;a href="https://github.com/kubernetes-csi/external-snapshotter">external-snapshotter source code repository&lt;/a>.&lt;/p>
&lt;p>Check out additional documentation on the snapshot feature &lt;a href="http://k8s.io/docs/concepts/storage/volume-snapshots">here&lt;/a> and &lt;a href="https://kubernetes-csi.github.io/docs/">here&lt;/a>.&lt;/p>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.&lt;/p>
&lt;p>We offer a huge thank you to the contributors who stepped up these last few quarters to help the project reach Beta:&lt;/p>
&lt;ul>
&lt;li>Xing Yang (xing-yang)&lt;/li>
&lt;li>Xiangqian Yu (yuxiangqian)&lt;/li>
&lt;li>Jing Xu (jingxu97)&lt;/li>
&lt;li>Grant Griffiths (ggriffiths)&lt;/li>
&lt;li>Can Zhu (zhucan)&lt;/li>
&lt;/ul>
&lt;p>With special thanks to the following people for their insightful reviews and thorough consideration with the design:&lt;/p>
&lt;ul>
&lt;li>Michelle Au (msau42)&lt;/li>
&lt;li>Saad Ali (saadali)&lt;/li>
&lt;li>Patrick Ohly (pohly)&lt;/li>
&lt;li>Tim Hockin (thockin)&lt;/li>
&lt;li>Jordan Liggitt (liggitt).&lt;/li>
&lt;/ul>
&lt;p>Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group (SIG)&lt;/a>. We’re rapidly growing and always welcome new contributors.&lt;/p>
&lt;p>We also hold regular &lt;a href="https://docs.google.com/document/d/1qdfvAj5O-tTAZzqJyz3B-yczLLxOiQd-XKpJmTEMazs/edit?usp=sharing">SIG-Storage Snapshot Working Group meetings&lt;/a>. New attendees are welcome to join for design and development discussions.&lt;/p></description></item><item><title>Blog: Kubernetes 1.17 Feature: Kubernetes In-Tree to CSI Volume Migration Moves to Beta</title><link>https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/</link><pubDate>Mon, 09 Dec 2019 09:00:00 +0800</pubDate><guid>https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> David Zhu, Software Engineer, Google&lt;/p>
&lt;p>The Kubernetes in-tree storage plugin to &lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">Container Storage Interface (CSI)&lt;/a> migration infrastructure is now beta in Kubernetes v1.17. CSI migration was introduced as alpha in Kubernetes v1.14.&lt;/p>
&lt;p>Kubernetes features are generally introduced as alpha and moved to beta (and eventually to stable/GA) over subsequent Kubernetes releases. This process allows Kubernetes developers to get feedback, discover and fix issues, iterate on the designs, and deliver high quality, production grade features.&lt;/p>
&lt;h2 id="why-are-we-migrating-in-tree-plugins-to-csi">Why are we migrating in-tree plugins to CSI?&lt;/h2>
&lt;p>Prior to CSI, Kubernetes provided a powerful volume plugin system. These volume plugins were “in-tree” meaning their code was part of the core Kubernetes code and shipped with the core Kubernetes binaries. However, adding support for new volume plugins to Kubernetes was challenging. Vendors that wanted to add support for their storage system to Kubernetes (or even fix a bug in an existing volume plugin) were forced to align with the Kubernetes release process. In addition, third-party storage code caused reliability and security issues in core Kubernetes binaries and the code was often difficult (and in some cases impossible) for Kubernetes maintainers to test and maintain. Using the Container Storage Interface in Kubernetes resolves these major issues.&lt;/p>
&lt;p>As more CSI Drivers were created and became production ready, we wanted all Kubernetes users to reap the benefits of the CSI model. However, we did not want to force users into making workload/configuration changes by breaking the existing generally available storage APIs. The way forward was clear - we would have to replace the backend of the “in-tree plugin” APIs with CSI.&lt;/p>
&lt;h2 id="what-is-csi-migration">What is CSI migration?&lt;/h2>
&lt;p>The CSI migration effort enables the replacement of existing in-tree storage plugins such as &lt;code>kubernetes.io/gce-pd&lt;/code> or &lt;code>kubernetes.io/aws-ebs&lt;/code> with a corresponding &lt;a href="https://kubernetes-csi.github.io/docs/introduction.html">CSI driver&lt;/a>. If CSI Migration is working properly, Kubernetes end users shouldn’t notice a difference. After migration, Kubernetes users may continue to rely on all the functionality of in-tree storage plugins using the existing interface.&lt;/p>
&lt;p>When a Kubernetes cluster administrator updates a cluster to enable CSI migration, existing stateful deployments and workloads continue to function as they always have; however, behind the scenes Kubernetes hands control of all storage management operations (previously targeting in-tree drivers) to CSI drivers.&lt;/p>
&lt;p>The Kubernetes team has worked hard to ensure the stability of storage APIs and for the promise of a smooth upgrade experience. This involves meticulous accounting of all existing features and behaviors to ensure backwards compatibility and API stability. You can think of it like changing the wheels on a racecar while it’s speeding down the straightaway.&lt;/p>
&lt;h2 id="how-to-try-out-csi-migration-for-existing-plugins">How to try out CSI migration for existing plugins?&lt;/h2>
&lt;p>If you are Kubernetes distributor that deploys in one of the environments listed below, now would be a good time to start testing the CSI migration and figuring out how to deploy/manage the appropriate CSI driver.&lt;/p>
&lt;p>To try out CSI migration in beta for an existing plugin you must be using Kubernetes v1.17 or higher. First, you must update/create a Kubernetes cluster with the feature flags &lt;code>CSIMigration&lt;/code> (on by default in 1.17) and &lt;code>CSIMigration{provider}&lt;/code> (off by default) enabled on all Kubernetes components (master and node). Where {provider} is the in-tree cloud provider storage type that is used in your cluster. Please note that during a cluster upgrade you must drain each node (remove running workloads) before updating or changing configuration of your Kubelet. You may also see an optional &lt;code>CSIMigration{provider}Complete&lt;/code> flag that you &lt;em>may&lt;/em> enable if all of your nodes have CSI migration enabled.&lt;/p>
&lt;p>You must also install the requisite CSI driver on your cluster - instructions for this can generally be found from you provider of choice. CSI migration is available for GCE Persistent Disk and AWS Elastic Block Store in beta as well as for Azure File/Disk and Openstack Cinder in alpha. Kubernetes distributors should look at automating the deployment and management (upgrade, downgrade, etc.) of the CSI Drivers they will depend on.&lt;/p>
&lt;p>To verify the feature flag is enabled and driver installed on a particular node you can get the CSINode object. You should see the in-tree plugin name of the migrated plugin as well as your [installed] driver in the drivers list.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get csinodes -o yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">- &lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>CSINode&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">annotations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">storage.alpha.kubernetes.io/migrated-plugins&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kubernetes.io/gce-pd&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-node&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">drivers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pd.csi.storage.gke.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>After the above set up is complete you can confirm that your cluster has functioning CSI migration by deploying a stateful workload using the legacy APIs.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-disk&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">storageClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>standard&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>10Gi&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>---&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Pod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>web-server&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>web-server&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nginx&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/www/html&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>mypvc&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>mypvc&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">persistentVolumeClaim&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">claimName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-disk&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Verify that the pod is RUNNING after some time&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get pods web-server
&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>NAME READY STATUS RESTARTS AGE
web-server 1/1 Running 0 39s
&lt;/code>&lt;/pre>&lt;p>To confirm that the CSI driver is actually serving your requests it may be prudent to check the container logs of the CSI Driver after exercising the storage management operations. Note that your container logs may look different depending on the provider used.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl logs &lt;span style="color:#666">{&lt;/span>CSIdriverPodName&lt;span style="color:#666">}&lt;/span> --container&lt;span style="color:#666">={&lt;/span>CSIdriverContainerName&lt;span style="color:#666">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>/csi.v1.Controller/ControllerPublishVolume called with request: ...
Attaching disk ... to ...
ControllerPublishVolume succeeded for disk ... to instance ...
&lt;/code>&lt;/pre>&lt;h2 id="current-limitations">Current limitations&lt;/h2>
&lt;p>Although CSI migration is now beta there is one major limitation that prevents us from turning it on by default. Turning on migration still requires a cluster administrator to install a CSI driver before storage functionality is seamlessly handed over. We are currently working with SIG-CloudProvider to provide a frictionless experience of bundling the required CSI Drivers with cloud distributions.&lt;/p>
&lt;h2 id="what-is-the-timeline-status">What is the timeline/status?&lt;/h2>
&lt;p>The timeline for CSI migration is actually set by the cloud provider extraction project. It is part of the effort to remove all cloud provider code from Kubernetes. By migrating cloud storage plugins to external CSI drivers we are able to extract out all the cloud provider dependencies.&lt;/p>
&lt;p>Although the overall feature is beta and not on by default, there is still work to be done on a per-plugin basis. Currently only GCE PD and AWS EBS have gone beta with Migration and yet both are still off by default since they depend on a manual installation of their respective CSI Drivers. Azure File/Disk, OpenStack, and VMWare plugins are currently in less mature states and non-cloud plugins such as NFS, Portworx, RBD etc are still in the planning stages.&lt;/p>
&lt;p>The current and targeted releases for each individual cloud driver is shown in the table below:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Driver&lt;/th>
&lt;th>Alpha&lt;/th>
&lt;th>Beta (in-tree deprecated)&lt;/th>
&lt;th>GA&lt;/th>
&lt;th>Target &amp;quot;in-tree plugin&amp;quot; removal&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>AWS EBS&lt;/td>
&lt;td>1.14&lt;/td>
&lt;td>1.17&lt;/td>
&lt;td>1.19 (Target)&lt;/td>
&lt;td>1.21&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GCE PD&lt;/td>
&lt;td>1.14&lt;/td>
&lt;td>1.17&lt;/td>
&lt;td>1.19 (Target)&lt;/td>
&lt;td>1.21&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenStack Cinder&lt;/td>
&lt;td>1.14&lt;/td>
&lt;td>1.18 (Target)&lt;/td>
&lt;td>1.19 (Target)&lt;/td>
&lt;td>1.21&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Azure Disk + File&lt;/td>
&lt;td>1.15&lt;/td>
&lt;td>1.18 (Target)&lt;/td>
&lt;td>1.19 (Target)&lt;/td>
&lt;td>1.21&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>VSphere&lt;/td>
&lt;td>1.18 (Target)&lt;/td>
&lt;td>1.19 (Target)&lt;/td>
&lt;td>1.20 (Target)&lt;/td>
&lt;td>1.22&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="what-s-next">What's next?&lt;/h2>
&lt;p>Major upcoming work includes implementing and hardening CSI migration for the remaining in-tree plugins, installing CSI Drivers by default in distributions, turning on CSI migration by default, and finally removing all in-tree plugin code as a part of cloud provider extraction. We expect to complete this project including the full switch to “on-by-default” migration by Kubernetes v1.21.&lt;/p>
&lt;h2 id="what-should-i-do-as-a-user">What should I do as a user?&lt;/h2>
&lt;p>Note that all new features for the Kubernetes storage system (like volume snapshotting) will only be added to the CSI interface. Therefore, if you are starting up a new cluster, creating stateful applications for the first time, or require these new features we recommend using CSI drivers natively (instead of the in-tree volume plugin API). Follow the &lt;a href="https://kubernetes-csi.github.io/docs/drivers.html">updated user guides for CSI drivers&lt;/a> and use the new CSI APIs.&lt;/p>
&lt;p>However, if you choose to roll a cluster forward or continue using specifications with the legacy volume APIs, CSI Migration will ensure we continue to support those deployments with the new CSI drivers.&lt;/p>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>The Kubernetes Slack channel csi-migration along with any of the standard &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact">SIG Storage communication channels&lt;/a> are great mediums to reach out to the SIG Storage and migration working group teams.&lt;/p>
&lt;p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. We offer a huge thank you to the contributors who stepped up these last quarters to help the project reach Beta:&lt;/p>
&lt;ul>
&lt;li>David Zhu&lt;/li>
&lt;li>Deep Debroy&lt;/li>
&lt;li>Cheng Pan&lt;/li>
&lt;li>Jan Šafránek&lt;/li>
&lt;/ul>
&lt;p>With special thanks to:&lt;/p>
&lt;ul>
&lt;li>Michelle Au&lt;/li>
&lt;li>Saad Ali&lt;/li>
&lt;li>Jonathan Basseri&lt;/li>
&lt;li>Fabio Bertinatto&lt;/li>
&lt;li>Ben Elder&lt;/li>
&lt;li>Andrew Sy Kim&lt;/li>
&lt;li>Hemant Kumar&lt;/li>
&lt;/ul>
&lt;p>For fruitful dialogues, insightful reviews, and thorough consideration of CSI migration in other features.&lt;/p></description></item><item><title>Blog: When you're in the release team, you're family: the Kubernetes 1.16 release interview</title><link>https://kubernetes.io/blog/2019/12/06/when-youre-in-the-release-team-youre-family-the-kubernetes-1.16-release-interview/</link><pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2019/12/06/when-youre-in-the-release-team-youre-family-the-kubernetes-1.16-release-interview/</guid><description>
&lt;p>&lt;b>Author&lt;/b>: Craig Box (Google)&lt;/p>
&lt;p>It is a pleasure to co-host the weekly &lt;a href="https://kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> with Adam Glick. We get to talk to friends old and new from the community, as well as give people a download on the Cloud Native news every week.&lt;/p>
&lt;p>It was also a pleasure to see Lachlan Evenson, the release team lead for Kubernetes 1.16, &lt;a href="https://www.cncf.io/announcement/2019/11/19/cloud-native-computing-foundation-announces-2019-community-awards-winners/">win the CNCF &amp;quot;Top Ambassador&amp;quot; award&lt;/a> at KubeCon. We &lt;a href="https://kubernetespodcast.com/episode/072-kubernetes-1.16/">talked with Lachie&lt;/a> when 1.16 was released, and as is &lt;a href="https://kubernetes.io/blog/2018/07/16/how-the-sausage-is-made-the-kubernetes-1.11-release-interview-from-the-kubernetes-podcast/">becoming&lt;/a> a &lt;a href="https://kubernetes.io/blog/2019/05/13/cat-shirts-and-groundhog-day-the-kubernetes-1.14-release-interview/">tradition&lt;/a>, we are delighted to share an abridged version of that interview with the readers of the Kubernetes Blog.&lt;/p>
&lt;p>If you're paying attention to the release calendar, you'll see 1.17 is due out soon. &lt;a href="https://kubernetespodcast.com/subscribe/">Subscribe to our show&lt;/a> in your favourite podcast player for another release interview!&lt;/p>
&lt;hr/>
&lt;p>&lt;b>CRAIG BOX: Lachie, I've been looking forward to chatting to you for some time. We first met at KubeCon Berlin in 2017 when you were with Deis. Let's start with a question on everyone's ears-- which part of England are you from?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: The prison part! See, we didn't have a choice about going to Australia, but I'd like to say we got the upper hand in the long run. We got that beautiful country, so yes, from Australia, the southern part of England-- the southern tip.&lt;/p>
&lt;p>&lt;b>CRAIG BOX: We did set that question up a little bit. I'm actually in Australia this week, and I'll let you know it's quite a nice place. I can't imagine why you would have left.&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: Yeah, it seems fitting that you're interviewing an Australian from Australia, and that Australian is in San Francisco.&lt;/p>
&lt;p>&lt;b>CRAIG BOX: Oh, well, thank you very much for joining us and making it work. This is the third in our occasional series of release lead interviews. We talked to Josh and Tim from Red Hat and VMware, respectively, in &lt;a href="https://kubernetespodcast.com/episode/010-kubernetes-1.11/">episode 10&lt;/a>, and we talked to Aaron from Google in &lt;a href="https://kubernetespodcast.com/episode/046-kubernetes-1.14/">episode 46&lt;/a>. And we asked all three how their journey in cloud-native started. What was your start in cloud-native?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: I remember back in early 2014, I was working for a company called Lithium Technologies. We'd been using containers for quite some time, and my boss at the time had put a challenge out to me-- go and find a way to orchestrate these containers, because they seem to be providing quite a bit of value to our developer velocity.&lt;/p>
&lt;p>He gave me a week, and he said, go and check out both Mesos and Kubernetes. And at the end of that week, I had Kubernetes up and running, and I had workloads scheduled. I was a little bit more challenged on the Mesos side, but Kubernetes was there, and I had it up and running. And from there, I actually went and was offered to speak at the Kubernetes 1.0 launch in OSCOM in Portland in 2014, I believe.&lt;/p>
&lt;p>&lt;b>CRAIG BOX: So, a real early adopter?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: Really, really early. I remember, I think, I started in 0.8, before CrashLoopBackOff was a thing. I remember writing that thing myself.&lt;/p>
&lt;p>[LAUGHING]&lt;/p>
&lt;p>&lt;b>CRAIG BOX: You were contributing to the code at that point as well?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: I was just a user. I was part of the community at that point, but from a user perspective. I showed up to things like the community meeting. I remember meeting Sarah Novotny in the very early years of the community meeting, and I spent some time in SIG Apps, so really looking at how people were putting workloads onto Kubernetes-- so going through that whole process.&lt;/p>
&lt;p>It turned out we built some tools like Helm, before Helm existed, to facilitate rollout and putting applications onto Kubernetes. And then, once Helm existed, that's when I met the folks from Deis, and I said, hey, I think you want to get rid of this code that we've built internally and then go and use the open-source code that Helm provided.&lt;/p>
&lt;p>So we got into the Helm ecosystem there, and I subsequently went and worked for Deis, specifically on professional services-- helping people out in the community with their Kubernetes journey. And that was when we actually met, Craig, back in Berlin. It seems, you know, I say container years are like dog years; it's 7:1.&lt;/p>
&lt;p>&lt;b>CRAIG BOX: Right.&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: Seven years ago, we were about 50 years-- much younger.&lt;/p>
&lt;p>&lt;b>CRAIG BOX: That sounds like the same ratio as kangaroos to people in Australia.&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: It's much the same arithmetic, yes.&lt;/p>
&lt;p>&lt;b>ADAM GLICK: What was the most interesting implementation that you ran into at that time?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: There wasn't a lot of the workload APIs. Back in 1.0, there wasn't even Deployments. There wasn't Ingress. Back in the day, there were a lot of people in those points trying to build those workload APIs on top of Kubernetes, but they didn't actually have any way to extend Kubernetes itself. There were no third-party resources. There were no operators, no custom resources.&lt;/p>
&lt;p>A lot of people are actually trying to figure out how to interact with the Kubernetes API and deliver things like deployments, because you just had-- in those days, you didn't have replica sets. You had a ReplicationController that we called the RC, back in the day. You didn't have a lot of these things that we take for granted today. There wasn't RBAC. There wasn't a lot of the things that we have today.&lt;/p>
&lt;p>So it's great to have seen and been a part of the Kubernetes community from 0.8 to 1.16, and actually leading that release. So I've seen a lot, and it's been a wonderful part of my adventures in open-source.&lt;/p>
&lt;p>&lt;b>ADAM GLICK: You were also part of the Deis team that transitioned and became a part of the Microsoft team. What was that transition like, from small startup to joining a large player in the cloud and technology community?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: It was fantastic. When we came on board with Microsoft, they didn't have a managed Kubernetes offering, and we were brought on to try and seed that. There was also a bigger part that we were actually building open-source tools to help people in the community integrate. We had the autonomy with-- Brendan Burns was on the team. We had Gabe Monroy. And we really had that top-down autonomy that was believing and placing a bet on open-source and helping us build tools and give us that autonomy to go and solve problems in open-source, along with contributing to things like Kubernetes.&lt;/p>
&lt;p>I'm part of the upstream team from a PM perspective, and we have a bunch of engineers, a bunch of PMs that are actually working on these things in the Cloud Native Compute Foundation to help folks integrate their workloads into things like Kubernetes and build and aid their cloud-native journeys.&lt;/p>
&lt;p>&lt;b>CRAIG BOX: There are a number of new tools, and specifications, and so on that are still coming out from Microsoft under the Deis brand. That must be exciting to you as one of the people who joined from Deis initially.&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: Yeah, absolutely. We really took that Deis brand-- it's now Deis Labs-- but we really wanted this a home to signal to the community that we were building things in the hope to put them out into foundation. You may see things like CNAB, Cloud Native Application Bundles. I know &lt;a href="https://kubernetespodcast.com/episode/061-cnab/">you've had both Ralph and Jeremy on the show before&lt;/a> talking about CNAB, SMI - Service Mesh Interface, other tooling in the ecosystem where we want to signal to the community that we want to go give that to a foundation. We really want a neutral place to begin that nascent work, but then things, for example, Virtual Kubelet started there as well, and it went out into the Cloud Native Compute Foundation.&lt;/p>
&lt;p>&lt;b>ADAM GLICK: Is there any consternation about the fact that Phippy has become the character people look to rather than the actual &amp;quot;Captain Kube&amp;quot; owl, in the &lt;a href="https://www.cncf.io/phippy/">family of donated characters&lt;/a>?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: Yes, so it's interesting because I didn't actually work on that project back at Deis, but the Deis folks, Karen Chu and Matt Butcher actually created &amp;quot;The Children's Guide to Kubernetes,&amp;quot; which I thought was fantastic.&lt;/p>
&lt;p>&lt;b>ADAM GLICK: Totally.&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: Because I could sit down and read it to my parents, as well, and tell them-- it wasn't for children. It was more for the adults in my life, I like to say. And so when I give out a copy of that book, I'm like, take it home and read it to mum. She might actually understand what you do by the end of that book.&lt;/p>
&lt;p>But it was really a creative way, because this was back in that nascent Kubernetes where people were trying to get their head around those concepts-- what is a pod? What is a secret? What is a namespace? Having that vehicle of a fun set of characters--&lt;/p>
&lt;p>&lt;b>ADAM GLICK: Yep.&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: And Phippy is a PHP app. Remember them? So yeah, it's totally in line with the things that we're seeing people want to containerize and put onto Kubernetes at that. But Phippy is still cute. I was questioned last week about Captain Kube, as well, on the release logo, so we could talk about that a little bit more. But there's a swag of characters in there that are quite cute and illustrate the fun concept behind the Kubernetes community.&lt;/p>
&lt;p>&lt;b>CRAIG BOX: &lt;a href="https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/">1.16 has just been released&lt;/a>. You were the release team lead for that-- congratulations.&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: Thank you very much. It was a pleasure to serve the community.&lt;/p>
&lt;p>&lt;b>CRAIG BOX: What are the headline announcements in Kubernetes 1.16?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: Well, I think there are a few. Custom Resources hit GA. Now, that is a big milestone for extensibility and Kubernetes. I know we've spoken about them for some time-- custom resources were introduced in 1.7, and we've been trying to work through that ecosystem to bring the API up to a GA standard. So it hit GA, and I think a lot of the features that went in as part of the GA release will help people in the community that are writing operators.&lt;/p>
&lt;p>There's a lot of lifecycle management, a lot of tooling that you can put into the APIs themselves. Doing strict dependency checks-- you can do typing, you can do validation, you can do pruning superfluous fields, and allowing for that ecosystem of operators and extensibility in the community to exist on top of Kubernetes.&lt;/p>
&lt;p>It's been a long road to get to GA for Custom Resources, but it's great now that they're here and people can really bank on that being an API they can use to extend Kubernetes. So I'd say that's a large headline feature. The metrics overhaul, as well-- I know this was on the release blog.&lt;/p>
&lt;p>The metrics team have actually tried to standardize the metrics in Kubernetes and put them through the same paces as all other enhancements that go into Kubernetes. So they're really trying to put through, what are the criteria? How do we make them standard? How do we test them? How to make sure that they're extensible? So it was great to see that team actually step up and create stable metrics that everybody can build and stack on.&lt;/p>
&lt;p>Finally, there were some other additions to CSI, as well. Volume resizing was added. This is a maturity story around the Container Storage Interface, which was introduced several releases ago in GA. But really, you've seen volume providers actually build on that interface and that interface get a little bit more broader to adopt things like &amp;quot;I want to resize dynamically at runtime on my storage volume&amp;quot;. That's a great story as well, for those providers out there.&lt;/p>
&lt;p>I think they're the big headline features for 1.16, but there are a slew. There were 31 enhancements that went into Kubernetes 1.16. And I know there have been questions out there in the community saying, well, how do we decide what's stable? Eight of those were stable, eight of those were beta, and the rest of those features, the 15 remaining, were actually in alpha. There were quite a few things that went from alpha into beta and beta into stable, so I think that's a good progression for the release, as well.&lt;/p>
&lt;p>&lt;b>ADAM GLICK: As you've looked at all these, which of them is your personal favorite?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: I probably have two. One is a little bit biased, but I personally worked on, with the &lt;a href="https://kubernetes.io/docs/concepts/services-networking/dual-stack/">dual-stack&lt;/a> team in the community. Dual-stack is the ability to give IPv4 and IPv6 addresses to both pods and services. And I think where this is interesting in the community is Kubernetes is becoming a runtime that is going to new spaces. Think IoT, think edge, think cloud edge.&lt;/p>
&lt;p>When you're pushing Kubernetes into these new operational environments, things like addressing may become a problem, where you might want to run thousands and thousands of pods which all need IP addresses. So, having that same crossover point where I can have v4 and v6 at the same time, get comfortable with v6, I think Kubernetes may be an accelerator to v6 adoption through things like IoT workloads on top of Kubernetes.&lt;/p>
&lt;p>The other one is &lt;a href="https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/">Endpoint Slices&lt;/a>. Endpoint slices is about scaling. As you may know, services have endpoints attached to them, and endpoints are all the pod IPs that actually match that label selector on a service. Now, when you have large clusters, you can imagine the number of pod IPs being attached to that service growing to tens of thousands. And when you update that, everything that actually watches those service endpoints needs to get an update, which is the delta change over time, which gets rather large as things are being attached, added, and removed, as is the dynamic nature of Kubernetes.&lt;/p>
&lt;p>But what endpoint slices makes available is you can actually slice those endpoints up into groups of 100 and then only update the ones that you really need to worry about, which means as a scaling factor, we don't need to update everybody listening into tens of thousands of updates. We only need to update a subsection. So I'd say they're my two highlights, yeah.&lt;/p>
&lt;p>&lt;b>CRAIG BOX: Are there any early stage or alpha features that you're excited to see where they go personally?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: Personally, &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/">ephemeral containers&lt;/a>. The tooling that you have available at runtime in a pod is dependent on the constituents or the containers that are part of that pod. And what we've seen in containers being built by scratch and tools like &lt;a href="https://github.com/GoogleContainerTools/distroless">distroless&lt;/a> from the folks out of Google, where you can build scratch containers that don't actually have any tooling inside them but just the raw compiled binaries, if you want to go in and debug that at runtime, it's incredibly difficult to insert something in.&lt;/p>
&lt;p>And this is where ephemeral containers come in. I can actually insert a container into a running pod-- and let's just call that a debug container-- that has all my slew of tools that I need to debug that running workload, and I can insert that into a pod at runtime. So I think ephemeral containers is a really interesting feature that's been included in 1.16 in alpha, which allows a greater debugging story for the Kubernetes community.&lt;/p>
&lt;p>&lt;b>ADAM GLICK: What feature that slipped do you wish would have made it into the release?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: The feature that slipped that I was a little disappointed about was &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/sidecarcontainers.md">sidecar containers&lt;/a>.&lt;/p>
&lt;p>&lt;b>ADAM GLICK: Right.&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: In the world of service meshes, you may want to order the start of some containers, and it's very specific to things like service meshes in the case of the data plane. I need the Envoy sidecar to start before everything else so that it can wire up the networking.&lt;/p>
&lt;p>The inverse is true as well. I need it to stop last. Sidecar containers gave you that ordered start. And what we see a lot of people doing in the ecosystem is just laying down one sidecar per node as a DaemonSet, and they want that to start before all the other pods on the machine. Or if it's inside the pod, or the context of one pod, they want to say that sidecar needs to stop before all the other containers in a pod. So giving you that ordered guarantee, I think, is really interesting and is really hot, especially given the service mesh ecosystem heating up.&lt;/p>
&lt;p>&lt;b>CRAIG BOX: This release &lt;a href="https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/">deprecates a few beta API groups&lt;/a>, for things like ReplicaSets and Deployments. That will break deployment for the group of people who have just taken example code off the web and don't really understand it. The GA version of these APIs were released in 1.9, so it's obviously a long time ago. There's been a lot of preparation going into this. But what considerations and concerns have we had about the fact that these are now being deprecated in this particular release?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: Let me start by saying that this is the first release that we've had a big API deprecation, so the proof is going to be in the pudding. And we do have an API deprecation policy. So as you mentioned, Craig, the apps/v1 group has been around since 1.9. If you go and read the &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">API deprecation policy&lt;/a>, you can see that we have a three-release announcement. Around the 1.12, 1.13 time frame, we actually went and announced this deprecation, and over the last few releases, we've been reiterating that.&lt;/p>
&lt;p>But really, what we want to do is get the whole community on those stable APIs because it really starts to become a problem when we're supporting all these many now-deprecated APIs, and people are building tooling around them and trying to build reliable tooling. So this is the first test for us to move people, and I'm sure it will break a lot of tools that depend on things. But I think in the long run, once we get onto those stable APIs, people can actually guarantee that their tools work, and it's going to become easier in the long run.&lt;/p>
&lt;p>So we've put quite a bit of work in announcing this. There was a blog sent out about six months ago by Valerie Lancey in the Kubernetes community which said, hey, go use 'kubectl convert', where you can actually say, I want to convert this resource from this API version to that API version, and it actually makes that really easy. But I think there'll be some problems in the ecosystem, but we need to do this going forward, pruning out the old APIs and making sure that people are on the stable ones.&lt;/p>
&lt;p>&lt;b>ADAM GLICK: Congratulations on the release of 1.16. Obviously, that's a big thing. It must have been a lot of work for you. Can you talk a little bit about what went into leading this release?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: The job of the release lead is to oversee throughout the process of the release and make sure that the release gets out the door on a specific schedule. So really, what that is is wrangling a lot of different resources and a lot of different people in the community, and making sure that they show up and do the things that they are committed to as part of their duties as either SIG chairs or other roles in the community, and making sure that enhancements are in the right state, and code shows up at the right time, and that things are looking green.&lt;/p>
&lt;p>A lot of it is just making sure you know who to contact and how to contact them, and ask them to actually show up. But when I was asked at the end of the 1.15 release cycle if I would lead, you have to consider how much time it's going to take and the scheduling, where hours a week are dedicated to making sure that this release actually hits the shelves on time and is of a certain quality. So there is lots of pieces to that.&lt;/p>
&lt;p>&lt;b>ADAM GLICK: Had you been on the path through the shadow program for release management?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: Yeah, I had. I actually joined the shadow program-- so the shadow program for the release team. The Kubernetes release team is tasked with staffing a specific release, and I came in the 1.14 release under the lead of Aaron Crickenberger. And I was an enhancement shadow at that point. I was really interested in how KEPs worked, so the Kubernetes Enhancement Proposal work. I wanted to make sure that I understood that part of the release team, and I came in and helped in that release.&lt;/p>
&lt;p>And then, in 1.15, I was asked if I could be a lead shadow. And the lead shadow is to stand alongside the lead and help the lead fill their duties. So if they're out, if they need people to wrangle different parts of the community, I would go out and do that. I've served on three releases at this point-- 1.14, 1.15, and 1.16.&lt;/p>
&lt;p>&lt;b>CRAIG BOX: Thank you for your service.&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: Absolutely, it's my pleasure.&lt;/p>
&lt;p>&lt;b>ADAM GLICK: Release lead emeritus is the next role for you, I assume?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: [LAUGHS] Yes. We also have a new role on the release lead team called Emeritus Advisors, which are actually to go back and help answer the questions of, why was this decision made? How can we do better? What was this like in the previous release? So we do have that continuity, and in 1.17, we have the old release lead from 1.15. Claire Lawrence is coming back to fill in as emeritus advisor. So that is something we do take.&lt;/p>
&lt;p>And I think for the shadow program in general, the release team is a really good example of how you can actually build continuity across releases in an open-source fashion. We &lt;a href="https://www.youtube.com/watch?v=ritHCLd2xeE">actually have a session at KubeCon San Diego&lt;/a> on how that shadowing program works. But it's really to get people excited about how we can do mentoring in open-source communities and make sure that the project goes on after all of us have rolled on and off the team.&lt;/p>
&lt;p>&lt;b>ADAM GLICK: Speaking of the team, &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.16">there were 32 people involved&lt;/a>, including yourself, in this release. What is it like to coordinate that group? That sounds like a full time job.&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: It is a full time job. And let me say that this release team in 1.16 represented five different continents. We can count Antarctica as not having anybody, but we didn't have anybody from South America for that release, which was unfortunate. But we had people from Australia, China, India, Tanzania. We have a good spread-- Europe, North America. It's great to have that spread and that continuity, which allowed for us to get things done throughout the day.&lt;/p>
&lt;p>&lt;b>CRAIG BOX: Until you want to schedule a meeting.&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: Scheduling a meeting was extremely difficult. Typically, on the release team, we run one Europe, Western Europe, and North American-friendly meeting, and then we ask the team if they would like to hold another meeting. Now, in the case of 1.16, they didn't want to hold another meeting. We actually put it out to survey. But in previous releases, we held an EU in the morning so that people in India, as well, or maybe even late-night in China, could be involved.&lt;/p>
&lt;p>&lt;b>ADAM GLICK: Any interesting facts about the team, besides the incredible geographic diversity that you had, to work around that?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: I really appreciate about the release team that we're from all different backgrounds, from all different parts of the world and all different companies. There are people who are doing this on their own time, There are people who are doing this on company time, but we all come together with that shared common goal of shipping that release.&lt;/p>
&lt;p>This release was we had the five continents. It was really exciting in 1.17 that we have in the lead roles, it was represented mainly by women. So 1.17, watch out-- most of the leads for 1.17 are women, which is a great result, and that's through that shadow program that we can foster different types of talent. I'm excited to see future releases benefiting from different diverse groups of people from the Kubernetes community.&lt;/p>
&lt;p>&lt;b>CRAIG BOX: What are you going to put in the proverbial envelope for the 1.17 team?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: We've had this theme of a lot of roles in the release team being cut and dry, right? We have these release handbooks, so for each of the members of the team, they're cut into different roles. There's seven different roles on the team. There's the lead. There's the CI signal role. There's bug triage. There's comms. There's docs. And there's release notes. And there's also the release branch managers who actually cut the code and make sure that they have shipped and it ends up in all the repositories.&lt;/p>
&lt;p>What we did in the previous 1.15, we actually had a role call the test-infra role. And thanks to the wonderful work of the folks of the test-infra team out of Google-- &lt;a href="https://kubernetespodcast.com/episode/077-eng-prod-and-testing/">Katharine Berry&lt;/a>, and &lt;a href="https://kubernetespodcast.com/episode/069-kind/">Ben Elder&lt;/a>, and other folks-- they actually automated this role completely that we could get rid of it in the 1.16 release and still have our same-- and be able to get a release out the door.&lt;/p>
&lt;p>I think a lot of these things are ripe for automation, and therefore, we can have a lot less of a footprint going forward. Let's automate the bits of the process that we can and actually refine the process to make sure that the people that are involved are not doing the repetitive tasks over and over again. In the era of enhancements, we could streamline that process. CI signal and bug triage, there are places we could actually go in and automate that as well. I think one place that's been done really well in 1.16 was in the release notes.&lt;/p>
&lt;p>I don't know if you've seen &lt;a href="https://relnotes.k8s.io">relnotes.k8s.io&lt;/a>, but you can go and check out the release notes and now, basically, annotated PRs show up as release notes that are searchable and sortable, all through an automated means, whereas that was previously some YAML jockeying to make sure that that would actually happen and be digestible to the users.&lt;/p>
&lt;p>&lt;b>CRAIG BOX: Come on, Lachie, all Kubernetes is just YAML jockeying.&lt;/b>&lt;/p>
&lt;p>[LAUGHING]&lt;/p>
&lt;p>LACHLAN EVENSON: Yeah, but it's great to have an outcome where we can actually make that searchable and get people out of the mundaneness of things like, let's make sure we're copying and pasting YAML from left to right.&lt;/p>
&lt;p>&lt;b>ADAM GLICK: After the release, you had a &lt;a href="https://docs.google.com/document/d/1VQDIAB0OqiSjIHI8AWMvSdceWhnz56jNpZrLs6o7NJY/edit#heading=h.ipohe1hgr315">retrospective meeting&lt;/a>. What was the takeaway from that meeting?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: At the end of each release, we do have a retrospective. It's during the community meeting. That retrospective, it was good. I was just really excited to see that there were so many positives. It's a typical retrospective where we go, what did we say we were going to do last release? Did we do that? What was great? What can we do better? And some actions out of that.&lt;/p>
&lt;p>It was great to see people giving other people on the team so many compliments. It was really, really deep and rich, saying, thank you for doing this, thank you for doing that. People showed up and pulled their weight in the release team, and other people were acknowledging that. That was great.&lt;/p>
&lt;p>I think one thing we want to do is-- we have a code freeze as part of the release process, which is where we make sure that code basically stops going into master in Kubernetes. Only things destined for the release can actually be put in there. But we don't actually stop the test infrastructure from changing, so the test infrastructure has a lifecycle of its own.&lt;/p>
&lt;p>One of the things that was proposed was that we actually code freeze the test infrastructure as well, to make sure that we're not actually looking at changes in the test-infra causing jobs to fail while we're trying to stabilize the code. I think that's something we have some high level agreement about, but getting down into the low-level nitty-gritty would be great in 1.17 and beyond.&lt;/p>
&lt;p>&lt;b>ADAM GLICK: We talked about sidecar containers slipping out of this release. Most of the features are on a release train, and are put in when they're ready. What does it mean for the process of managing a release when those things happen?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: Basically, we have an enhancements freeze, and that says that enhancements-- so the KEPs that are backing these enhancements-- so the sidecar containers would have had an enhancement proposal. And the SIG that owns that code would then need to sign off and say that this is in a state called &amp;quot;implementable.&amp;quot; When we've agreed on the high-level details, you can go and proceed and implement that.&lt;/p>
&lt;p>Now, that had actually happened in the case of sidecar containers. The challenge was you still need to write the code and get the code actually implemented, and there's a month gap between enhancement freeze and code freeze. If the code doesn't show up, or the code shows up and needs to be reviewed a little bit more, you may miss that deadline.&lt;/p>
&lt;p>I think that's what happened in the case of this specific feature. It went all the way through to code freeze, the code wasn't complete at that time, and we basically had to make a call-- do we want to grant it an exception? In this case, they didn't ask for an exception. They said, let's just move it to 1.17.&lt;/p>
&lt;p>There's still a lot of people and SIGs show up at the start of a new release and put forward the whole release of all the things they want to ship, and obviously, throughout the release, a lot of those things get plucked off. I think we started with something like 60 enhancements, and then what we got out the door was 31. They either fall off as part of the enhancement freeze or as part of the code freeze, and that is absolutely typical of any release.&lt;/p>
&lt;p>&lt;b>ADAM GLICK: Do you think that a three-month wait is acceptable for something that might have had a one- or two-week slip, or would you like to see enhancements be able to be released in point releases between the three-month releases?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: Yeah, there's back and forth about this in the community, about how can we actually roll things at different cadences, I think, is the high-level question. Tim Hockin actually put out, how about we do stability cycles as well? Because there are a lot of new features going in, and there are a lot of stability features going in. But if you look at it, half of the features were beta or stable, and the other half were alpha, which means we're still introducing a lot more complexity and largely untested code into alpha state-- which, as much as we wouldn't like to admit, it does affect the stability of the system.&lt;/p>
&lt;p>There's talk of LTS. There's talk of stability releases as well. I think they're all things that are interesting now that Kubernetes has that momentum, and you are seeing a lot of things go to GA. People are like, &amp;quot;I don't need to be drinking from the firehose as fast. I have CRDs in GA. I have all these other things in GA. Do I actually need to consume this at the rate?&amp;quot; So I think-- stay tuned. If you're interested in those discussions, the upstream community is having those. Show up there and voice your opinion.&lt;/p>
&lt;p>&lt;b>CRAIG BOX: Is this the first release with its own &lt;a href="https://raw.githubusercontent.com/kubernetes/sig-release/master/releases/release-1.16/116_unlimited_breadsticks_for_all.png">release mascot&lt;/a>?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: I think that release mascot goes back to-- I would like to say 1.11? If you &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.11/README.md">go back to 1.11&lt;/a>, you can actually see the different mascots. I remember 1.11 being &amp;quot;The Hobbit.&amp;quot; So it's the Hobbiton front door of Bilbo Baggins with the Kubernetes Helm on the front of it, and that was called 11ty-one--&lt;/p>
&lt;p>&lt;b>CRAIG BOX: Uh-huh.&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: A long-expected release. So they go through from each release, and you can actually go check them out on the SIG release repository upstream.&lt;/p>
&lt;p>&lt;b>CRAIG BOX: I do think this is the first time that's managed to make it into a blog post, though.&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: I do think it is the case. I wanted to have a little bit of fun with the release team, so typically you will see the release teams have a t-shirt. I have, &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.14/README.md">from 1.14, the Caternetes&lt;/a>, which Aaron designed, which has a bunch of cats kind of trying to look at a Kubernetes logo.&lt;/p>
&lt;p>&lt;b>CRAIG BOX: We had a fun conversation with Aaron about his love of cats.&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: [LAUGHS] And it becomes a token of, hey, remember this hard work that you put together? It becomes a badge of honor for everybody that participated in the release. I wanted to highlight it as a release mascot. I don't think a lot of people knew that we did have those across the last few releases. But it's just a bit of fun, and I wanted to put my own spin on things just so that the team could come together. A lot of it was around the laughs that we had as a team throughout this release-- and my love of Olive Garden.&lt;/p>
&lt;p>&lt;b>CRAIG BOX: Your love of Olive Garden feels like it may have become a meme to a community which might need a little explanation for our audience. For those who are not familiar with American fine dining, can we start with-- what exactly is Olive Garden?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: Olive Garden is the finest Italian dining experience you will have in the continental United States. I see everybody's faces saying, is he sure about that? I'm sure.&lt;/p>
&lt;p>&lt;b>CRAIG BOX: That might require a slight justification on behalf of some of our Italian-American listeners.&lt;/b>&lt;/p>
&lt;p>&lt;b>ADAM GLICK: Is it the unlimited breadsticks and salad that really does it for you, or is the plastic boat that it comes in?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: I think it's a combination of all three things. You know, the tour of Italy, you can't go past. The free breadsticks are fantastic. But Olive Garden just represents the large chain restaurant and that kind of childhood I had growing up and thinking about these large-scale chain restaurants. You don't get to choose your meme. And the legacy-- I would have liked to have had a different mascot.&lt;/p>
&lt;p>But I just had a run with the meme of Olive Garden. And this came about, I would like to say, about three or four months ago. Paris Pittman from Google, who is another member of the Kubernetes community, kind of put out there, what's your favorite sit-down large-scale restaurant? And of course, I pitched in very early and said, it's got to be the Olive Garden.&lt;/p>
&lt;p>And then everybody kind of jumped onto that. And my inbox is full of free Olive Garden gift certificates now, and it's taken on a life of its own. And at this point, I'm just embracing it-- so much so that we might even have the 1.16 release party at an Olive Garden in San Diego, if it can accommodate 10,000 people.&lt;/p>
&lt;p>&lt;b>ADAM GLICK: &lt;a href="https://www.youtube.com/watch?v=9ZJF5-EyjXs">When you're there, are you family?&lt;/a>&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: Yes. Absolutely, absolutely. And I would have loved to put that. I think the release name was &amp;quot;unlimited breadsticks for all.&amp;quot; I would have liked to have done, &amp;quot;When you're here, you're family,&amp;quot; but that is, sadly, trademarked.&lt;/p>
&lt;p>&lt;b>ADAM GLICK: Aww. What's next for you in the community?&lt;/b>&lt;/p>
&lt;p>LACHLAN EVENSON: I've really been looking at Cluster API a lot-- so building Kubernetes clusters on top of a declarative approach. I've been taking a look at what we can do in the Cluster API ecosystem. I'm also a chair of SIG PM, so helping foster the KEP process as well-- making sure that that continues to happen and continues to be fruitful for the community.&lt;/p>
&lt;hr/>
&lt;p>&lt;i>&lt;a href="https://twitter.com/lachlanevenson">Lachlan Evenson&lt;/a> is a Principal Program Manager at Microsoft and an Australian living in the US, and most recently served as the Kubernetes 1.16 release team lead.&lt;/p>
&lt;p>You can find the &lt;a href="http://www.kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> at &lt;a href="https://twitter.com/KubernetesPod">@kubernetespod&lt;/a> on Twitter, and you can &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe&lt;/a> so you never miss an episode.&lt;/i>&lt;/p></description></item><item><title>Blog: Gardener Project Update</title><link>https://kubernetes.io/blog/2019/12/02/gardener-project-update/</link><pubDate>Mon, 02 Dec 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2019/12/02/gardener-project-update/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> &lt;a href="mailto:rafael.franzke@sap.com">Rafael Franzke&lt;/a> (SAP), &lt;a href="mailto:vasu.chandrasekhara@sap.com">Vasu
Chandrasekhara&lt;/a> (SAP)&lt;/p>
&lt;p>Last year, we introduced &lt;a href="https://gardener.cloud">Gardener&lt;/a> in the &lt;a href="https://www.youtube.com/watch?v=DpFTcTnBxbM&amp;amp;feature=youtu.be&amp;amp;t=1642">Kubernetes
Community
Meeting&lt;/a>
and in a post on the &lt;a href="https://kubernetes.io/blog/2018/05/17/gardener/">Kubernetes
Blog&lt;/a>. At SAP, we have been
running Gardener for more than two years, and are successfully managing
thousands of &lt;a href="https://k8s-testgrid.appspot.com/conformance-gardener">conformant&lt;/a>
clusters in various versions on all major hyperscalers as well as in numerous
infrastructures and private clouds that typically join an enterprise via
acquisitions.&lt;/p>
&lt;p>We are often asked why a handful of dynamically scalable clusters would not
suffice. We also started our journey into Kubernetes with a similar mindset. But
we realized that applying the architecture and principles of Kubernetes to
productive scenarios, our internal and external customers very quickly required
the rational separation of concerns and ownership, which in most circumstances
led to the use of multiple clusters. Therefore, a scalable and managed
Kubernetes as a service solution is often also the basis for adoption.
Particularly, when a larger organization runs multiple products on different
providers and in different regions, the number of clusters will quickly rise to
the hundreds or even thousands.&lt;/p>
&lt;p>Today, we want to give an update on what we have implemented in the past year
regarding extensibility and customizability, and what we plan to work on for our
next milestone.&lt;/p>
&lt;h2 id="short-recap-what-is-gardener">Short Recap: What Is Gardener?&lt;/h2>
&lt;p>Gardener's main principle is to leverage Kubernetes primitives for all of its
operations, commonly described as inception or kubeception. The feedback from
the community was that initially our &lt;a href="https://github.com/gardener/documentation/wiki/Architecture">architecture
diagram&lt;/a> looks
&amp;quot;overwhelming&amp;quot;, but after some little digging into the material, everything we
do is the &amp;quot;Kubernetes way&amp;quot;. One can re-use all learnings with respect to APIs,
control loops, etc.&lt;br>
The essential idea is that so-called &lt;strong>seed&lt;/strong> clusters are used to host the
control planes of end-user clusters (botanically named &lt;strong>shoots&lt;/strong>).&lt;br>
Gardener provides vanilla Kubernetes clusters as a service independent of the
underlying infrastructure provider in a homogenous way, utilizing the upstream
provided &lt;code>k8s.gcr.io/*&lt;/code> images as open distribution. The project is built
entirely on top of Kubernetes extension concepts, and as such adds a custom API
server, a controller-manager, and a scheduler to create and manage the lifecycle
of Kubernetes clusters. It extends the Kubernetes API with custom resources,
most prominently the Gardener cluster specification (&lt;code>Shoot&lt;/code> resource), that can
be used to &amp;quot;order&amp;quot; a Kubernetes cluster in a declarative way (for day-1, but
also reconcile all management activities for day-2).&lt;/p>
&lt;p>By leveraging Kubernetes as base infrastructure, we were able to devise a
combined &lt;a href="https://github.com/gardener/hvpa-controller">Horizontal and Vertical Pod Autoscaler
(HVPA)&lt;/a> that, when configured with
custom heuristics, scales all control plane components up/down or out/in
automatically. This enables a fast scale-out, even beyond the capacity of
typically some fixed number of master nodes. This architectural feature is one
of the main differences compared to many other Kubernetes cluster provisioning
tools. But in our production, Gardener does not only effectively reduce the
total costs of ownership by bin-packing control planes. It also simplifies
implementation of &amp;quot;day-2 operations&amp;quot; (like cluster updates or robustness
qualities). Again, essentially by relying on all the mature Kubernetes features
and capabilities.&lt;/p>
&lt;p>The newly introduced extension concepts for Gardener now enable providers to
only maintain their specific extension without the necessity to develop inside
the core source tree.&lt;/p>
&lt;h2 id="extensibility">Extensibility&lt;/h2>
&lt;p>As result of its growth over the past years, the Kubernetes code base contained
a numerous amount of provider-specific code that is now being externalized from
its core source tree. The same has happened with Project Gardener: over time,
lots of specifics for cloud providers, operating systems, network plugins, etc.
have been accumulated. Generally, this leads to a significant increase of
efforts when it comes to maintainability, testability, or to new releases. Our
community member &lt;a href="https://www.packet.com">Packet&lt;/a> contributed &lt;a href="https://www.packet.com/kubernetes/">Gardener
support&lt;/a> for their infrastructure in-tree,
and suffered from the mentioned downsides.&lt;/p>
&lt;p>Consequently, similar to how the Kubernetes community decided to move their
cloud-controller-managers out-of-tree, or volumes plugins to CSI, etc., the
Gardener community
&lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md">proposed&lt;/a>
and implemented likewise extension concepts. The Gardener core source-tree is
now devoid of any provider specifics, allowing vendors to solely focus on their
infrastructure specifics, and enabling core contributors becoming more agile
again.&lt;/p>
&lt;p>Typically, setting up a cluster requires a flow of interdependent steps,
beginning with the generation of certificates and preparation of the
infrastructure, continuing with the provisioning of the control plane and the
worker nodes, and ending with the deployment of system components. We would like
to emphasize here that all these steps are necessary (cf. &lt;a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes the Hard
Way&lt;/a>) and all
Kubernetes cluster creation tools implement the same steps (automated to some
degree) in one way or another.&lt;/p>
&lt;p>The general idea of Gardener's extensibility concept was to make &lt;a href="https://github.com/gardener/gardener/blob/0.31.1/pkg/controllermanager/controller/shoot/shoot_control_reconcile.go#L69-L298">this
flow&lt;/a>
more generic and to carve out custom resources for each step which can serve as
ideal extension points.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-10-gardener-project-update/flow.png"
alt="Cluster reconciliation flow with extension points"/>
&lt;/figure>
&lt;p>&lt;em>Figure 1 Cluster reconciliation flow with extension points.&lt;/em>&lt;/p>
&lt;p>With Gardener's flow framework we implicitly have a reproducible state machine
for all infrastructures and all possible states of a cluster.&lt;/p>
&lt;p>The Gardener extensibility approach defines custom resources that serve as ideal
extension points for the following categories:&lt;/p>
&lt;ul>
&lt;li>DNS providers (e.g., Route53, CloudDNS, ...),&lt;/li>
&lt;li>Blob storage providers (e.g., S3, GCS, ABS,...),&lt;/li>
&lt;li>Infrastructure providers (e.g., AWS, GCP, Azure, ...),&lt;/li>
&lt;li>Operating systems (e.g., CoreOS Container Linux, Ubuntu, FlatCar Linux, ...),&lt;/li>
&lt;li>Network plugins (e.g., Calico, Flannel, Cilium, ...),&lt;/li>
&lt;li>Non-essential extensions (e.g., Let's Encrypt certificate service).&lt;/li>
&lt;/ul>
&lt;h3 id="extension-points">Extension Points&lt;/h3>
&lt;p>Besides leveraging custom resource definitions, we also effectively use mutating
/ validating webhooks in the seed clusters. Extension controllers themselves run
in these clusters and react on CRDs and workload resources (like &lt;code>Deployment&lt;/code>,
&lt;code>StatefulSet&lt;/code>, etc.) they are responsible for. Similar to the &lt;a href="https://cluster-api.sigs.k8s.io">Cluster
API&lt;/a>'s approach, these CRDs may also contain
provider specific information.&lt;/p>
&lt;p>The steps 2. - 10. [cf. Figure 1] involve infrastructure specific meta data
referring to infrastructure specific implementations, e.g. for DNS records there
might be &lt;code>aws-route53&lt;/code>, &lt;code>google-clouddns&lt;/code>, or for isolated networks even
&lt;code>openstack-designate&lt;/code>, and many more. We are going to examine the steps 4 and 6
in the next paragraphs as examples for the general concepts (based on the
implementation for AWS). If you're interested you can read up the fully
documented API contract in our &lt;a href="https://github.com/gardener/gardener/tree/master/docs/extensions">extensibility
documents&lt;/a>.&lt;/p>
&lt;h3 id="example-infrastructure-crd">Example: &lt;code>Infrastructure&lt;/code> CRD&lt;/h3>
&lt;p>Kubernetes clusters on AWS require a certain infrastructure preparation before
they can be used. This includes, for example, the creation of a VPC, subnets,
etc. The purpose of the &lt;code>Infrastructure&lt;/code> CRD is to trigger this preparation:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">apiVersion:&lt;span style="color:#bbb"> &lt;/span>extensions.gardener.cloud/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>kind:&lt;span style="color:#bbb"> &lt;/span>Infrastructure&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>name:&lt;span style="color:#bbb"> &lt;/span>infrastructure&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>namespace:&lt;span style="color:#bbb"> &lt;/span>shoot--foobar--aws&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>type:&lt;span style="color:#bbb"> &lt;/span>aws&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>region:&lt;span style="color:#bbb"> &lt;/span>eu-west&lt;span style="color:#666">-1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secretRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">    &lt;/span>name:&lt;span style="color:#bbb"> &lt;/span>cloudprovider&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">    &lt;/span>namespace:&lt;span style="color:#bbb"> &lt;/span>shoot--foobar—aws&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>sshPublicKey:&lt;span style="color:#bbb"> &lt;/span>c3NoLXJzYSBBQUFBQ...&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>&lt;span style="color:#a2f;font-weight:bold">providerConfig&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">    &lt;/span>apiVersion:&lt;span style="color:#bbb"> &lt;/span>aws.provider.extensions.gardener.cloud/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">    &lt;/span>kind:&lt;span style="color:#bbb"> &lt;/span>InfrastructureConfig&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">    &lt;/span>&lt;span style="color:#a2f;font-weight:bold">networks&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">      &lt;/span>&lt;span style="color:#a2f;font-weight:bold">vpc&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">        &lt;/span>cidr:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">10.250.0.0&lt;/span>/&lt;span style="color:#666">16&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">      &lt;/span>&lt;span style="color:#a2f;font-weight:bold">zones&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">      &lt;/span>-&lt;span style="color:#bbb"> &lt;/span>name:&lt;span style="color:#bbb"> &lt;/span>eu-west-1a&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">        &lt;/span>internal:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">10.250.112.0&lt;/span>/&lt;span style="color:#666">22&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">        &lt;/span>public:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">10.250.96.0&lt;/span>/&lt;span style="color:#666">22&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">        &lt;/span>workers:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">10.250.0.0&lt;/span>/&lt;span style="color:#666">19&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Based on the &lt;code>Shoot&lt;/code> resource, Gardener creates this &lt;code>Infrastructure&lt;/code> resource
as part of its reconciliation flow. The AWS-specific &lt;code>providerConfig&lt;/code> is part of
the end-user's configuration in the &lt;code>Shoot&lt;/code> resource and not evaluated by
Gardener but just passed to the extension controller in the seed cluster.&lt;/p>
&lt;p>In its current implementation, the AWS extension creates a new VPC and three
subnets in the &lt;code>eu-west-1a&lt;/code> zones. Also, it creates a NAT and an internet
gateway, elastic IPs, routing tables, security groups, IAM roles, instances
profiles, and an EC2 key pair.&lt;/p>
&lt;p>After it has completed its tasks it will report the status and some
provider-specific output:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">apiVersion:&lt;span style="color:#bbb"> &lt;/span>extensions.gardener.cloud/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>kind:&lt;span style="color:#bbb"> &lt;/span>Infrastructure&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>name:&lt;span style="color:#bbb"> &lt;/span>infrastructure&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>namespace:&lt;span style="color:#bbb"> &lt;/span>shoot--foobar--aws&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">status&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">lastOperation&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Reconcile&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">state&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Succeeded&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>&lt;span style="color:#a2f;font-weight:bold">providerStatus&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">    &lt;/span>apiVersion:&lt;span style="color:#bbb"> &lt;/span>aws.provider.extensions.gardener.cloud/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">    &lt;/span>kind:&lt;span style="color:#bbb"> &lt;/span>InfrastructureStatus&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">    &lt;/span>&lt;span style="color:#a2f;font-weight:bold">ec2&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">      &lt;/span>keyName:&lt;span style="color:#bbb"> &lt;/span>shoot--foobar--aws-ssh-publickey&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">    &lt;/span>&lt;span style="color:#a2f;font-weight:bold">iam&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">      &lt;/span>&lt;span style="color:#a2f;font-weight:bold">instanceProfiles&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">      &lt;/span>-&lt;span style="color:#bbb"> &lt;/span>name:&lt;span style="color:#bbb"> &lt;/span>shoot--foobar--aws-nodes&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">        &lt;/span>purpose:&lt;span style="color:#bbb"> &lt;/span>nodes&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">      &lt;/span>&lt;span style="color:#a2f;font-weight:bold">roles&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">      &lt;/span>-&lt;span style="color:#bbb"> &lt;/span>arn:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;arn:aws:iam::&amp;lt;accountID&amp;gt;:role/shoot...&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">        &lt;/span>purpose:&lt;span style="color:#bbb"> &lt;/span>nodes&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">    &lt;/span>&lt;span style="color:#a2f;font-weight:bold">vpc&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">      &lt;/span>id:&lt;span style="color:#bbb"> &lt;/span>vpc&lt;span style="color:#666">-0815&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">      &lt;/span>&lt;span style="color:#a2f;font-weight:bold">securityGroups&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">      &lt;/span>-&lt;span style="color:#bbb"> &lt;/span>id:&lt;span style="color:#bbb"> &lt;/span>sg&lt;span style="color:#666">-0246&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">        &lt;/span>purpose:&lt;span style="color:#bbb"> &lt;/span>nodes&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">      &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subnets&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">      &lt;/span>-&lt;span style="color:#bbb"> &lt;/span>id:&lt;span style="color:#bbb"> &lt;/span>subnet&lt;span style="color:#666">-1234&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">        &lt;/span>purpose:&lt;span style="color:#bbb"> &lt;/span>nodes&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">        &lt;/span>zone:&lt;span style="color:#bbb"> &lt;/span>eu-west-1b&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">      &lt;/span>-&lt;span style="color:#bbb"> &lt;/span>id:&lt;span style="color:#bbb"> &lt;/span>subnet&lt;span style="color:#666">-5678&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">        &lt;/span>purpose:&lt;span style="color:#bbb"> &lt;/span>public&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">        &lt;/span>zone:&lt;span style="color:#bbb"> &lt;/span>eu-west-1b&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The information inside the &lt;code>providerStatus&lt;/code> can be used in subsequent steps,
e.g. to configure the cloud-controller-manager or to instrument the
machine-controller-manager.&lt;/p>
&lt;h3 id="example-deployment-of-the-cluster-control-plane">Example: Deployment of the Cluster Control Plane&lt;/h3>
&lt;p>One of the major features of Gardener is the homogeneity of the clusters it
manages across different infrastructures. Consequently, it is still in charge of
deploying the provider-independent control plane components into the seed
cluster (like etcd, kube-apiserver). The deployment of provider-specific control
plane components like cloud-controller-manager or CSI controllers is triggered
by a dedicated &lt;code>ControlPlane&lt;/code> CRD. In this paragraph, however, we want to focus
on the customization of the standard components.&lt;/p>
&lt;p>Let's focus on both the kube-apiserver and the kube-controller-manager
&lt;code>Deployment&lt;/code>s. Our AWS extension for Gardener is not yet using CSI but relying
on the in-tree EBS volume plugin. Hence, it needs to enable the
&lt;code>PersistentVolumeLabel&lt;/code> admission plugin and to provide the cloud provider
config to the kube-apiserver. Similarly, the kube-controller-manager will be
instructed to use its in-tree volume plugin.&lt;/p>
&lt;p>The kube-apiserver &lt;code>Deployment&lt;/code> incorporates the &lt;code>kube-apiserver&lt;/code> container and
is deployed by Gardener like this:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">- command&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>-&lt;span style="color:#bbb"> &lt;/span>/hyperkube&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>-&lt;span style="color:#bbb"> &lt;/span>apiserver&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>-&lt;span style="color:#bbb"> &lt;/span>--enable-admission-plugins=Priority,...,NamespaceLifecycle&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>-&lt;span style="color:#bbb"> &lt;/span>--allow-privileged=&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>-&lt;span style="color:#bbb"> &lt;/span>--anonymous-auth=&lt;span style="color:#a2f;font-weight:bold">false&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Using a &lt;code>MutatingWebhookConfiguration&lt;/code> the AWS extension injects the mentioned
flags and modifies the spec as follows:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">- command&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>-&lt;span style="color:#bbb"> &lt;/span>/hyperkube&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>-&lt;span style="color:#bbb"> &lt;/span>apiserver&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>-&lt;span style="color:#bbb"> &lt;/span>--enable-admission-plugins=Priority,...,NamespaceLifecycle,PersistentVolumeLabel&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>-&lt;span style="color:#bbb"> &lt;/span>--allow-privileged=&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>-&lt;span style="color:#bbb"> &lt;/span>--anonymous-auth=&lt;span style="color:#a2f;font-weight:bold">false&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>-&lt;span style="color:#bbb"> &lt;/span>--cloud-provider=aws&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>-&lt;span style="color:#bbb"> &lt;/span>--cloud-config=/etc/kubernetes/cloudprovider/cloudprovider.conf&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>-&lt;span style="color:#bbb"> &lt;/span>--endpoint-reconciler-type=none&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>-&lt;span style="color:#bbb"> &lt;/span>mountPath:&lt;span style="color:#bbb"> &lt;/span>/etc/kubernetes/cloudprovider&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">    &lt;/span>name:&lt;span style="color:#bbb"> &lt;/span>cloud-provider-config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">- configMap&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">    &lt;/span>defaultMode:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">420&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">    &lt;/span>name:&lt;span style="color:#bbb"> &lt;/span>cloud-provider-config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">  &lt;/span>name:&lt;span style="color:#bbb"> &lt;/span>cloud-provider-config&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The kube-controller-manager &lt;code>Deployment&lt;/code> is handled in a similar way.&lt;/p>
&lt;p>Webhooks in the seed cluster can be used to mutate anything related to the shoot
cluster control plane deployed by Gardener or any other extension. There is a
similar webhook concept for resources in shoot clusters in case extension
controllers need to customize system components deployed by Gardener.&lt;/p>
&lt;h3 id="registration-of-extension-controllers">Registration of Extension Controllers&lt;/h3>
&lt;p>The Gardener API uses two special resources to register and install extensions.
The registration itself is declared via the &lt;code>ControllerRegistration&lt;/code> resource.
The easiest option is to define the Helm chart as well as some values to render
the chart, however, any other deployment mechanism is supported via custom code
as well.&lt;/p>
&lt;p>Gardener determines whether an extension controller is required in a specific
seed cluster, and creates a &lt;code>ControllerInstallation&lt;/code> that is used to trigger the
deployment.&lt;/p>
&lt;p>To date, every registered extension controller is deployed to every seed cluster
which is not necessary in general. In the future, Gardener will become more
selective to only deploy those extensions required on the specific seed
clusters.&lt;/p>
&lt;p>Our dynamic registration approach allows to add or remove extensions in the
running system - without the necessity to rebuild or restart any component.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-10-gardener-project-update/architecture.png"
alt="Gardener architecture with extension controllers"/>
&lt;/figure>
&lt;p>&lt;em>Figure 2 Gardener architecture with extension controllers.&lt;/em>&lt;/p>
&lt;h3 id="status-quo">Status Quo&lt;/h3>
&lt;p>We have recently introduced the new &lt;code>core.gardener.cloud&lt;/code> API group that
incorporates fully forwards and backwards compatible &lt;code>Shoot&lt;/code> resources, and that
allows providers to use Gardener without modifying anything in its core source
tree.&lt;/p>
&lt;p>We have already adapted all controllers to use this new API group and have
deprecated the old API. Eventually, after a few months we will remove it, so
end-users are advised to start migrating to the new API soon.&lt;/p>
&lt;p>Apart from that, we have enabled all relevant extensions to contribute to the
shoot health status and implemented the respective contract. The basic idea is
that the CRDs may have &lt;code>.status.conditions&lt;/code> that are picked up by Gardener and
merged with its standard health checks into the &lt;code>Shoot&lt;/code> status field.&lt;/p>
&lt;p>Also, we want to implement some easy-to-use library functions facilitating
defaulting and validation webhooks for the CRDs in order to validate the
&lt;code>providerConfig&lt;/code> field controlled by end-users.&lt;/p>
&lt;p>Finally, we will split the
&lt;a href="https://github.com/gardener/gardener-extensions">&lt;code>gardener/gardener-extensions&lt;/code>&lt;/a>
repository into separate repositories and keep it only for the generic library
functions that can be used to write extension controllers.&lt;/p>
&lt;h2 id="next-steps">Next Steps&lt;/h2>
&lt;p>Kubernetes has externalized many of the infrastructural management challenges.
The inception design solves most of them by delegating lifecycle operations to a
separate management plane (seed clusters). But what if the garden cluster or a
seed cluster goes down? How do we scale beyond tens of thousands of managed
clusters that need to be reconciled in parallel? We are further investing into
hardening the Gardener scalability and disaster recovery features. Let's briefly
highlight three of the features in more detail:&lt;/p>
&lt;h3 id="gardenlet">Gardenlet&lt;/h3>
&lt;p>Right from the beginning of the Gardener Project we started implementing the
&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/">operator
pattern&lt;/a>: We
have a custom controller-manager that acts on our own custom resources. Now,
when you start thinking about the &lt;a href="https://github.com/gardener/documentation/wiki/Architecture">Gardener
architecture&lt;/a>, you
will recognize some interesting similarity with respect to the Kubernetes
architecture: Shoot clusters can be compared with pods, and seed clusters can be
seen as worker nodes. Guided by this observation we introduced the
&lt;strong>gardener-scheduler&lt;/strong>. Its main task is to find an appropriate seed cluster to
host the control-plane for newly ordered clusters, similar to how the
kube-scheduler finds an appropriate node for newly created pods. By providing
multiple seed clusters for a region (or provider) and distributing the workload,
we reduce the blast-radius of potential hick-ups as well.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-10-gardener-project-update/gardenlet.png"
alt="Similarities between Kubernetes and Gardener architecture"/>
&lt;/figure>
&lt;p>&lt;em>Figure 3 Similarities between Kubernetes and Gardener architecture.&lt;/em>&lt;/p>
&lt;p>Yet, there is still a significant difference between the Kubernetes and the
Gardener architectures: Kubernetes runs a primary &amp;quot;agent&amp;quot; on every node, the
kubelet, which is mainly responsible for managing pods and containers on its
particular node. Gardener uses its controller-manager which is responsible for
all shoot clusters on all seed clusters, and it is performing its reconciliation
loops centrally from the garden cluster.&lt;/p>
&lt;p>While this works well at scale for thousands of clusters today, our goal is to
enable true scalability following the Kubernetes principles (beyond the capacity
of a single controller-manager): We are now working on distributing the logic
(or the Gardener operator) into the seed cluster and will introduce a
corresponding component, adequately named the &lt;strong>gardenlet&lt;/strong>. It will be
Gardener's primary &amp;quot;agent&amp;quot; on every seed cluster and will be only responsible
for shoot clusters located in its particular seed cluster.&lt;/p>
&lt;p>The gardener-controller-manager will still keep its control loops for other
resources of the Gardener API, however, it will no longer talk to seed/shoot
clusters.&lt;/p>
&lt;p>Reversing the control flow will even allow placing seed/shoot clusters behind
firewalls without the necessity of direct accessibility (via VPN tunnels)
anymore.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-10-gardener-project-update/gardenlet-detailed.png"
alt="Detailed architecture with Gardenlet"/>
&lt;/figure>
&lt;p>&lt;em>Figure 4 Detailed architecture with Gardenlet.&lt;/em>&lt;/p>
&lt;h3 id="control-plane-migration-between-seed-clusters">Control Plane Migration between Seed Clusters&lt;/h3>
&lt;p>When a seed cluster fails, the user's static workload will continue to operate.
However, administrating the cluster won't be possible anymore because the shoot
cluster's API server running in the failed seed is no longer reachable.&lt;/p>
&lt;p>We have implemented the relocation of failed control planes hit by some seed
disaster to another seed and are now working on fully automating this unique
capability. In fact, this approach is not only feasible, we have performed the
fail-over procedure multiple times in our production.&lt;/p>
&lt;p>The automated failover capability will enable us to implement even more
comprehensive disaster recovery and scalability qualities, e.g., the automated
provisioning and re-balancing of seed clusters or automated migrations for all
non-foreseeable cases. Again, think about the similarities with Kubernetes with
respect to pod eviction and node drains.&lt;/p>
&lt;h3 id="gardener-ring">Gardener Ring&lt;/h3>
&lt;p>The Gardener Ring is our novel approach for provisioning and managing Kubernetes
clusters without relying on an external provision tool for the initial cluster.
By using Kubernetes in a recursive manner, we can drastically reduce the
management complexity by avoiding imperative tool sets, while creating new
qualities with a self-stabilizing circular system.&lt;/p>
&lt;p>The Ring approach is conceptually different from self-hosting and static pod
based deployments. The idea is to create a ring of three (or more) shoot
clusters that each host the control plane of its successor.&lt;/p>
&lt;p>An outage of one cluster will not affect the stability and availability of the
Ring, and as the control plane is externalized the failed cluster can be
automatically recovered by Gardener's self-healing capabilities. As long as
there is a quorum of at least &lt;code>n/2+1&lt;/code> available clusters the Ring will always
stabilize itself. Running these clusters on different cloud providers (or at
least in different regions / data centers) reduces the potential for quorum
losses.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-10-gardener-project-update/ring.png"
alt="Self-stabilizing ring of Kubernetes clusters"/>
&lt;/figure>
&lt;p>&lt;em>Figure 5 Self-stabilizing ring of Kubernetes clusters.&lt;/em>&lt;/p>
&lt;p>The way how the distributed instances of Gardener can share the same data is by
deploying separate kube-apiserver instances talking to the same etcd cluster.
These kube-apiservers are forming a node-less Kubernetes cluster that can be
used as &amp;quot;data container&amp;quot; for Gardener and its associated applications.&lt;/p>
&lt;p>We are running test landscapes internally protected by the ring and it has saved
us from manual interventions. With the automated control plane migration in
place we can easily bootstrap the Ring and will solve the &amp;quot;initial cluster
problem&amp;quot; as well as improve the overall robustness.&lt;/p>
&lt;h2 id="getting-started">Getting Started!&lt;/h2>
&lt;p>If you are interested in writing an extension, you might want to check out the
following resources:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md">GEP-1: Extensibility proposal
document&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/04-new-core-gardener-cloud-apis.md">GEP-4: New &lt;code>core.gardener.cloud/v1alpha1&lt;/code>
API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/gardener-extensions/tree/master/controllers/provider-aws">Example extension controller implementation for
AWS&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://godoc.org/github.com/gardener/gardener-extensions/pkg">Gardener Extensions Golang
library&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/gardener/tree/master/docs/extensions">Extension contract
documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/api-reference/">Gardener API Reference&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Of course, any other contribution to our project is very welcome as well! We are
always looking for new community members.&lt;/p>
&lt;p>If you want to try out Gardener, please check out our &lt;a href="https://gardener.cloud/installer/">quick installation
guide&lt;/a>. This installer will setup a complete
Gardener environment ready to be used for testing and evaluation within just a
few minutes.&lt;/p>
&lt;h2 id="contributions-welcome">Contributions Welcome!&lt;/h2>
&lt;p>The Gardener project is developed as Open Source and hosted on GitHub:
&lt;a href="https://github.com/gardener">https://github.com/gardener&lt;/a>&lt;/p>
&lt;p>If you see the potential of the Gardener project, please join us via GitHub.&lt;/p>
&lt;p>We are having a weekly &lt;a href="https://docs.google.com/document/d/1314v8ziVNQPjdBrWp-Y4BYrTDlv7dq2cWDFIa9SMaP4">public community
meeting&lt;/a>
scheduled every Friday 10-11 a.m. CET, and a public &lt;a href="https://kubernetes.slack.com/messages/gardener">#gardener
Slack&lt;/a> channel in the Kubernetes
workspace. Also, we are planning a &lt;a href="https://docs.google.com/document/d/1EQ_kt70gwybiL7FY8F7Dx--GtiNwdv0oRDwqQqAIYMk/edit#heading=h.a43vkkp847f1">Gardener Hackathon in Q1
2020&lt;/a>
and are looking forward meeting you there!&lt;/p></description></item><item><title>Blog: Develop a Kubernetes controller in Java</title><link>https://kubernetes.io/blog/2019/11/26/develop-a-kubernetes-controller-in-java/</link><pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2019/11/26/develop-a-kubernetes-controller-in-java/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Min Kim (Ant Financial), Tony Ado (Ant Financial)&lt;/p>
&lt;p>The official &lt;a href="https://github.com/kubernetes-client/java">Kubernetes Java SDK&lt;/a> project
recently released their latest work on providing the Java Kubernetes developers
a handy Kubernetes controller-builder SDK which is helpful for easily developing
advanced workloads or systems.&lt;/p>
&lt;h2 id="overall">Overall&lt;/h2>
&lt;p>Java is no doubt one of the most popular programming languages in the world but
it's been difficult for a period time for those non-Golang developers to build up
their customized controller/operator due to the lack of library resources in the
community. In the world of Golang, there're already some excellent controller
frameworks, for example, &lt;a href="https://github.com/kubernetes-sigs/controller-runtime">controller runtime&lt;/a>,
&lt;a href="https://github.com/operator-framework/operator-sdk">operator SDK&lt;/a>. These
existing Golang frameworks are relying on the various utilities from the
&lt;a href="https://github.com/kubernetes/client-go">Kubernetes Golang SDK&lt;/a> proven to
be stable over years. Driven by the emerging need of further integration into
the platform of Kubernetes, we not only ported many essential toolings from the Golang
SDK into the kubernetes Java SDK including informers, work-queues, leader-elections,
etc. but also developed a controller-builder SDK which wires up everything into
a runnable controller without hiccups.&lt;/p>
&lt;h2 id="backgrounds">Backgrounds&lt;/h2>
&lt;p>Why use Java to implement Kubernetes tooling? You might pick Java for:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Integrating legacy enterprise Java systems&lt;/strong>: Many companies have their legacy
systems or frameworks written in Java in favor of stability. We are not able to
move everything to Golang easily.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>More open-source community resources&lt;/strong>: Java is mature and has accumulated abundant open-source
libraries over decades, even though Golang is getting more and more fancy and
popular for developers. Additionally, nowadays developers are able to develop
their aggregated-apiservers over SQL-storage and Java has way better support on SQLs.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="how-to-use">How to use?&lt;/h2>
&lt;p>Take maven project as example, adding the following dependencies into your dependencies:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-xml" data-lang="xml">&lt;span style="color:#008000;font-weight:bold">&amp;lt;dependency&amp;gt;&lt;/span>
&lt;span style="color:#008000;font-weight:bold">&amp;lt;groupId&amp;gt;&lt;/span>io.kubernetes&lt;span style="color:#008000;font-weight:bold">&amp;lt;/groupId&amp;gt;&lt;/span>
&lt;span style="color:#008000;font-weight:bold">&amp;lt;artifactId&amp;gt;&lt;/span>client-java-extended&lt;span style="color:#008000;font-weight:bold">&amp;lt;/artifactId&amp;gt;&lt;/span>
&lt;span style="color:#008000;font-weight:bold">&amp;lt;version&amp;gt;&lt;/span>6.0.1&lt;span style="color:#008000;font-weight:bold">&amp;lt;/version&amp;gt;&lt;/span>
&lt;span style="color:#008000;font-weight:bold">&amp;lt;/dependency&amp;gt;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then we can make use of the provided builder libraries to write your own controller.
For example, the following one is a simple controller prints out node information
on watch notification, see complete example &lt;a href="https://github.com/kubernetes-client/java/blob/master/examples/src/main/java/io/kubernetes/client/examples/ControllerExample.java">here&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-java" data-lang="java">&lt;span style="color:#666">...&lt;/span>
Reconciler reconciler &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">new&lt;/span> Reconciler&lt;span style="color:#666">()&lt;/span> &lt;span style="color:#666">{&lt;/span>
&lt;span style="color:#a2f">@Override&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">public&lt;/span> Result &lt;span style="color:#00a000">reconcile&lt;/span>&lt;span style="color:#666">(&lt;/span>Request request&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#666">{&lt;/span>
V1Node node &lt;span style="color:#666">=&lt;/span> nodeLister&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">get&lt;/span>&lt;span style="color:#666">(&lt;/span>request&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">getName&lt;/span>&lt;span style="color:#666">());&lt;/span>
System&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">out&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">println&lt;/span>&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#b44">&amp;#34;triggered reconciling &amp;#34;&lt;/span> &lt;span style="color:#666">+&lt;/span> node&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">getMetadata&lt;/span>&lt;span style="color:#666">().&lt;/span>&lt;span style="color:#b44">getName&lt;/span>&lt;span style="color:#666">());&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> &lt;span style="color:#a2f;font-weight:bold">new&lt;/span> Result&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#a2f;font-weight:bold">false&lt;/span>&lt;span style="color:#666">);&lt;/span>
&lt;span style="color:#666">}&lt;/span>
&lt;span style="color:#666">};&lt;/span>
Controller controller &lt;span style="color:#666">=&lt;/span>
ControllerBuilder&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">defaultBuilder&lt;/span>&lt;span style="color:#666">(&lt;/span>informerFactory&lt;span style="color:#666">)&lt;/span>
&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">watch&lt;/span>&lt;span style="color:#666">(&lt;/span>
&lt;span style="color:#666">(&lt;/span>workQueue&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#666">-&amp;gt;&lt;/span> ControllerBuilder&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">controllerWatchBuilder&lt;/span>&lt;span style="color:#666">(&lt;/span>V1Node&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">class&lt;/span>&lt;span style="color:#666">,&lt;/span> workQueue&lt;span style="color:#666">).&lt;/span>&lt;span style="color:#b44">build&lt;/span>&lt;span style="color:#666">())&lt;/span>
&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">withReconciler&lt;/span>&lt;span style="color:#666">(&lt;/span>nodeReconciler&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#080;font-style:italic">// required, set the actual reconciler
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">withName&lt;/span>&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#b44">&amp;#34;node-printing-controller&amp;#34;&lt;/span>&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#080;font-style:italic">// optional, set name for controller for logging, thread-tracing
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">withWorkerCount&lt;/span>&lt;span style="color:#666">(&lt;/span>4&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#080;font-style:italic">// optional, set worker thread count
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">withReadyFunc&lt;/span>&lt;span style="color:#666">(&lt;/span> nodeInformer&lt;span style="color:#666">::&lt;/span>hasSynced&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#080;font-style:italic">// optional, only starts controller when the cache has synced up
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">build&lt;/span>&lt;span style="color:#666">();&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you notice, the new Java controller framework learnt a lot from the design of
&lt;a href="https://github.com/kubernetes-sigs/controller-runtime">controller-runtime&lt;/a> which
successfully encapsulates the complex components inside controller into several
clean interfaces. With the help of Java Generics, we even move on a bit and simply
the encapsulation in a better way.&lt;/p>
&lt;p>As for more advanced usage, we can wrap multiple controllers into a controller-manager
or a leader-electing controller which helps deploying in HA setup. In a word, we can
basically find most of the equivalence implementations here from Golang SDK and
more advanced features are under active development by us.&lt;/p>
&lt;h2 id="future-steps">Future steps&lt;/h2>
&lt;p>The community behind the official Kubernetes Java SDK project will be focusing on
providing more useful utilities for developers who hope to program cloud native
Java applications to extend Kubernetes. If you are interested in more details,
please look at our repo &lt;a href="https://github.com/kubernetes-client/java">kubernetes-client/java&lt;/a>.
Feel free to share also your feedback with us, through Issues or &lt;a href="http://kubernetes.slack.com/messages/kubernetes-client/">Slack&lt;/a>.&lt;/p></description></item><item><title>Blog: Running Kubernetes locally on Linux with Microk8s</title><link>https://kubernetes.io/blog/2019/11/26/running-kubernetes-locally-on-linux-with-microk8s/</link><pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2019/11/26/running-kubernetes-locally-on-linux-with-microk8s/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: &lt;a href="https://twitter.com/idvoretskyi">Ihor Dvoretskyi&lt;/a>, Developer Advocate, Cloud Native Computing Foundation; &lt;a href="https://twitter.com/carminerimi">Carmine Rimi&lt;/a>&lt;/p>
&lt;p>This article, the second in a &lt;a href="https://kubernetes.io/blog/2019/03/28/running-kubernetes-locally-on-linux-with-minikube-now-with-kubernetes-1.14-support/">series&lt;/a> about local deployment options on Linux, and covers &lt;a href="https://microk8s.io/">MicroK8s&lt;/a>. Microk8s is the click-and-run solution for deploying a Kubernetes cluster locally, originally developed by Canonical, the publisher of Ubuntu.&lt;/p>
&lt;p>While Minikube usually spins up a local virtual machine (VM) for the Kubernetes cluster, MicroK8s doesn’t require a VM. It uses &lt;a href="https://snapcraft.io/">snap&lt;/a> packages, an application packaging and isolation technology.&lt;/p>
&lt;p>This difference has its pros and cons. Here we’ll discuss a few of the interesting differences, and comparing the benefits of a VM based approach with the benefits of a non-VM approach. One of the first factors is cross-platform portability. While a Minikube VM is portable across operating systems - it supports not only Linux, but Windows, macOS, and even FreeBSD - Microk8s requires Linux, and only on those distributions &lt;a href="https://snapcraft.io/docs/installing-snapd">that support snaps&lt;/a>. Most popular Linux distributions are supported.&lt;/p>
&lt;p>Another factor to consider is resource consumption. While a VM appliance gives you greater portability, it does mean you’ll consume more resources to run the VM, primarily because the VM ships a complete operating system, and runs on top of a hypervisor. You’ll consume more disk space when the VM is dormant. You’ll consume more RAM and CPU while it is running. Since Microk8s doesn’t require spinning up a virtual machine you’ll have more resources to run your workloads and other applications. Given its smaller footprint, MicroK8s is ideal for IoT devices - you can even use it on a Raspberry Pi device!&lt;/p>
&lt;p>Finally, the projects appear to follow a different release cadence and strategy. MicroK8s, and snaps in general provide &lt;a href="https://snapcraft.io/docs/channels">channels&lt;/a> that allow you to consume beta and release candidate versions of new releases of Kubernetes, as well as the previous stable release. Microk8s generally releases the stable release of upstream Kubernetes almost immediately.&lt;/p>
&lt;p>But wait, there’s more! Minikube and MicroK8s both started as single-node clusters. Essentially, they allow you to create a Kubernetes cluster with a single worker node. That is about to change - there’s an early alpha release of MicroK8s that includes clustering. With this capability, you can create Kubernetes clusters with as many worker nodes as you wish. This is effectively an un-opinionated option for creating a cluster - the developer must create the network connectivity between the nodes, as well as integrate with other infrastructure that may be required, like an external load-balancer. In summary, MicroK8s offers a quick and easy way to turn a handful of computers or VMs into a multi-node Kubernetes cluster. We’ll write more about this kind of architecture in a future article.&lt;/p>
&lt;h2 id="disclaimer">Disclaimer&lt;/h2>
&lt;p>This is not an official guide to MicroK8s. You may find detailed information on running and using MicroK8s on it's official &lt;a href="https://microk8s.io/docs/">webpage&lt;/a>, where different use cases, operating systems, environments, etc. are covered. Instead, the purpose of this post is to provide clear and easy guidelines for running MicroK8s on Linux.&lt;/p>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;p>A Linux distribution that &lt;a href="https://snapcraft.io/docs/installing-snapd">supports snaps&lt;/a>, is required. In this guide, we’ll use Ubuntu 18.04 LTS, it supports snaps out-of-the-box.
If you are interested in running Microk8s on Windows or Mac, you should check out &lt;a href="https://multipass.run">Multipass&lt;/a> to stand up a quick Ubuntu VM as the official way to run virtual Ubuntu on your system.&lt;/p>
&lt;h2 id="microk8s-installation">MicroK8s installation&lt;/h2>
&lt;p>MicroK8s installation is straightforward:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo snap install microk8s --classic
&lt;/code>&lt;/pre>&lt;/div>&lt;center>&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/001-install.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;p>The command above installs a local single-node Kubernetes cluster in seconds. Once the command execution is finished, your Kubernetes cluster is up and running.&lt;/p>
&lt;p>You may verify the MicroK8s status with the following command:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo microk8s.status
&lt;/code>&lt;/pre>&lt;/div>&lt;center>&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/002-status.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;h2 id="using-microk8s">Using microk8s&lt;/h2>
&lt;p>Using MicroK8s is as straightforward as installing it. MicroK8s itself includes a &lt;code>kubectl&lt;/code> binary, which can be accessed by running the &lt;code>microk8s.kubectl&lt;/code> command. As an example:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">microk8s.kubectl get nodes
&lt;/code>&lt;/pre>&lt;/div>&lt;center>&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/003-nodes.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;p>While using the prefix &lt;code>microk8s.kubectl&lt;/code> allows for a parallel install of another system-wide kubectl without impact, you can easily get rid of it by using the &lt;code>snap alias&lt;/code> command:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo snap &lt;span style="color:#a2f">alias&lt;/span> microk8s.kubectl kubectl
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will allow you to simply use &lt;code>kubectl&lt;/code> after. You can revert this change using the &lt;code>snap unalias&lt;/code> command.&lt;/p>
&lt;center>&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/004-alias.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get nodes
&lt;/code>&lt;/pre>&lt;/div>&lt;center>&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/005-nodes.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;h2 id="microk8s-addons">MicroK8s addons&lt;/h2>
&lt;p>One of the biggest benefits of using Microk8s is the fact that it also supports various add-ons and extensions. What is even more important is they are shipped out of the box, the user just has to enable them.&lt;/p>
&lt;p>The full list of extensions can be checked by running the &lt;code>microk8s.status&lt;/code> command:&lt;/p>
&lt;pre>&lt;code>sudo microk8s.status
&lt;/code>&lt;/pre>&lt;p>As of the time of writing this article, the following add-ons are supported:&lt;/p>
&lt;center>&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/006-status.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;p>More add-ons are being created and contributed by the community all the time, it definitely helps to check often!&lt;/p>
&lt;h2 id="release-channels">Release channels&lt;/h2>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo snap info microk8s
&lt;/code>&lt;/pre>&lt;/div>&lt;center>&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/010-releases.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;h2 id="installing-the-sample-application">Installing the sample application&lt;/h2>
&lt;p>In this tutorial we’ll use NGINX as a sample application (&lt;a href="https://hub.docker.com/_/nginx">the official Docker Hub image&lt;/a>).&lt;/p>
&lt;p>It will be installed as a Kubernetes deployment:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl create deployment nginx --image&lt;span style="color:#666">=&lt;/span>nginx
&lt;/code>&lt;/pre>&lt;/div>&lt;p>To verify the installation, let’s run the following:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get deployments
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get pods
&lt;/code>&lt;/pre>&lt;/div>&lt;center>&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/007-deployments.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;p>Also, we can retrieve the full output of all available objects within our Kubernetes cluster:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get all --all-namespaces
&lt;/code>&lt;/pre>&lt;/div>&lt;center>&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/008-all.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;h2 id="uninstalling-microk8s">Uninstalling MicroK8s&lt;/h2>
&lt;p>Uninstalling your microk8s cluster is so easy as uninstalling the snap:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo snap remove microk8s
&lt;/code>&lt;/pre>&lt;/div>&lt;center>&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/009-remove.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;h2 id="screencast">Screencast&lt;/h2>
&lt;p>&lt;a href="https://asciinema.org/a/263394">&lt;img src="https://asciinema.org/a/263394.svg" alt="asciicast">&lt;/a>&lt;/p></description></item><item><title>Blog: Grokkin' the Docs</title><link>https://kubernetes.io/blog/2019/11/05/grokkin-the-docs/</link><pubDate>Tue, 05 Nov 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2019/11/05/grokkin-the-docs/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> &lt;a href="https://www.linkedin.com/in/aimee-ukasick/">Aimee Ukasick&lt;/a>, Independent Contributor&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/grokkin-the-docs/grok-definition.png"
alt="grok: to understand profoundly and intuitively"/> &lt;figcaption>
&lt;h4>Definition courtesy of Merriam Webster online dictionary&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="intro-observations-of-a-new-sig-docs-contributor">Intro - Observations of a new SIG Docs contributor&lt;/h2>
&lt;p>I began contributing to the SIG Docs community in August 2019. Sometimes I feel
like I am a stranger in a strange land adapting to a new community:
investigating community organization, understanding contributor society,
learning new lessons, and incorporating new jargon. I'm an observer as well as a
contributor.&lt;/p>
&lt;h2 id="observation-1">Observation 01: Read the &lt;em>Contribute&lt;/em> pages!&lt;/h2>
&lt;p>I contributed code and documentation to OpenStack, OPNFV, and Acumos, so I
thought contributing to the Kubernetes documentation would be the same. I was
wrong. I should have thoroughly &lt;strong>read&lt;/strong> the &lt;a href="https://kubernetes.io/docs/contribute/">Contribute to Kubernetes
docs&lt;/a> pages instead of skimming them.&lt;/p>
&lt;p>I am very familiar with the git/gerrit workflow. With those tools, a contributor clones
the &lt;code>master&lt;/code> repo and then creates a local branch. Kubernetes uses a different
approach, called &lt;em>Fork and Pull&lt;/em>. Each contributor &lt;code>forks&lt;/code> the master repo, and
then the contributor pushes work to their fork before creating a pull request. I
created a simple pull request (PR), following the instructions in the &lt;strong>Start
contributing&lt;/strong> page's &lt;a href="https://kubernetes.io/docs/contribute/start/#submit-a-pull-request">Submit a pull
request&lt;/a>
section. This section describes how to make a documentation change using the
GitHub UI. I learned that this method is fine for a change that requires a
single commit to fix. However, this method becomes complicated when you have to
make additional updates to your PR. GitHub creates a new commit for each change
made using the GitHub UI. The Kubernetes GitHub org requires squashing commits.
The &lt;strong>Start contributing&lt;/strong> page didn't mention squashing commits, so I looked at
the GitHub and git documentation. I could not squash my commits using the GitHub
UI. I had to &lt;code>git fetch&lt;/code> and &lt;code>git checkout&lt;/code> my pull request locally, squash the
commits using the command line, and then push my changes. If the &lt;strong>Start
contributing&lt;/strong> had mentioned squashing commits, I would have worked from a local
clone instead of using the GitHub UI.&lt;/p>
&lt;h2 id="observation-2">Observation 02: Reach out and ping someone&lt;/h2>
&lt;p>While working on my first PRs, I had questions about working from a local clone
and about keeping my fork updated from &lt;code>upstream master&lt;/code>. I turned to searching
the internet instead of asking on the &lt;a href="http://slack.k8s.io/">Kubernetes Slack&lt;/a>
#sig-docs channel. I used the wrong process to update my fork, so I had to &lt;code>git rebase&lt;/code> my PRs, which did not go well at all. As a result, I closed those PRs
and submitted new ones. When I asked for help on the #sig-docs channel,
contributors posted useful links, what my local git config file should look
like, and the exact set of git commands to run. The process used by contributors
was different than the one defined in the &lt;strong>Intermediate contributing&lt;/strong> page.
I would have saved myself so much time if I had asked what GitHub workflow to
use. The more community knowledge that is documented, the easier it is for new
contributors to be productive quickly.&lt;/p>
&lt;h2 id="observation-3">Observation 03: Don't let conflicting information ruin your day&lt;/h2>
&lt;p>The Kubernetes community has a contributor guide for
&lt;a href="https://github.com/kubernetes/community/tree/master/contributors/guide">code&lt;/a>
and another one for &lt;a href="https://kubernetes.io/docs/contribute/">documentation&lt;/a>. The
guides contain conflicting information on the same topic. For example, the SIG
Docs GitHub process recommends creating a local branch based on
&lt;code>upstream/master&lt;/code>. The &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/guide/github-workflow.md">Kubernetes Community Contributor
Guide&lt;/a>
advocates updating your fork from upstream and then creating a local branch
based on your fork. Which process should a new contributor follow? Are the two
processes interchangeable? The best place to ask questions about conflicting
information is the #sig-docs or #sig-contribex channels. I asked for
clarification about the GitHub workflows in the #sig-contribex channel.
@cblecker provided an extremely detailed response, which I used to update the
&lt;strong>Intermediate contributing&lt;/strong> page.&lt;/p>
&lt;h2 id="observation-4">Observation 04: Information may be scattered&lt;/h2>
&lt;p>It's common for large open source projects to have information scattered around
various repos or duplicated between repos. Sometimes groups work in silos, and
information is not shared. Other times, a person leaves to work on a
different project without passing on specialized knowledge.
Documentation gaps exist and may never be rectified because of higher priority
items. So new contributors may have difficulty finding basic information, such
as meeting details.&lt;/p>
&lt;p>Attending SIG Docs meetings is a great way to become involved. However, people
have had a hard time locating the meeting URL. Most new contributors ask in the
#sig-docs channel, but I decided to locate the meeting information in the docs.
This required several clicks over multiple pages. How many new contributors miss
meetings because they can't locate the meeting details?&lt;/p>
&lt;h2 id="observation-5">Observation 05: Patience is a virtue&lt;/h2>
&lt;p>A contributor may wait days for feedback on a larger PR. The process from
submission to final approval may take weeks instead of days. There are two
reasons for this: 1) most reviewers work part-time on SIG Docs; and 2) reviewers
want to provide meaningful reviews. &amp;quot;Drive-by reviewing&amp;quot; doesn't happen in SIG
Docs! Reviewers check for the following:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Do the commit message and PR description adequately describe the change?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Does the PR follow the guidelines in the style and content guides?&lt;/p>
&lt;ul>
&lt;li>Overall, is the grammar and punctuation correct?&lt;/li>
&lt;li>Is the content clear, concise, and appropriate for non-native speakers?&lt;/li>
&lt;li>Does the content stylistically fit in with the rest of the documentation?&lt;/li>
&lt;li>Does the flow of the content make sense?&lt;/li>
&lt;li>Can anything be changed to make the content better, such as using a Hugo shortcode?&lt;/li>
&lt;li>Does the content render correctly?&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Is the content technically correct?&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Sometimes the review process made me feel defensive, annoyed, and frustrated. I'm
sure other contributors have felt the same way. Contributors need to be patient!
Writing excellent documentation is an iterative process. Reviewers scrutinize
PRs because they want to maintain a high level of quality in the documentation,
not because they want to annoy contributors!&lt;/p>
&lt;h2 id="observation-6">Observation 06: Make every word count&lt;/h2>
&lt;p>Non-native English speakers read and contribute to the Kubernetes documentation.
When you are writing content, use simple, direct language in clear, concise
sentences. Every sentence you write may be translated into another language, so
remove words that don't add substance. I admit that implementing these
guidelines is challenging at times.&lt;/p>
&lt;p>Issues and pull requests aren't translated into other languages. However, you
should still follow the aforementioned guidelines when you write the description
for an issue or pull request. You should add details and background
information to an issue so the person doing triage doesn't have to apply the
&lt;code>triage/needs-information&lt;/code> label. Likewise, when you create a pull request, you
should add enough information about the content change that reviewers don't have
to figure out the reason for the pull request. Providing details in clear,
concise language speeds up the process.&lt;/p>
&lt;h2 id="observation-7">Observation 07: Triaging issues is more difficult than it should be&lt;/h2>
&lt;p>In SIG Docs, triaging issues requires the ability to distinguish between
support, bug, and feature requests not only for the documentation but also for
Kubernetes code projects. How to route, label, and prioritize issues has become
easier week by week. I'm still not 100% clear on which SIG and/or project is
responsible for which parts of the documentation. The SIGs and Working Groups
&lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">page&lt;/a> helps,
but it is not enough. At a page level in the documentation, it's not
always obvious which SIG or project has domain expertise. The page's front
matter sometimes list reviewers but never lists a SIG or project. Each page should
indicate who is responsible for content, so that SIG Docs triagers know where to
route issues.&lt;/p>
&lt;h2 id="observation-8">Observation 08: SIG Docs is understaffed&lt;/h2>
&lt;p>Documentation is the number one driver of software adoption&lt;sup>1&lt;/sup>.&lt;/p>
&lt;p>Many contributors devote a small amount of time to SIG Docs but only a handful
are trained technical writers. Few companies have hired tech writers to work on
Kubernetes docs at least half-time. That's very disheartening for online
documentation that has had over 53 million unique page views from readers in 229
countries year to date in 2019.&lt;/p>
&lt;p>SIG Docs faces challenges due to lack of technical writers:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Maintaining a high quality in the Kubernetes documentation&lt;/strong>:
There are over 750 pages of documentation. That's &lt;em>750 pages&lt;/em> to check for
stale content on a regular basis. This involves more than running a link
checker against the &lt;code>kubernetes/website&lt;/code> repo. This involves people having a
technical understanding of Kubernetes, knowing which code release changes
impact documentation, and knowing where content is located in the
documentation so that &lt;em>all&lt;/em> impacted pages and example code files are updated
in a timely fashion. Other SIGs help with this, but based on the number of
issues created by readers, enough people aren't working on keeping the content
fresh.&lt;/li>
&lt;li>&lt;strong>Reducing the time to review and merge a PR&lt;/strong>:
The larger the size of the PR, the longer it takes to get the &lt;code>lgtm&lt;/code> label
and eventual approval. My &lt;code>size/M&lt;/code> and larger PRs took from five to thirty
days to approve. Sometimes I politely poked reviewers to review again after
I had pushed updates. Other times I asked on the #sig-docs channel for &lt;em>any
approver&lt;/em> to take a look and approve. People are busy. People go on
vacation. People also move on to new roles that don't involve SIG Docs and
forget to remove themselves from the reviewer and approver assignment file.
A large part of the time-to-merge problem is not having enough reviewers and
approvers. The other part is the &lt;a href="https://github.com/kubernetes/community/blob/master/community-membership.md#reviewer">high
barrier&lt;/a>
to becoming a reviewer or approver, much higher than what I've seen on other
open source projects. Experienced open source tech writers who want to
contribute to SIG Docs aren't fast-tracked into approver and reviewer roles.
On one hand, that high barrier ensures that those roles are filled by folks
with a minimum level of Kubernetes documentation knowledge; on the other
hand, it might deter experienced tech writers from contributing at all, or
from a company allocating a tech writer to SIG Docs. Maybe SIG Docs should
consider deviating from the Kubernetes community requirements by lowering
the barrier to becoming a reviewer or approver, on a case-by-case basis, of
course.&lt;/li>
&lt;li>&lt;strong>Ensuring consistent naming across all pages&lt;/strong>:
Terms should be identical to what is used in the &lt;strong>Standardized Glossary&lt;/strong>. Being consistent reduces confusion.
Tracking down and fixing these occurrences is time-consuming but worthwhile for readers.&lt;/li>
&lt;li>&lt;strong>Working with the Steering Committee to create project documentation guidelines&lt;/strong>:
The &lt;a href="https://github.com/kubernetes/community/blob/master/github-management/kubernetes-repositories.md">Kubernetes Repository Guidelines&lt;/a> don't mention documentation at all. Between a
project's GitHub docs and the Kubernetes docs, some projects have almost
duplicate content, whereas others have conflicting content. Create clear
guidelines so projects know to put roadmaps, milestones, and comprehensive
feature details in the &lt;code>kubernetes/&amp;lt;project&amp;gt;&lt;/code> repo and to put installation,
configuration, usage details, and tutorials in the Kubernetes docs.&lt;/li>
&lt;li>&lt;strong>Removing duplicate content&lt;/strong>:
Kubernetes users install Docker, so a good example of duplicate content is
Docker installation instructions. Rather than repeat what's in the Docker
docs, state which version of Docker works with which version of Kubernetes
and link to the Docker docs for installation. Then detail any
Kubernetes-specific configuration. That idea is the same for the container
runtimes that Kubernetes supports.&lt;/li>
&lt;li>&lt;strong>Removing third-party vendor content&lt;/strong>:
This is tightly coupled to removing duplicate content. Some third-party
content consists of lists or tables detailing external products. Other
third-party content is found in the &lt;strong>Tasks&lt;/strong> and &lt;strong>Tutorials&lt;/strong> sections.
SIG Docs should not be responsible for verifying that third-party products
work with the latest version of Kubernetes. Nor should SIG Docs be
responsible for maintaining lists of training courses or cloud providers.
Additionally, the Kubernetes documentation isn't the place to pitch vendor
products. If SIG Docs is forced to reverse its policy on not allowing
third-party content, there could be a tidal wave of
vendor-or-commercially-oriented pull requests. Maintaining that content
places an undue burden on SIG Docs.&lt;/li>
&lt;li>&lt;strong>Indicating which version of Kubernetes works with each task and tutorial&lt;/strong>:
This means reviewing each task and tutorial for every release. Readers
assume if a task or tutorial is in the latest version of the docs, it works
with the latest version of Kubernetes.&lt;/li>
&lt;li>&lt;strong>Addressing issues&lt;/strong>:
There are 470 open issues in the &lt;code>kubernetes/website&lt;/code> repo. It's hard to keep up with all the issues that are created. We encourage
those creating simpler issues to submit PRs: some do; most do not.&lt;/li>
&lt;li>&lt;strong>Creating more detailed content&lt;/strong>:
Readers
&lt;a href="https://kubernetes.io/blog/2019/10/29/kubernetes-documentation-end-user-survey">indicated&lt;/a>
they would like to see more detailed content across all sections of the
documentation, including tutorials.&lt;/li>
&lt;/ul>
&lt;p>Kubernetes has seen unparalleled growth since its first release in 2015. Like
any fast-growing project, it has growing pains. Providing consistently
high-quality documentation is one of those pains, and one incredibly important
to an open source project. SIG Docs needs a larger core team of tech writers who
are allocated at least 50%. SIG Docs can then better achieve goals, move forward
with new content, update existing content, and address open issues in a timely fashion.&lt;/p>
&lt;h2 id="observation-9">Observation 09: Contributing to technical documentation projects requires, on average, more skills than developing software&lt;/h2>
&lt;p>When I said that to my former colleagues, the response was a healthy dose of
skepticism and lots of laughter. It seems that many developers, as well as
managers, don't fully know what tech writers contributing to open source
projects actually do. Having done both development and technical writing for the
better part of 22 years, I've noticed that tech writers are valued far less than
software developers of comparative standing.&lt;/p>
&lt;p>SIG Docs core team members do far more than write content based on requirements:&lt;/p>
&lt;ul>
&lt;li>We use some of the same processes and tools as developers, such as the
terminal, git workflow, GitHub, and IDEs like Atom, Golang, and Visual Studio Code; we
also use documentation-specific plugins and tools.&lt;/li>
&lt;li>We possess a good eye for detail as well as design and organization: the big picture &lt;em>and&lt;/em> the little picture.&lt;/li>
&lt;li>We provide documentation which has a logical flow; it is not merely content on a page
but the way pages fit into sections and sections fit into the overall structure.&lt;/li>
&lt;li>We write content that is comprehensive and uses language that readers not fluent in English can understand.&lt;/li>
&lt;li>We have a firm grasp of English composition using various markup languages.&lt;/li>
&lt;li>We are technical, sometimes to the level of a Kubernetes admin.&lt;/li>
&lt;li>We read, understand, and occasionally write code.&lt;/li>
&lt;li>We are project managers, able to plan new work as well as assign issues to releases.&lt;/li>
&lt;li>We are educators and diplomats with every review we do and with every comment we leave on an issue.&lt;/li>
&lt;li>We use site analytics to plan work based on which pages readers access most often as well as which pages readers say are unhelpful.&lt;/li>
&lt;li>We are surveyors, soliciting feedback from the community on a regular basis.&lt;/li>
&lt;li>We analyze the documentation as a whole, deciding what content should stay and
what content should be removed based on available resources and reader needs.&lt;/li>
&lt;li>We have a working knowledge of Hugo and other frameworks used for
online documentation; we know how to create, use, and debug Hugo shortcodes that
enable content to be more robust than pure Markdown.&lt;/li>
&lt;li>We troubleshoot performance issues not only with Hugo but with Netlify.&lt;/li>
&lt;li>We grapple with the complex problem of API documentation.&lt;/li>
&lt;li>We are dedicated to providing the highest quality documentation that we can.&lt;/li>
&lt;/ul>
&lt;p>If you have any doubts about the complexity of the Kubernetes documentation
project, watch presentations given by SIG Docs Chair Zach Corleissen:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://archive.fosdem.org/2019/schedule/event/multikuber/">Multilingual Kubernetes&lt;/a> - the kubernetes.io stack, how we got there, and what it took to get there&lt;/li>
&lt;li>&lt;a href="https://youtu.be/GXkpHAruNV8">Found in Translation: Lessons from a Year of Open Source Localization&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Additionally, &lt;a href="https://youtu.be/JvRd7MmAxPw">Docs as Code: The Missing Manual&lt;/a>
(Jennifer Rondeau, Margaret Eker; 2016) is an excellent presentation on the
complexity of documentation projects in general.&lt;/p>
&lt;p>The Write the Docs &lt;a href="http://www.writethedocs.org/">website&lt;/a> and &lt;a href="https://www.youtube.com/channel/UCr019846MitZUEhc6apDdcQ">YouTube
channel&lt;/a> are
fantastic places to delve into the good, the bad, and the ugly of technical writing.&lt;/p>
&lt;p>Think what an open source project would be without talented, dedicated tech writers!&lt;/p>
&lt;h2 id="observation-10">Observation 10: Community is everything&lt;/h2>
&lt;p>The SIG Docs community, and the larger Kubernetes community, is dedicated,
intelligent, friendly, talented, fun, helpful, and a whole bunch of other
positive adjectives! People welcomed me with open arms, and not only because SIG
Docs needs more technical writers. I have never felt that my ideas and contributions were
dismissed because I was the newbie. Humility and respect go a long way.
Community members have a wealth of knowledge to share. Attend meetings, ask
questions, propose improvements, thank people, and contribute in
every way that you can!&lt;/p>
&lt;p>Big shout out to those who helped me, and put up with me (LOL), during my
break-in period: @zacharaysarah, @sftim, @kbhawkey, @jaypipes, @jrondeau,
@jmangel, @bradtopol, @cody_clark, @thecrudge, @jaredb, @tengqm, @steveperry-53,
@mrbobbytables, @cblecker, and @kbarnard10.&lt;/p>
&lt;h2 id="outro">Outro&lt;/h2>
&lt;p>Do I grok SIG Docs? Not quite yet, but I do understand that SIG Docs needs more
dedicated resources to continue to be successful.&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
&lt;p>&lt;sup>1&lt;/sup> @linuxfoundation. &amp;quot;Megan Byrd-Sanicki, Open Source Strategist, Google @megansanicki - documentation is the #1 driver of software adoption. #ossummit.&amp;quot; &lt;em>Twitter&lt;/em>, Oct 29, 2019, 3:54 a.m., twitter.com/linuxfoundation/status/1189103201439637510.&lt;/p></description></item><item><title>Blog: Kubernetes Documentation Survey</title><link>https://kubernetes.io/blog/2019/10/29/kubernetes-documentation-end-user-survey/</link><pubDate>Tue, 29 Oct 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2019/10/29/kubernetes-documentation-end-user-survey/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> &lt;a href="https://www.linkedin.com/in/aimee-ukasick/">Aimee Ukasick&lt;/a> and SIG Docs&lt;/p>
&lt;p>In September, SIG Docs conducted its first survey about the &lt;a href="https://kubernetes.io/docs/">Kubernetes
documentation&lt;/a>. We'd like to thank the CNCF's Kim
McMahon for helping us create the survey and access the results.&lt;/p>
&lt;h1 id="key-takeaways">Key takeaways&lt;/h1>
&lt;p>Respondents would like more example code, more detailed content, and more
diagrams in the Concepts, Tasks, and Reference sections.&lt;/p>
&lt;p>74% of respondents would like the Tutorials section to contain advanced content.&lt;/p>
&lt;p>69.70% said the Kubernetes documentation is the first place they look for
information about Kubernetes.&lt;/p>
&lt;h1 id="survey-methodology-and-respondents">Survey methodology and respondents&lt;/h1>
&lt;p>We conducted the survey in English. The survey was only available for 4 days due
to time constraints. We announced the survey on Kubernetes mailing lists, in
Kubernetes Slack channels, on Twitter, and in Kube Weekly. There were 23
questions, and respondents took an average of 4 minutes to complete the survey.&lt;/p>
&lt;h2 id="quick-facts-about-respondents">Quick facts about respondents:&lt;/h2>
&lt;ul>
&lt;li>48.48% are experienced Kubernetes users, 26.26% expert, and 25.25% beginner&lt;/li>
&lt;li>57.58% use Kubernetes in both administrator and developer roles&lt;/li>
&lt;li>64.65% have been using the Kubernetes documentation for more than 12 months&lt;/li>
&lt;li>95.96% read the documentation in English&lt;/li>
&lt;/ul>
&lt;h1 id="question-and-response-highlights">Question and response highlights&lt;/h1>
&lt;h2 id="why-people-access-the-kubernetes-documentation">Why people access the Kubernetes documentation&lt;/h2>
&lt;p>The majority of respondents stated that they access the documentation for the Concepts.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-sig-docs-survey/Q9-k8s-docs-use.png"
alt="Why respondents access the Kubernetes documentation"/>
&lt;/figure>
&lt;p>This deviates only slightly from what we see in Google Analytics: of the top 10
most viewed pages this year, #1 is the kubectl cheatsheet in the Reference section,
followed overwhelmingly by pages in the Concepts section.&lt;/p>
&lt;h2 id="satisfaction-with-the-documentation">Satisfaction with the documentation&lt;/h2>
&lt;p>We asked respondents to record their level of satisfaction with the detail in
the Concepts, Tasks, Reference, and Tutorials sections:&lt;/p>
&lt;ul>
&lt;li>Concepts: 47.96% Moderately Satisfied&lt;/li>
&lt;li>Tasks: 50.54% Moderately Satisfied&lt;/li>
&lt;li>Reference: 40.86% Very Satisfied&lt;/li>
&lt;li>Tutorial: 47.25% Moderately Satisfied&lt;/li>
&lt;/ul>
&lt;h2 id="how-sig-docs-can-improve-each-documentation-section">How SIG Docs can improve each documentation section&lt;/h2>
&lt;p>We asked how we could improve each section, providing respondents with
selectable answers as well as a text field. The clear majority would like more
example code, more detailed content, more diagrams, and advanced tutorials:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">- Personally, would like to see more analogies to help further understanding.
- Would be great if corresponding sections of code were explained too
- Expand on the concepts to bring them together - they&amp;#39;re a bucket of separate eels moving in different directions right now
- More diagrams, and more example code
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Respondents used the &amp;quot;Other&amp;quot; text box to record areas causing frustration:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">- Keep concepts up to date and accurate
- Keep task topics up to date and accurate. Human testing.
- Overhaul the examples. Many times the output of commands shown is not actual.
- I&amp;#39;ve never understood how to navigate or interpret the reference section
- Keep the tutorials up to date, or remove them
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="how-sig-docs-can-improve-the-documentation-overall">How SIG Docs can improve the documentation overall&lt;/h2>
&lt;p>We asked respondents how we can improve the Kubernetes documentation
overall. Some took the opportunity to tell us we are doing a good job:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">- For me, it is the best documented open source project.
- Keep going!
- I find the documentation to be excellent.
- You [are] doing a great job. For real.
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Other respondents provided feedback on the content:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">- ...But since we&amp;#39;re talking about docs, more is always better. More
advanced configuration examples would be, to me, the way to go. Like a Use Case page for each
configuration topic with beginner to advanced example scenarios. Something like that would be
awesome....
- More in-depth examples and use cases would be great. I often feel that the Kubernetes
documentation scratches the surface of a topic, which might be great for new users, but it leaves
more experienced users without much &amp;#34;official&amp;#34; guidance on how to implement certain things.
- More production like examples in the resource sections (notably secrets) or links to production like
examples
- It would be great to see a very clear &amp;#34;Quick Start&amp;#34; A-&amp;gt;Z up and running like many other tech
projects. There are a handful of almost-quick-starts, but no single guidance. The result is
information overkill.
&lt;/code>&lt;/pre>&lt;/div>&lt;p>A few respondents provided technical suggestions:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">- Make table columns sortable and filterable using a ReactJS or Angular component.
- For most, I think creating documentation with Hugo - a system for static site generation - is not
appropriate. There are better systems for documenting large software project. Specifically, I would
like to see k8s switch to Sphinx for documentation. It has an excellent built-in search, it is easy to
learn if you know markdown, it is widely adopted by other projects (e.g. every software project in
readthedocs.io, linux kernel, docs.python.org etc).
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Overall, respondents provided constructive criticism focusing on the need for
advanced use cases as well as more in-depth examples, guides, and walkthroughs.&lt;/p>
&lt;h1 id="where-to-see-more">Where to see more&lt;/h1>
&lt;p>Survey results summary, charts, and raw data are available in &lt;code>kubernetes/community&lt;/code> sig-docs &lt;a href="https://github.com/kubernetes/community/tree/master/sig-docs/survey">survey&lt;/a> directory.&lt;/p></description></item><item><title>Blog: Contributor Summit San Diego Schedule Announced!</title><link>https://kubernetes.io/blog/2019/10/10/contributor-summit-san-diego-schedule/</link><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2019/10/10/contributor-summit-san-diego-schedule/</guid><description>
&lt;p>Authors: Josh Berkus (Red Hat), Paris Pittman (Google), Jonas Rosland (VMware)&lt;/p>
&lt;p>tl;dr A week ago we announced that &lt;a href="https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/">registration is open&lt;/a> for the contributor
summit , and we're now live with &lt;a href="https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/program/schedule/">the full Contributor Summit schedule!&lt;/a>
Grab your spot while tickets are still available. There is currently a waitlist
for new contributor workshop. (&lt;a href="https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/">Register here!&lt;/a>)&lt;/p>
&lt;p>There are many great sessions planned for the Contributor Summit, spread across
five rooms of current contributor content in addition to the new contributor
workshops. Since this is an upstream contributor summit and we don't often meet,
being a globally distributed team, most of these sessions are discussions or
hands-on labs, not just presentations. We want folks to learn and have a
good time meeting their OSS teammates.&lt;/p>
&lt;p>Unconference tracks are returning from last year with sessions to be chosen
Monday morning. These are ideal for the latest hot topics and specific
discussions that contributors want to have. In previous years, we've covered
flaky tests, cluster lifecycle, KEPs (Kubernetes Enhancement Proposals), mentoring,
security, and more.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2019-10-10-contributor-summit-san-diego-schedule/DSCF0806.jpg" alt="Unconference">&lt;/p>
&lt;p>While the schedule contains difficult decisions in every timeslot, we've picked
a few below to give you a taste of what you'll hear, see, and participate in, at
the summit:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VvMc">Vision&lt;/a>&lt;/strong>: SIG-Architecture will be sharing their vision of where we're going
with Kubernetes development for the next year and beyond.&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VvMj">Security&lt;/a>&lt;/strong>: Tim Allclair and CJ Cullen will present on the current state of
Kubernetes security. In another security talk, Vallery Lancey will lead a
discussion about making our platform secure by default.&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/Vv6Z">Prow&lt;/a>&lt;/strong>: Interested in working with Prow and contributing to Test-Infra, but
not sure where to start? Rob Keilty will help you get a Prow test environment
running on your laptop.&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VvNa">Git&lt;/a>&lt;/strong>: Staff from GitHub will be collaborating with Christoph Blecker to share
practical Git tips for Kubernetes contributors.&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VutA">Reviewing&lt;/a>&lt;/strong>: Tim Hockin will share the secrets of becoming a great code
reviewer, and Jordan Liggitt will conduct a live API review so that you can do
one, or at least pass one.&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VvNJ">End Users&lt;/a>&lt;/strong>: Several end users from the CNCF partner ecosystem, invited by
Cheryl Hung, will hold a Q&amp;amp;A with contributors to strengthen our feedback loop.&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/Vux2">Docs&lt;/a>&lt;/strong>: As always, SIG-Docs will run a three-hour contributing-to-documentation
workshop.&lt;/li>
&lt;/ul>
&lt;p>We're also giving out awards to contributors who distinguished themselves in 2019,
and there will be a huge Meet &amp;amp; Greet for new contributors to find their SIG
(and for existing contributors to ask about their PRs) at the end of the day on
Monday.&lt;/p>
&lt;p>Hope to see you all there, and &lt;a href="https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/">make sure you register!&lt;/a>
&lt;a href="http://git.k8s.io/community/events/events-team">San Diego team&lt;/a>&lt;/p></description></item><item><title>Blog: 2019 Steering Committee Election Results</title><link>https://kubernetes.io/blog/2019/10/03/2019-steering-committee-election-results/</link><pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2019/10/03/2019-steering-committee-election-results/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Bob Killen (University of Michigan), Jorge Castro (VMware),
Brian Grant (Google), and Ihor Dvoretskyi (CNCF)&lt;/p>
&lt;p>The &lt;a href="https://git.k8s.io/community/events/elections/2019">2019 Steering Committee Election&lt;/a> is a landmark milestone for the
Kubernetes project. The initial bootstrap committee is graduating to emeritus
and the committee has now shrunk to its final allocation of seven seats. All
members of the Steering Committee are now fully elected by the Kubernetes
Community.&lt;/p>
&lt;p>Moving forward elections will elect either 3 or 4 people to the committee for
two-year terms.&lt;/p>
&lt;h2 id="results">&lt;strong>Results&lt;/strong>&lt;/h2>
&lt;p>The Kubernetes Steering Committee Election is now complete and the following
candidates came ahead to secure two-year terms that start immediately
(in alphabetical order by GitHub handle):&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Christoph Blecker (&lt;a href="https://github.com/cblecker">@cblecker&lt;/a>), Red Hat&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Derek Carr (&lt;a href="https://github.com/derekwaynecarr">@derekwaynecarr&lt;/a>), Red Hat&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Nikhita Raghunath (&lt;a href="https://github.com/nikhita">@nikhita&lt;/a>), Loodse&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Paris Pittman (&lt;a href="https://github.com/parispittman">@parispittman&lt;/a>)&lt;/strong>, &lt;strong>Google&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>They join Aaron Crickenberger (&lt;a href="https://github.com/spiffxp">@spiffxp&lt;/a>), Google; Davanum Srinivas (&lt;a href="https://github.com/dims">@dims&lt;/a>),
VMware; and Timothy St. Clair (&lt;a href="https://github.com/timothysc">@timothysc&lt;/a>), VMware, to round out the committee.
The seats held by Aaron, Davanum, and Timothy will be up for election around
this time next year.&lt;/p>
&lt;h2 id="big-thanks">Big Thanks!&lt;/h2>
&lt;ul>
&lt;li>Thanks to the initial bootstrap committee for establishing the initial
project governance and overseeing a multi-year transition period:
&lt;ul>
&lt;li>Joe Beda (&lt;a href="https://github.com/jbeda">@jbeda&lt;/a>), VMware&lt;/li>
&lt;li>Brendan Burns (&lt;a href="https://github.com/brendandburns">@brendandburns&lt;/a>), Microsoft&lt;/li>
&lt;li>Clayton Coleman (&lt;a href="https://github.com/smarterclayton">@smarterclayton&lt;/a>), Red Hat&lt;/li>
&lt;li>Brian Grant (&lt;a href="https://github.com/bgrant0607">@bgrant0607&lt;/a>), Google&lt;/li>
&lt;li>Tim Hockin (&lt;a href="https://github.com/thockin">@thockin&lt;/a>), Google&lt;/li>
&lt;li>Sarah Novotny (&lt;a href="https://github.com/sarahnovotny">@sarahnovotny&lt;/a>), Microsoft&lt;/li>
&lt;li>Brandon Philips (&lt;a href="https://github.com/philips">@philips&lt;/a>), Red Hat&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>And also thanks to the other Emeritus Steering Committee Members. Your
prior service is appreciated by the community:
&lt;ul>
&lt;li>Quinton Hoole (&lt;a href="https://github.com/quinton-hoole">@quinton-hoole&lt;/a>), Huawei&lt;/li>
&lt;li>Michelle Noorali (&lt;a href="https://github.com/michelleN">@michelleN&lt;/a>), Microsoft&lt;/li>
&lt;li>Phillip Wittrock (&lt;a href="https://github.com/pwittrock">@pwittrock&lt;/a>), Google&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Thanks to the candidates that came forward to run for election. May we always
have a strong set of people who want to push the community forward like yours
in every election.&lt;/li>
&lt;li>Thanks to all 377 voters who cast a ballot.&lt;/li>
&lt;li>And last but not least…Thanks to Cornell University for hosting &lt;a href="https://civs.cs.cornell.edu/">CIVS&lt;/a>!&lt;/li>
&lt;/ul>
&lt;h2 id="get-involved-with-the-steering-committee">Get Involved with the Steering Committee&lt;/h2>
&lt;p>You can follow along with Steering Committee &lt;a href="https://github.com/kubernetes/steering/projects/1">backlog items&lt;/a> and weigh in by
filing an issue or creating a PR against their &lt;a href="https://github.com/kubernetes/steering">repo&lt;/a>. They meet bi-weekly on
&lt;a href="https://github.com/kubernetes/steering">Wednesdays at 8pm UTC&lt;/a> and regularly attend Meet Our Contributors. They can
also be contacted at their public mailing list &lt;a href="mailto:steering@kubernetes.io">steering@kubernetes.io&lt;/a>.&lt;/p>
&lt;p>Steering Committee Meetings:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM">YouTube Playlist&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Contributor Summit San Diego Registration Open!</title><link>https://kubernetes.io/blog/2019/09/24/san-diego-contributor-summit/</link><pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2019/09/24/san-diego-contributor-summit/</guid><description>
&lt;p>&lt;strong>Authors: Paris Pittman (Google), Jeffrey Sica (Red Hat), Jonas Rosland (VMware)&lt;/strong>&lt;/p>
&lt;p>&lt;a href="https://events.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/">Contributor Summit San Diego 2019 Event Page&lt;/a>&lt;br>
Registration is now open and in record time, we’ve hit capacity for the
&lt;em>new contributor workshop&lt;/em> session of the event! Waitlist is now available.&lt;/p>
&lt;p>&lt;strong>Sunday, November 17&lt;/strong>&lt;br>
Evening Contributor Celebration:&lt;br>
&lt;a href="https://quartyardsd.com/">QuartYard&lt;/a>*&lt;br>
Address: 1301 Market Street, San Diego, CA 92101&lt;br>
Time: 6:00PM - 9:00PM&lt;/p>
&lt;p>&lt;strong>Monday, November 18&lt;/strong>&lt;br>
All Day Contributor Summit:&lt;br>
&lt;a href="https://www.marriott.com/hotels/travel/sandt-marriott-marquis-san-diego-marina/?scid=bb1a189a-fec3-4d19-a255-54ba596febe2">Marriott Marquis San Diego Marina&lt;/a>&lt;br>
Address: 333 W Harbor Dr, San Diego, CA 92101&lt;br>
Time: 9:00AM - 5:00PM&lt;/p>
&lt;p>While the Kubernetes project is only five years old, we’re already going into our
9th Contributor Summit this November in San Diego before KubeCon + CloudNativeCon.
The rapid increase is thanks to adding European and Asian Contributor Summits to
the North American events we’ve done previously. We will continue to run Contributor
Summits across the globe, as it is important that our contributor base grows in
all forms of diversity.&lt;/p>
&lt;p>Kubernetes has a large distributed remote contributing team, from &lt;a href="https://k8s.devstats.cncf.io/d/8/company-statistics-by-repository-group?orgId=1&amp;amp;var-period=y&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=All&amp;amp;var-companies=All">individuals and
organizations&lt;/a> all over the world. The Contributor Summits give the community three
chances a year to get together, work on community topics, and have hallway track
time. The upcoming San Diego summit is expected to bring over 450 attendees, and
will contain multiple tracks with something for everyone. The focus will be around
contributor growth and sustainability. We're going to stop here with capacity for
future summits; we want this event to offer value to individuals and the project.
We've heard from past summit attendee feedback that getting work done, learning,
and meeting folks face to face is a priority. By capping attendance and offering
the contributor gatherings in more locations, it will help us achieve those goals.&lt;/p>
&lt;p>This summit is unique as we’ve taken big moves on sustaining ourselves, the
contributor experience events team. Taking a page from the release team’s playbook,
we have added additional core team and shadow roles making it a natural mentoring
(watching+doing) relationship. The shadows are expected to fill another role at
one of the three events in 2020, and core team members to take the lead.
In preparation for this team, we’ve open sourced our &lt;a href="https://github.com/kubernetes/community/tree/master/events/events-team">rolebooks, guidelines,
best practices&lt;/a> and opened up our &lt;a href="https://docs.google.com/document/d/1oLXv5_rM4f645jlXym_Vd7AUq7x6DV-O87E6tcW1sjU/edit?usp=sharing">meetings&lt;/a> and &lt;a href="https://github.com/orgs/kubernetes/projects/21">project board&lt;/a>. Our team makes up
many parts of the Kubernetes project and takes care of making sure all voices
are represented.&lt;/p>
&lt;p>Are you at KubeCon + CloudNativeCon but can’t make it to the summit? Check out
the &lt;a href="https://kccncna19.sched.com/overview/type/Maintainer+Track+Sessions?iframe=yes">SIG Intro and Deep Dive sessions&lt;/a> during KubeCon + CloudNativeCon to
participate in Q&amp;amp;A and hear what’s up with each Special interest Group (SIG).
We’ll also record all of Contributor Summit’s presentation sessions, take notes
in discussions, and share it back with you, after the event is complete.&lt;/p>
&lt;p>We hope to see you all at Kubernetes Contributor Summit San Diego, make sure you
head over and &lt;a href="https://events.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/">register right now&lt;/a>! This event will sell out - here’s your warning.
:smiley:&lt;/p>
&lt;p>Check out past blogs on &lt;a href="https://kubernetes.io/blog/2019/03/20/a-look-back-and-whats-in-store-for-kubernetes-contributor-summits/">persona building around our events&lt;/a> and the &lt;a href="https://kubernetes.io/blog/2019/06/25/recap-of-kubernetes-contributor-summit-barcelona-2019/">Barcelona summit story&lt;/a>.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2019-09-24-san-diego-contributor-summit/IMG_2588.JPG" alt="Group Picture in 2018">&lt;/p>
&lt;p>*=QuartYard has a huge stage! Want to perform something in front of your contributor peers? Reach out to us! &lt;a href="mailto:community@kubernetes.io">community@kubernetes.io&lt;/a>&lt;/p></description></item><item><title>Blog: Kubernetes 1.16: Custom Resources, Overhauled Metrics, and Volume Extensions</title><link>https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/</link><pubDate>Wed, 18 Sep 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.16/release_team.md">Kubernetes 1.16 Release Team&lt;/a>&lt;/p>
&lt;p>We’re pleased to announce the delivery of Kubernetes 1.16, our third release of 2019! Kubernetes 1.16 consists of 31 enhancements: 8 enhancements moving to stable, 8 enhancements in beta, and 15 enhancements in alpha.&lt;/p>
&lt;h1 id="major-themes">Major Themes&lt;/h1>
&lt;h2 id="custom-resources">Custom resources&lt;/h2>
&lt;p>CRDs are in widespread use as a Kubernetes extensibility mechanism and have been available in beta since the 1.7 release. The 1.16 release marks the graduation of CRDs to general availability (GA).&lt;/p>
&lt;h2 id="overhauled-metrics">Overhauled metrics&lt;/h2>
&lt;p>Kubernetes has previously made extensive use of a global metrics registry to register metrics to be exposed. By implementing a metrics registry, metrics are registered in more transparent means. Previously, Kubernetes metrics have been excluded from any kind of stability requirements.&lt;/p>
&lt;h2 id="volume-extension">Volume Extension&lt;/h2>
&lt;p>There are quite a few enhancements in this release that pertain to volumes and volume modifications. Volume resizing support in CSI specs is moving to beta which allows for any CSI spec volume plugin to be resizable.&lt;/p>
&lt;h1 id="significant-changes-to-the-kubernetes-api">Significant Changes to the Kubernetes API&lt;/h1>
&lt;p>As the Kubernetes API has evolved, we have promoted some API resources to &lt;em>stable&lt;/em>, others have been reorganized to different groups. We deprecate older versions of a resource and make newer versions available in accordance with the &lt;a href="https://kubernetes.io/docs/concepts/overview/kubernetes-api/#api-versioning">API versioning policy&lt;/a>.&lt;/p>
&lt;p>An example of this is the &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">&lt;code>Deployment&lt;/code>&lt;/a> resource. This was introduced under the &lt;code>extensions/v1beta1&lt;/code> group in 1.6 and as the project changed has been promoted to &lt;code>extensions/v1beta2&lt;/code>, &lt;code>apps/v1beta2&lt;/code> and finally promoted to &lt;code>stable&lt;/code> and moved to &lt;code>apps/v1&lt;/code> in 1.9.&lt;/p>
&lt;p>It's important to note that until this release the project has not stopped serving any of the previous versions of the any of the deprecated resources.&lt;/p>
&lt;p>This means that folks interacting with the Kubernetes API have not been &lt;em>required&lt;/em> to move to the new version of any of the deprecated API objects.&lt;/p>
&lt;p>In 1.16 if you submit a &lt;code>Deployment&lt;/code> to the API server and specify &lt;code>extensions/v1beta1&lt;/code> as the API group it will be rejected with:&lt;/p>
&lt;pre>&lt;code>error: unable to recognize &amp;quot;deployment&amp;quot;: no matches for kind &amp;quot;Deployment&amp;quot; in version &amp;quot;extensions/v1beta1&amp;quot;
&lt;/code>&lt;/pre>&lt;p>With this release we are taking a very important step in the maturity of the Kubernetes API, and are no longer serving the deprecated APIs. Our earlier post &lt;a href="https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/">Deprecated APIs Removed In 1.16: Here’s What You Need To Know&lt;/a> tells you more, including which resources are affected.&lt;/p>
&lt;h1 id="additional-enhancements">Additional Enhancements&lt;/h1>
&lt;h2 id="custom-resources-reach-general-availability">Custom Resources Reach General Availability&lt;/h2>
&lt;p>CRDs have become the basis for extensions in the Kubernetes ecosystem. Started as a ground-up redesign of the ThirdPartyResources prototype, they have finally reached GA in 1.16 with apiextensions.k8s.io/v1, as the hard-won lessons of API evolution in Kubernetes have been integrated. As we transition to GA, the focus is on data consistency for API clients.&lt;/p>
&lt;p>As you upgrade to the GA API, you’ll notice that several of the previously optional guard rails have become required and/or default behavior. Things like structural schemas, pruning unknown fields, validation, and protecting the *.k8s.io group are important for ensuring the longevity of your APIs and are now much harder to accidentally miss. Defaulting is another important part of API evolution and that support will be on by default for CRD.v1. The combination of these, along with CRD conversion mechanisms are enough to build stable APIs that evolve over time, the same way that native Kubernetes resources have changed without breaking backward-compatibility.&lt;/p>
&lt;p>Updates to the CRD API won’t end here. We have ideas for features like arbitrary subresources, API group migration, and maybe a more efficient serialization protocol, but the changes from here are expected to be optional and complementary in nature to what’s already here in the GA API. Happy operator writing!&lt;/p>
&lt;p>Details on how to work with custom resources can be found &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">in the Kubernetes documentation&lt;/a>.&lt;/p>
&lt;h2 id="opening-doors-with-windows-enhancements">Opening Doors With Windows Enhancements&lt;/h2>
&lt;h3 id="beta-enhancing-the-workload-identity-options-for-windows-containers">Beta: Enhancing the workload identity options for Windows containers&lt;/h3>
&lt;p>Active Directory Group Managed Service Account (GMSA) support is graduating to beta and certain annotations that were introduced with the alpha support are being deprecated. GMSA is a specific type of Active Directory account that enables Windows containers to carry an identity across the network and communicate with other resources. Windows containers can now gain authenticated access to external resources. In addition, GMSA provides automatic password management, simplified service principal name (SPN) management, and the ability to delegate the management to other administrators across multiple servers.&lt;/p>
&lt;p>Adding support for RunAsUserName as an alpha release. The RunAsUserName is a string specifying the windows identity (or username) in Windows to run the entrypoint of the container and is a part of the newly introduced windowsOptions component of the securityContext (WindowsSecurityContextOptions).&lt;/p>
&lt;h3 id="alpha-improvements-to-setup-node-join-experience-with-kubeadm">Alpha: Improvements to setup &amp;amp; node join experience with kubeadm&lt;/h3>
&lt;p>Introducing alpha support for kubeadm, enabling Kubernetes users to easily join (and reset) Windows worker nodes to an existing cluster the same way they do for Linux nodes. Users can utilize kubeadm to prepare and add a Windows node to cluster. When the operations are complete, the node will be in a Ready state and able to run Windows containers. In addition, we will also provide a set of Windows-specific scripts to enable the installation of prerequisites and CNIs ahead of joining the node to the cluster.&lt;/p>
&lt;h3 id="alpha-introducing-support-for-container-storage-interface-csi">Alpha: Introducing support for Container Storage Interface (CSI)&lt;/h3>
&lt;p>Introducing CSI plugin support for out-of-tree providers, enabling Windows nodes in a Kubernetes cluster to leverage persistent storage capabilities for Windows-based workloads. This significantly expands the storage options of Windows workloads, adding onto a list that included FlexVolume and in-tree storage plugins. This capability is achieved through a host OS proxy that enables the execution of privileged operations on the Windows node on behalf of containers.&lt;/p>
&lt;h2 id="introducing-endpoint-slices">Introducing Endpoint Slices&lt;/h2>
&lt;p>The release of Kubernetes 1.16 includes an exciting new alpha feature: the EndpointSlice API. This API provides a scalable and extensible alternative to the &lt;a href="https://v1-16.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#endpoints-v1-core">Endpoints&lt;/a> resource, which dates back to the very first versions of Kubernetes. Behind the scenes, Endpoints play a big role in network routing within Kubernetes. Each Service endpoint is tracked within these resources - kube-proxy uses them for generating proxy rules that allow pods to communicate with each other so easily in Kubernetes, and many ingress controllers use them to route HTTP traffic directly to pods.&lt;/p>
&lt;h3 id="providing-greater-scalability">Providing Greater Scalability&lt;/h3>
&lt;p>A key goal for EndpointSlices is to enable greater scalability for Kubernetes Services. With the existing Endpoints API, a single instance must include network endpoints representing all pods matching a Service. As Services start to scale to thousands of pods, the corresponding Endpoints resources become quite large. Simply adding or removing one endpoint from a Service at this scale can be quite costly. As the Endpoints instance is updated, every piece of code watching Endpoints will need to be sent a full copy of the resource. With kube-proxy running on every node in a cluster, a copy needs to be sent to every single node. At a small scale, this is not an issue, but it becomes increasingly noticeable as clusters get larger.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2019-09-18-kubernetes-1-16-release-announcement/endpoint-slices.png" alt="Endpoints to Endpoint Slice">&lt;/p>
&lt;p>With EndpointSlices, network endpoints for a Service are split into multiple instances, significantly decreasing the amount of data required for updates at scale. By default, EndpointSlices are limited to 100 endpoints each.&lt;/p>
&lt;p>For example, let’s take a cluster with 10,000 Service endpoints spread over 5,000 nodes. A single Pod update would result in approximately 5GB transmitted with the Endpoints API (that’s enough to fill a DVD). This becomes increasingly significant given how frequently Endpoints can change during events like rolling updates on Deployments. The same update will be much more efficient with EndpointSlices since each one includes only a tiny portion of the total number of Service endpoints. Instead of transferring a big Endpoints object to each node, only the small EndpointSlice that’s been changed has to be transferred. In this example, EndpointSlices would decrease data transferred by approximately 100x.&lt;/p>
&lt;table>
&lt;tr>
&lt;td>
&lt;/td>
&lt;td>&lt;strong>Endpoints &lt;/strong>
&lt;/td>
&lt;td>&lt;strong>Endpoint Slices&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># of resources
&lt;/td>
&lt;td>&lt;em>1&lt;/em>
&lt;/td>
&lt;td>&lt;em>20k / 100 = 200&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># of network endpoints stored
&lt;/td>
&lt;td>&lt;em>1 * 20k = 20k&lt;/em>
&lt;/td>
&lt;td>&lt;em>200 * 100 = 20k&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>size of each resource
&lt;/td>
&lt;td>&lt;em>20k * const = ~2.0 MB&lt;/em>
&lt;/td>
&lt;td>&lt;em> 100 * const = ~10 kB&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>watch event data transferred
&lt;/td>
&lt;td>&lt;em>~2.0MB * 5k = 10GB&lt;/em>
&lt;/td>
&lt;td>&lt;em>~10kB * 5k = 50MB&lt;/em>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;h3 id="providing-greater-extensibility">Providing Greater Extensibility&lt;/h3>
&lt;p>A second goal for EndpointSlices was to provide a resource that would be highly extensible and useful across a wide variety of use cases. One of the key additions with EndpointSlices involves a new topology attribute. By default, this will be populated with the existing topology labels used throughout Kubernetes indicating attributes such as region and zone. Of course, this field can be populated with custom labels as well for more specialized use cases.&lt;/p>
&lt;p>EndpointSlices also include greater flexibility for address types. Each contains a list of addresses. An initial use case for multiple addresses would be to support dual-stack endpoints with both IPv4 and IPv6 addresses. As an example, here’s a simple EndpointSlice showing how one could be represented:&lt;/p>
&lt;pre>&lt;code>apiVersion: discovery.k8s.io/v1alpha
kind: EndpointSlice
metadata:
name: example-abc
labels:
kubernetes.io/service-name: example
addressType: IP
ports:
- name: http
protocol: TCP
port: 80
endpoints:
- addresses:
- &amp;quot;10.1.2.3&amp;quot;
- &amp;quot;2001:db8::1234:5678&amp;quot;
topology:
kubernetes.io/hostname: node-1
topology.kubernetes.io/zone: us-west2-a
&lt;/code>&lt;/pre>&lt;h3 id="more-about-endpoint-slices">More About Endpoint Slices&lt;/h3>
&lt;p>EndpointSlices are an alpha feature in Kubernetes 1.16 and not enabled by default. The Endpoints API will continue to be enabled by default, but we’re working to move the largest Endpoints consumers to the new EndpointSlice API. Notably, kube-proxy in Kubernetes 1.16 includes alpha support for EndpointSlices.&lt;/p>
&lt;p>The official Kubernetes documentation contains more information about EndpointSlices as well as how to enable them in your cluster. There’s also a &lt;a href="https://www.youtube.com/watch?v=Y5JOCCbJ_Fg">great KubeCon talk&lt;/a> that provides more background on the initial rationale for developing this API.&lt;/p>
&lt;h4 id="notable-feature-updates">Notable Feature Updates&lt;/h4>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/693">Topology Manager&lt;/a>, a new Kubelet component, aims to co-ordinate resource assignment decisions to provide optimized resource allocations.&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs//concepts/services-networking/dual-stack/">IPv4/IPv6 dual-stack&lt;/a> enables the allocation of both IPv4 and IPv6 addresses to Pods and Services.&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-cloud-provider/20190422-cloud-controller-manager-migration.md">Extensions&lt;/a> for Cloud Controller Manager Migration.&lt;/li>
&lt;/ul>
&lt;h2 id="availability">Availability&lt;/h2>
&lt;p>Kubernetes 1.16 is available for &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.16.0">download on GitHub&lt;/a>. To get started with Kubernetes, check out these &lt;a href="https://kubernetes.io/docs/tutorials/">interactive tutorials&lt;/a>. You can also easily install 1.16 using &lt;a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a>.&lt;/p>
&lt;h2 id="release-team">Release Team&lt;/h2>
&lt;p>This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the &lt;a href="http://bit.ly/k8s116-team">release team&lt;/a> led by Lachlan Evenson, Principal Program Manager at Microsoft. The 32 individuals on the release team coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.&lt;/p>
&lt;p>As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over &lt;a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">32,000 individual contributors&lt;/a> to date and an active community of more than 66,000 people.&lt;/p>
&lt;h2 id="release-mascot">Release Mascot&lt;/h2>
&lt;p>The Kubernetes 1.16 release crest was loosely inspired by the Apollo 16 mission crest. It represents the hard work of the release-team and the community alike and is an ode to the challenges and fun times we shared as a team throughout the release cycle. Many thanks to Ronan Flynn-Curran of Microsoft for creating this magnificent piece.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2019-09-18-kubernetes-1-16-release-announcement/mascot.png" alt="Kubernetes 1.16 Release Mascot">&lt;/p>
&lt;h1 id="kubernetes-updates">Kubernetes Updates&lt;/h1>
&lt;h2 id="project-velocity">Project Velocity&lt;/h2>
&lt;p>The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. &lt;a href="https://k8s.devstats.cncf.io">K8s DevStats&lt;/a> illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times. This past year, &lt;a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=Last%20year&amp;amp;var-metric=contributions">1,147 different companies and over 3,149 individuals&lt;/a> contribute to Kubernetes each month. &lt;a href="https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;amp;var-period=m&amp;amp;var-repogroup_name=All">Check out DevStats&lt;/a> to learn more about the overall velocity of the Kubernetes project and community.&lt;/p>
&lt;h2 id="ecosystem">Ecosystem&lt;/h2>
&lt;ul>
&lt;li>The Kubernetes project leadership created the Security Audit Working Group to oversee the very first third-part &lt;a href="https://www.cncf.io/blog/2019/08/06/open-sourcing-the-kubernetes-security-audit/">Kubernetes security audit&lt;/a>, in an effort to improve the overall security of the ecosystem.&lt;/li>
&lt;li>The &lt;a href="https://www.cncf.io/announcement/2019/07/09/the-cloud-native-computing-foundation-announces-the-kubernetes-certified-service-providers-program-has-reached-100-participants/">Kubernetes Certified Service Providers&lt;/a> program (KCSP) reached 100 member companies, ranging from the largest multinational cloud, enterprise software, and consulting companies to tiny startups.&lt;/li>
&lt;li>The first &lt;a href="https://www.cncf.io/blog/2019/08/29/announcing-the-cncf-kubernetes-project-journey-report/">Kubernetes Project Journey Report&lt;/a> was released, showcasing the massive growth of the project.&lt;/li>
&lt;/ul>
&lt;h2 id="kubecon-cloudnativecon">KubeCon + CloudNativeCon&lt;/h2>
&lt;p>The Cloud Native Computing Foundation’s flagship conference gathers adopters and technologists from leading open source and cloud native communities in San Diego, California from November 18-21, 2019. Join Kubernetes, Prometheus, Envoy, CoreDNS, containerd, Fluentd, OpenTracing, gRPC, CNI, Jaeger, Notary, TUF, Vitess, NATS, Linkerd, Helm, Rook, Harbor, etcd, Open Policy Agent, CRI-O, and TiKV as the community gathers for four days to further the education and advancement of cloud native computing. &lt;a href="https://www.cncf.io/community/kubecon-cloudnativecon-events/">Register today&lt;/a>!&lt;/p>
&lt;h2 id="webinar">Webinar&lt;/h2>
&lt;p>Join members of the Kubernetes 1.16 release team on Oct 22, 2019 to learn about the major features in this release. Register &lt;a href="https://zoom.us/webinar/register/9015681469655/WN_JTLYA0DMRD6Mnm2f64KYMg">here&lt;/a>.&lt;/p>
&lt;h2 id="get-involved">Get Involved&lt;/h2>
&lt;p>The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Special Interest Groups&lt;/a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href="https://github.com/kubernetes/community/tree/master/communication">community meeting&lt;/a>, and through the channels below. Thank you for your continued feedback and support.&lt;/p>
&lt;ul>
&lt;li>Follow us on Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for latest updates&lt;/li>
&lt;li>Join the community discussion on &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>&lt;/li>
&lt;li>Join the community on &lt;a href="http://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>Post questions (or answer questions) on &lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a>&lt;/li>
&lt;li>Share your Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">story&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Announcing etcd 3.4</title><link>https://kubernetes.io/blog/2019/08/30/announcing-etcd-3-4/</link><pubDate>Fri, 30 Aug 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2019/08/30/announcing-etcd-3-4/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Gyuho Lee (Amazon Web Services, @&lt;a href="https://github.com/gyuho">gyuho&lt;/a>), Jingyi Hu (Google, @&lt;a href="https://github.com/jingyih">jingyih&lt;/a>)&lt;/p>
&lt;p>etcd 3.4 focuses on stability, performance and ease of operation, with features like pre-vote and non-voting member and improvements to storage backend and client balancer.&lt;/p>
&lt;p>Please see &lt;a href="https://github.com/etcd-io/etcd/blob/master/CHANGELOG-3.4.md">CHANGELOG&lt;/a> for full lists of changes.&lt;/p>
&lt;h2 id="better-storage-backend">Better Storage Backend&lt;/h2>
&lt;p>etcd v3.4 includes a number of performance improvements for large scale Kubernetes workloads.&lt;/p>
&lt;p>In particular, etcd experienced performance issues with a large number of concurrent read transactions even when there is no write (e.g. &lt;code>“read-only range request ... took too long to execute”&lt;/code>). Previously, the storage backend commit operation on pending writes blocks incoming read transactions, even when there was no pending write. Now, the commit &lt;a href="https://github.com/etcd-io/etcd/pull/9296">does not block reads&lt;/a> which improve long-running read transaction performance.&lt;/p>
&lt;p>We further made &lt;a href="https://github.com/etcd-io/etcd/pull/10523">backend read transactions fully concurrent&lt;/a>. Previously, ongoing long-running read transactions block writes and upcoming reads. With this change, write throughput is increased by 70% and P99 write latency is reduced by 90% in the presence of long-running reads. We also ran &lt;a href="https://prow.k8s.io/view/gcs/kubernetes-jenkins/logs/ci-kubernetes-e2e-gce-scale-performance/1130745634945503235">Kubernetes 5000-node scalability test on GCE&lt;/a> with this change and observed similar improvements. For example, in the very beginning of the test where there are a lot of long-running “LIST pods”, the P99 latency of “POST clusterrolebindings” is &lt;a href="https://github.com/etcd-io/etcd/pull/10523#issuecomment-499262001">reduced by 97.4%&lt;/a>.&lt;/p>
&lt;p>More improvements have been made to lease storage. We enhanced &lt;a href="https://github.com/etcd-io/etcd/pull/9418">lease expire/revoke performance&lt;/a> by storing lease objects more efficiently, and made &lt;a href="https://github.com/etcd-io/etcd/pull/9229">lease look-up operation non-blocking&lt;/a> with current lease grant/revoke operation. And etcd v3.4 introduces &lt;a href="https://github.com/etcd-io/etcd/pull/9924">lease checkpoint&lt;/a> as an experimental feature to persist remaining time-to-live values through consensus. This ensures short-lived lease objects are not auto-renewed after leadership election. This also prevents lease object pile-up when the time-to-live value is relatively large (e.g. &lt;a href="https://github.com/kubernetes/kubernetes/issues/65497">1-hour TTL never expired in Kubernetes use case&lt;/a>).&lt;/p>
&lt;h2 id="improved-raft-voting-process">Improved Raft Voting Process&lt;/h2>
&lt;p>etcd server implements &lt;a href="https://raft.github.io">Raft consensus algorithm&lt;/a> for data replication. Raft is a leader-based protocol. Data is replicated from leader to follower; a follower forwards proposals to a leader, and the leader decides what to commit or not. Leader persists and replicates an entry, once it has been agreed by the quorum of cluster. The cluster members elect a single leader, and all other members become followers. The elected leader periodically sends heartbeats to its followers to maintain its leadership, and expects responses from each follower to keep track of its progress.&lt;/p>
&lt;p>In its simplest form, a Raft leader steps down to a follower when it receives a message with higher terms without any further cluster-wide health checks. This behavior can affect the overall cluster availability.&lt;/p>
&lt;p>For instance, a flaky (or rejoining) member drops in and out, and starts campaign. This member ends up with higher terms, ignores all incoming messages with lower terms, and sends out messages with higher terms. When the leader receives this message of a higher term, it reverts back to follower.&lt;/p>
&lt;p>This becomes more disruptive when there’s a network partition. Whenever the partitioned node regains its connectivity, it can possibly trigger the leader re-election. To address this issue, etcd Raft introduces a new node state pre-candidate with the &lt;a href="https://github.com/etcd-io/etcd/pull/9352">pre-vote feature&lt;/a>. The pre-candidate first asks other servers whether it's up-to-date enough to get votes. Only if it can get votes from the majority, it increments its term and starts an election. This extra phase improves the robustness of leader election in general. And helps the leader remain stable as long as it maintains its connectivity with the quorum of its peers.&lt;/p>
&lt;p>Similarly, etcd availability can be affected when a restarting node has not received the leader heartbeats in time (e.g. due to slow network), which triggers the leader election. Previously, etcd fast-forwards election ticks on server start, with only one tick left for leader election. For example, when the election timeout is 1-second, the follower only waits 100ms for leader contacts before starting an election. This speeds up initial server start, by not having to wait for the election timeouts (e.g. election is triggered in 100ms instead of 1-second). Advancing election ticks is also useful for cross datacenter deployments with larger election timeouts. However, in many cases, the availability is more critical than the speed of initial leader election. To ensure better availability with rejoining nodes, etcd now &lt;a href="https://github.com/etcd-io/etcd/pull/9415">adjusts election ticks&lt;/a> with more than one tick left, thus more time for the leader to prevent a disruptive restart.&lt;/p>
&lt;h2 id="raft-non-voting-member-learner">Raft Non-Voting Member, Learner&lt;/h2>
&lt;p>The challenge with membership reconfiguration is that it often leads to quorum size changes, which are prone to cluster unavailabilities. Even if it does not alter the quorum, clusters with membership change are more likely to experience other underlying problems. To improve the reliability and confidence of reconfiguration, a new role - learner is introduced in etcd 3.4 release.&lt;/p>
&lt;p>A new etcd member joins the cluster with no initial data, requesting all historical updates from the leader until it catches up to the leader’s logs. This means the leader’s network is more likely to be overloaded, blocking or dropping leader heartbeats to followers. In such cases, a follower may experience election-timeout and start a new leader election. That is, a cluster with a new member is more vulnerable to leader election. Both leader election and the subsequent update propagation to the new member are prone to causing periods of cluster unavailability (see &lt;em>Figure 1&lt;/em>).&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2019-08-30-announcing-etcd-3.4/figure-1.png" alt="learner-figure-1">&lt;/p>
&lt;p>The worst case is a misconfigured membership add. Membership reconfiguration in etcd is a two-step process: &lt;code>etcdctl member add&lt;/code> with peer URLs, and starting a new etcd to join the cluster. That is, &lt;code>member add&lt;/code> command is applied whether the peer URL value is invalid or not. If the first step is to apply the invalid URLs and change the quorum size, it is possible that the cluster already loses the quorum until the new node connects. Since the node with invalid URLs will never become online and there’s no leader, it is impossible to revert the membership change (see &lt;em>Figure 2&lt;/em>).&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2019-08-30-announcing-etcd-3.4/figure-2.png" alt="learner-figure-2">&lt;/p>
&lt;p>This becomes more complicated when there are partitioned nodes (see the &lt;a href="https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-learner.md">design document&lt;/a> for more).&lt;/p>
&lt;p>In order to address such failure modes, etcd introduces a &lt;a href="https://github.com/etcd-io/etcd/issues/10537">new node state “Learner”&lt;/a>, which joins the cluster as a non-voting member until it catches up to leader’s logs. This means the learner still receives all updates from leader, while it does not count towards the quorum, which is used by the leader to evaluate peer activeness. The learner only serves as a standby node until promoted. This relaxed requirements for quorum provides the better availability during membership reconfiguration and operational safety (see &lt;em>Figure 3&lt;/em>).&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2019-08-30-announcing-etcd-3.4/figure-3.png" alt="learner-figure-3">&lt;/p>
&lt;p>We will further improve learner robustness, and explore auto-promote mechanisms for easier and more reliable operation. Please read our &lt;a href="https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-learner.md">learner design documentation&lt;/a> and &lt;a href="https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/runtime-configuration.md#add-a-new-member-as-learner">runtime-configuration document&lt;/a> for user guides.&lt;/p>
&lt;h2 id="new-client-balancer">New Client Balancer&lt;/h2>
&lt;p>etcd is designed to tolerate various system and network faults. By design, even if one node goes down, the cluster “appears” to be working normally, by providing one logical cluster view of multiple servers. But, this does not guarantee the liveness of the client. Thus, etcd client has implemented a different set of intricate protocols to guarantee its correctness and high availability under faulty conditions.&lt;/p>
&lt;p>Historically, etcd client balancer heavily relied on old gRPC interface: every gRPC dependency upgrade broke client behavior. A majority of development and debugging efforts were devoted to fixing those client behavior changes. As a result, its implementation has become overly complicated with bad assumptions on server connectivity. The primary goal was to &lt;a href="https://github.com/etcd-io/etcd/pull/9860">simplify balancer failover logic in etcd v3.4 client&lt;/a>; instead of maintaining a list of unhealthy endpoints, which may be stale, simply roundrobin to the next endpoint whenever client gets disconnected from the current endpoint. It does not assume endpoint status. Thus, no more complicated status tracking is needed.&lt;/p>
&lt;p>Furthermore, the new client now creates its own credential bundle to &lt;a href="https://github.com/etcd-io/etcd/pull/10911">fix balancer failover against secure endpoints&lt;/a>. This resolves the &lt;a href="https://github.com/kubernetes/kubernetes/issues/72102">year-long bug&lt;/a>, where kube-apiserver loses its connectivity to etcd cluster when the first etcd server becomes unavailable.&lt;/p>
&lt;p>Please see &lt;a href="https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-client.md">client balancer design documentation&lt;/a> for more.&lt;/p></description></item></channel></rss>