<!doctype html><html id=caseStudies lang=ko><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36037335-10"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-36037335-10');</script><link rel=alternate hreflang=en href=https://kubernetes.io/case-studies/openai/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.74.3"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>OpenAI Case Study | Kubernetes</title><meta property="og:title" content="OpenAI Case Study"><meta property="og:description" content="CASE STUDY:Launching and Scaling Up Experiments, Made Simple   Company &nbsp;OpenAI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Location &nbsp;San Francisco, California&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Industry &nbsp;Artificial Intelligence Research   Challenge An artificial intelligence research lab, OpenAI needed infrastructure for deep learning that would allow experiments to be run either in the cloud or in its own data center, and to easily scale. Portability, speed, and cost were the main drivers. Solution OpenAI began running Kubernetes on top of AWS in 2016, and in early 2017 migrated to Azure."><meta property="og:type" content="article"><meta property="og:url" content="https://kubernetes.io/ko/case-studies/openai/"><meta property="og:image" content="https://kubernetes.io/ko/case-studies/openai/openai_featured.png"><meta property="article:modified_time" content="2020-07-24T11:09:21+09:00"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="OpenAI Case Study"><meta itemprop=description content="CASE STUDY:Launching and Scaling Up Experiments, Made Simple   Company &nbsp;OpenAI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Location &nbsp;San Francisco, California&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Industry &nbsp;Artificial Intelligence Research   Challenge An artificial intelligence research lab, OpenAI needed infrastructure for deep learning that would allow experiments to be run either in the cloud or in its own data center, and to easily scale. Portability, speed, and cost were the main drivers. Solution OpenAI began running Kubernetes on top of AWS in 2016, and in early 2017 migrated to Azure."><meta itemprop=dateModified content="2020-07-24T11:09:21+09:00"><meta itemprop=wordCount content="1324"><meta itemprop=image content="https://kubernetes.io/ko/case-studies/openai/openai_featured.png"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://kubernetes.io/ko/case-studies/openai/openai_featured.png"><meta name=twitter:title content="OpenAI Case Study"><meta name=twitter:description content="CASE STUDY:Launching and Scaling Up Experiments, Made Simple   Company &nbsp;OpenAI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Location &nbsp;San Francisco, California&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Industry &nbsp;Artificial Intelligence Research   Challenge An artificial intelligence research lab, OpenAI needed infrastructure for deep learning that would allow experiments to be run either in the cloud or in its own data center, and to easily scale. Portability, speed, and cost were the main drivers. Solution OpenAI began running Kubernetes on top of AWS in 2016, and in early 2017 migrated to Azure."><link rel=preload href=/scss/main.min.29d9d9a40037804116049b9cb05556d3c544759c5ed8a73b36824ddcb7c27564.css as=style><link href=/scss/main.min.29d9d9a40037804116049b9cb05556d3c544759c5ed8a73b36824ddcb7c27564.css rel=stylesheet integrity><script src=/js/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png"}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/base_fonts.css><link rel=stylesheet href=/scss/_case-studies.b15ae776455f6807aae6e0419f6c71fd5941367528843083356b1bc48b4630a1.css integrity="sha256-sVrndkVfaAeq5uBBn2xx/VlBNnUohDCDNWsbxItGMKE="><link rel=stylesheet href=/css/jquery-ui.min.css><link rel=stylesheet href=/css/callouts.css><link rel=stylesheet href=/css/custom-jekyll/tags.css><link rel=stylesheet href=/css/style_case_studies.css><meta name=description content="CASE STUDY:Launching and Scaling Up Experiments, Made Simple   Company &nbsp;OpenAI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Location &nbsp;San Francisco, California&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Industry &nbsp;Artificial Intelligence Research   Challenge An artificial intelligence research lab, OpenAI needed infrastructure for deep learning that would allow experiments to be run either in the cloud or in its own data center, and to easily scale. Portability, speed, and cost were the main drivers. Solution OpenAI began running Kubernetes on top of AWS in 2016, and in early 2017 migrated to Azure."><meta property="og:description" content="CASE STUDY:Launching and Scaling Up Experiments, Made Simple   Company &nbsp;OpenAI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Location &nbsp;San Francisco, California&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Industry &nbsp;Artificial Intelligence Research   Challenge An artificial intelligence research lab, OpenAI needed infrastructure for deep learning that would allow experiments to be run either in the cloud or in its own data center, and to easily scale. Portability, speed, and cost were the main drivers. Solution OpenAI began running Kubernetes on top of AWS in 2016, and in early 2017 migrated to Azure."><meta name=twitter:description content="CASE STUDY:Launching and Scaling Up Experiments, Made Simple   Company &nbsp;OpenAI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Location &nbsp;San Francisco, California&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Industry &nbsp;Artificial Intelligence Research   Challenge An artificial intelligence research lab, OpenAI needed infrastructure for deep learning that would allow experiments to be run either in the cloud or in its own data center, and to easily scale. Portability, speed, and cost were the main drivers. Solution OpenAI began running Kubernetes on top of AWS in 2016, and in early 2017 migrated to Azure."><meta property="og:url" content="https://kubernetes.io/ko/case-studies/openai/"><meta property="og:title" content="OpenAI Case Study"><meta name=twitter:title content="OpenAI Case Study"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-ui-1.12.1.min.js></script><script src=/js/sweetalert-2.1.2.min.js></script><script src=/js/script.js></script></head><body><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/ko/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ko/docs/>문서</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ko/training/>교육</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ko/partners/>파트너</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ko/community/>커뮤니티</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/ko/case-studies/>사례 연구</span></a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://kubernetes.io>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io>v1.21</a>
<a class=dropdown-item href=https://v1-20.docs.kubernetes.io>v1.20</a>
<a class=dropdown-item href=https://v1-19.docs.kubernetes.io>v1.19</a>
<a class=dropdown-item href=https://v1-18.docs.kubernetes.io>v1.18</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>한국어 Korean</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/case-studies/openai/>English</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav><section id=deprecation-warning><div class="content deprecation-warning"><h3>You are viewing documentation for Kubernetes version: v1.18</h3><p>Kubernetes v1.18 문서는 더 이상 적극적으로 관리되지 않음. 현재 보고있는 문서는 정적 스냅샷임. 최신 문서를 위해서는, 다음을 참고.
<a href=https://kubernetes.io/docs/home/>최신 버전.</a></p></div></section><div class="banner1 desktop" style=background-image:url(/images/case-studies/openAI/banner1.jpg)><h1>CASE STUDY:<img src=/images/openAI_logo.png style=margin-bottom:-1% class=header_logo><br><div class=subhead>Launching and Scaling Up Experiments, Made Simple</div></h1></div><div class=details>Company &nbsp;<b>OpenAI</b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Location &nbsp;<b>San Francisco, California</b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Industry &nbsp;<b>Artificial Intelligence Research</b></div><hr><section class=section1><div class=cols><div class=col1><h2>Challenge</h2>An artificial intelligence research lab, OpenAI needed infrastructure for deep learning that would allow experiments to be run either in the cloud or in its own data center, and to easily scale. Portability, speed, and cost were the main drivers.<br><h2>Solution</h2>OpenAI began running Kubernetes on top of AWS in 2016, and in early 2017 migrated to Azure. OpenAI runs key experiments in fields including robotics and gaming both in Azure and in its own data centers, depending on which cluster has free capacity. "We use Kubernetes mainly as a batch scheduling system and rely on our <a href=https://github.com/openai/kubernetes-ec2-autoscaler>autoscaler</a> to dynamically scale up and down our cluster," says Christopher Berner, Head of Infrastructure. "This lets us significantly reduce costs for idle nodes, while still providing low latency and rapid iteration."</div><div class=col2><h2>Impact</h2>The company has benefited from greater portability: "Because Kubernetes provides a consistent API, we can move our research experiments very easily between clusters," says Berner. Being able to use its own data centers when appropriate is "lowering costs and providing us access to hardware that we wouldn’t necessarily have access to in the cloud," he adds. "As long as the utilization is high, the costs are much lower there." Launching experiments also takes far less time: "One of our researchers who is working on a new distributed training system has been able to get his experiment running in two or three days. In a week or two he scaled it out to hundreds of GPUs. Previously, that would have easily been a couple of months of work."</div></div></section><div class=banner2><div class=banner2text><div class=video><iframe width=560 height=315 src=https://www.youtube.com/embed/v4N3Krzb8Eg frameborder=0 allow="autoplay; encrypted-media" allowfullscreen></iframe><br><span style=font-size:21px;line-height:.5em!important;width:60%>Check out "Building the Infrastructure that Powers the Future of AI" presented by Vicki Cheung, Member of Technical Staff & Jonas Schneider, Member of Technical Staff at OpenAI from KubeCon/CloudNativeCon Europe 2017.</span></div></div></div><section class=section2><div class=fullcol><h2>From experiments in robotics to old-school video game play research, OpenAI’s work in artificial intelligence technology is meant to be shared.</h2>With a mission to ensure powerful AI systems are safe, OpenAI cares deeply about open source—both benefiting from it and contributing safety technology into it. "The research that we do, we want to spread it as widely as possible so everyone can benefit," says OpenAI’s Head of Infrastructure Christopher Berner. The lab’s philosophy—as well as its particular needs—lent itself to embracing an open source, cloud native strategy for its deep learning infrastructure.<br><br>OpenAI started running Kubernetes on top of AWS in 2016, and a year later, migrated the Kubernetes clusters to Azure. "We probably use Kubernetes differently from a lot of people," says Berner. "We use it for batch scheduling and as a workload manager for the cluster. It’s a way of coordinating a large number of containers that are all connected together. We rely on our <a href=https://github.com/openai/kubernetes-ec2-autoscaler>autoscaler</a> to dynamically scale up and down our cluster. This lets us significantly reduce costs for idle nodes, while still providing low latency and rapid iteration."<br><br>In the past year, Berner has overseen the launch of several Kubernetes clusters in OpenAI’s own data centers. "We run them in a hybrid model where the control planes—the Kubernetes API servers, <a href=https://github.com/coreos/etcd>etcd</a> and everything—are all in Azure, and then all of the Kubernetes nodes are in our own data center," says Berner. "The cloud is really convenient for managing etcd and all of the masters, and having backups and spinning up new nodes if anything breaks. This model allows us to take advantage of lower costs and have the availability of more specialized hardware in our own data center."</div></section><div class=banner3 style=background-image:url(/images/case-studies/openAI/banner3.jpg)><div class=banner3text>OpenAI’s experiments take advantage of Kubernetes’ benefits, including portability. "Because Kubernetes provides a consistent API, we can move our research experiments very easily between clusters..."</div></div><section class=section3><div class=fullcol>Different teams at OpenAI currently run a couple dozen projects. While the largest-scale workloads manage bare cloud VMs directly, most of OpenAI’s experiments take advantage of Kubernetes’ benefits, including portability. "Because Kubernetes provides a consistent API, we can move our research experiments very easily between clusters," says Berner. The on-prem clusters are generally "used for workloads where you need lots of GPUs, something like training an ImageNet model. Anything that’s CPU heavy, that’s run in the cloud. But we also have a number of teams that run their experiments both in Azure and in our own data centers, just depending on which cluster has free capacity, and that’s hugely valuable."<br><br>Berner has made the Kubernetes clusters available to all OpenAI teams to use if it’s a good fit. "I’ve worked a lot with our games team, which at the moment is doing research on classic console games," he says. "They had been running a bunch of their experiments on our dev servers, and they had been trying out Google cloud, managing their own VMs. We got them to try out our first on-prem Kubernetes cluster, and that was really successful. They’ve now moved over completely to it, and it has allowed them to scale up their experiments by 10x, and do that without needing to invest significant engineering time to figure out how to manage more machines. A lot of people are now following the same path."</div></section><div class=banner4 style=background-image:url(/images/case-studies/openAI/banner4.jpg)><div class=banner4text>"One of our researchers who is working on a new distributed training system has been able to get his experiment running in two or three days," says Berner. "In a week or two he scaled it out to hundreds of GPUs. Previously, that would have easily been a couple of months of work."</div></div><section class=section5 style=padding:0!important><div class=fullcol>That path has been simplified by frameworks and tools that two of OpenAI’s teams have developed to handle interaction with Kubernetes. "You can just write some Python code, fill out a bit of configuration with exactly how many machines you need and which types, and then it will prepare all of those specifications and send it to the Kube cluster so that it gets launched there," says Berner. "And it also provides a bit of extra monitoring and better tooling that’s designed specifically for these machine learning projects."<br><br>The impact that Kubernetes has had at OpenAI is impressive. With Kubernetes, the frameworks and tooling, including the autoscaler, in place, launching experiments takes far less time. "One of our researchers who is working on a new distributed training system has been able to get his experiment running in two or three days," says Berner. "In a week or two he scaled it out to hundreds of GPUs. Previously, that would have easily been a couple of months of work."<br><br>Plus, the flexibility they now have to use their on-prem Kubernetes cluster when appropriate is "lowering costs and providing us access to hardware that we wouldn’t necessarily have access to in the cloud," he says. "As long as the utilization is high, the costs are much lower in our data center. To an extent, you can also customize your hardware to exactly what you need."</div><div class=banner5><div class=banner5text>"Research teams can now take advantage of the frameworks we’ve built on top of Kubernetes, which make it easy to launch experiments, scale them by 10x or 50x, and take little effort to manage."<br><span style=font-size:14px;letter-spacing:.12em;padding-top:20px>— CHRISTOPHER BERNER, HEAD OF INFRASTRUCTURE FOR OPENAI</span></div></div><div class=fullcol>OpenAI is also benefiting from other technologies in the CNCF cloud-native ecosystem. <a href=https://grpc.io/>gRPC</a> is used by many of its systems for communications between different services, and <a href=https://prometheus.io/>Prometheus</a> is in place "as a debugging tool if things go wrong," says Berner. "We actually haven’t had any real problems in our Kubernetes clusters recently, so I don’t think anyone has looked at our Prometheus monitoring in a while. If something breaks, it will be there."<br><br>One of the things Berner continues to focus on is Kubernetes’ ability to scale, which is essential to deep learning experiments. OpenAI has been able to push one of its Kubernetes clusters on Azure up to more than <a href=https://blog.openai.com/scaling-kubernetes-to-2500-nodes/>2,500 nodes</a>. "I think we’ll probably hit the 5,000-machine number that Kubernetes has been tested at before too long," says Berner, adding, "We’re definitely <a href=https://jobs.lever.co/openai>hiring</a> if you’re excited about working on these things!"</div></section><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/ko/docs/home/>홈</a>
<a class=text-white href=/ko/training/>교육</a>
<a class=text-white href=/ko/partners/>파트너</a>
<a class=text-white href=/ko/community/>커뮤니티</a>
<a class=text-white href=/ko/case-studies/>사례 연구</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script><script src=https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js integrity=sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy crossorigin=anonymous></script><script src=/js/main.min.4c88b9704d4f4a0c299df9314d9912a9d1b77eed43f42923d7e1efd15c8ed139.js integrity="sha256-TIi5cE1PSgwpnfkxTZkSqdG3fu1D9Ckj1+Hv0VyO0Tk=" crossorigin=anonymous></script></body></html>